using QuantumControl.QuantumPropagators.Controls: evaluate, evaluate!, discretize
using QuantumControl.QuantumPropagators: prop_step!, set_state!, reinit_prop!, propagate
using QuantumControl.QuantumPropagators.Storage:
    write_to_storage!, get_from_storage!, get_from_storage
using QuantumControl.QuantumPropagators.Interfaces: supports_inplace
using QuantumGradientGenerators: resetgradvec!
using QuantumControl: set_atexit_save_optimization, @threadsif
using QuantumControl.Functionals: make_chi, make_grad_J_a
using QuantumControl.QuantumPropagators: _StoreState
using LinearAlgebra
using Printf

# BEGIN DEBUG

DEBUG_FH = nothing

function open_debug_file(filename)
    global DEBUG_FH
    DEBUG_FH = open(filename, "w")
end

function close_debug_file()
    global DEBUG_FH
    close(DEBUG_FH)
    DEBUG_FH = nothing
end

function _c(c; fmt=:float)
    a = real(c)
    b = imag(c)
    if abs(a) < 1e-10
        a = 0.0
    end
    if abs(b) < 1e-10
        b = 0.0
    end
    if fmt == :float
        return @sprintf("%+.4f%+.4fğ•š", a, b)
    elseif fmt == :exp
        return @sprintf("%+.2e%+.2eğ•š", a, b)
    else
        throw(ArgumentError("Invalid fmt=$(repr(fmt))"))
    end
end

function _state(Î¨; fmt=:float)
    return "(" * join([_c(c; fmt) for c::ComplexF64 in Î¨], ",") * ")"
end
# END DEBUG

import QuantumControl: optimize, make_print_iters

@doc raw"""
```julia
using GRAPE
result = optimize(problem; method=GRAPE, kwargs...)
```

optimizes the given control [`problem`](@ref QuantumControl.ControlProblem)
via the GRAPE method, by minimizing the functional

```math
J(\{Ïµ_{nl}\}) = J_T(\{|Ï•_k(T)âŸ©\}) + Î»_a J_a(\{Ïµ_{nl}\})
```

where the final time functional ``J_T`` depends explicitly on the
forward-propagated states and the running cost ``J_a`` depends explicitly on
pulse values ``Ïµ_{nl}`` of the l'th control discretized on the n'th interval of
the time grid.

Returns a [`GrapeResult`](@ref).

Keyword arguments that control the optimization are taken from the keyword
arguments used in the instantiation of `problem`; any of these can be overridden
with explicit keyword arguments to `optimize`.


# Required problem keyword arguments

* `J_T`: A function `J_T(Î¨, trajectories)` that evaluates the final time
  functional from a list `Î¨` of forward-propagated states and
  `problem.trajectories`. The function `J_T` may also take a keyword argument
  `tau`. If it does, a vector containing the complex overlaps of the target
  states (`target_state` property of each trajectory in `problem.trajectories`)
  with the propagated states will be passed to `J_T`.

# Optional problem keyword arguments

* `chi`: A function `chi(Î¨, trajectories)` that receives a list `Î¨`
  of the forward propagated states and returns a vector of states
  ``|Ï‡â‚–âŸ© = -âˆ‚J_T/âˆ‚âŸ¨Î¨â‚–|``. If not given, it will be automatically determined
  from `J_T` via [`make_chi`](@ref) with the default parameters. Similarly to
  `J_T`, if `chi` accepts a keyword argument `tau`, it will be passed a vector
  of complex overlaps.
* `J_a`: A function `J_a(pulsevals, tlist)` that evaluates running costs over
  the pulse values, where `pulsevals` are the vectorized values ``Ïµ_{nl}``,
  where `n` are in indices of the time intervals and `l` are the indices over
  the controls, i.e., `[Ïµâ‚â‚, Ïµâ‚‚â‚, â€¦, Ïµâ‚â‚‚, Ïµâ‚‚â‚‚, â€¦]` (the pulse values for each
  control are contiguous). If not given, the optimization will not include a
  running cost.
* `gradient_method=:gradgen`: One of `:gradgen` (default) or `:taylor`.
  With `gradient_method=:gradgen`, the gradient is calculated using
  [QuantumGradientGenerators]
  (https://github.com/JuliaQuantumControl/QuantumGradientGenerators.jl).
  With `gradient_method=:taylor`, it is evaluated via a Taylor series, see
  Eq.Â (20) in Kuprov and Rogers,  J. Chem. Phys. 131, 234108
  (2009)Â [KuprovJCP09](@cite).
* `taylor_grad_max_order=100`: If given with `gradient_method=:taylor`, the
  maximum number of terms in the Taylor series. If
  `taylor_grad_check_convergence=true` (default), if the Taylor series does not
  convergence within the given number of terms, throw an an error. With
  `taylor_grad_check_convergence=true`, this is the exact order of the Taylor
  series.
* `taylor_grad_tolerance=1e-16`: If given with `gradient_method=:taylor` and
  `taylor_grad_check_convergence=true`, stop the Taylor series when the norm of
  the term falls below the given tolerance. Ignored if
  `taylor_grad_check_convergence=false`.
* `taylor_grad_check_convergence=true`: If given as `true` (default), check the
  convergence after each term in the Taylor series an stop as soon as the norm
  of the term drops below the given number. If `false`, stop after exactly
  `taylor_grad_max_order` terms.
* `lambda_a=1`: A weight for the running cost `J_a`.
* `grad_J_a`: A function to calculate the gradient of `J_a`. If not given, it
  will be automatically determined. See [`make_grad_J_a`](@ref) for the
  required interface.
* `upper_bound`: An upper bound for the value of any optimized control.
  Time-dependent upper bounds can be specified via `pulse_options`.
* `lower_bound`: A lower bound for the value of any optimized control.
  Time-dependent lower bounds can be specified via `pulse_options`.
* `pulse_options`: A dictionary that maps every control (as obtained by
  [`get_controls`](@ref
  QuantumControl.QuantumPropagators.Controls.get_controls) from the
  `problem.trajectories`) to a dict with the following possible keys:

  - `:upper_bounds`: A vector of upper bound values, one for each intervals of
    the time grid. Values of `Inf` indicate an unconstrained upper bound for
    that time interval, respectively the global `upper_bound`, if given.
  - `:lower_bounds`: A vector of lower bound values. Values of `-Inf` indicate
    an unconstrained lower bound for that time interval,

* `print_iters=true`: Whether to print information after each iteration.
* `store_iter_info=Set()`: Which fields from `print_iters` to store in
  `result.records`. A subset of
  `Set(["iter.", "J_T", "|âˆ‡J_T|", "Î”J_T", "FG(F)", "secs"])`.
* `callback`: A function (or tuple of functions) that receives the
  [GRAPE workspace](@ref GrapeWrk) and the iteration number. The function
  may return a tuple of values which are stored in the
  [`GrapeResult`](@ref) object `result.records`. The function can also mutate
  the workspace, in particular the updated `pulsevals`. This may be used,
  e.g., to apply a spectral filter to the updated pulses or to perform
  similar manipulations. Note that `print_iters=true` (default) adds an
  automatic callback to print information after each iteration. With
  `store_iter_info`, that callback automatically stores a subset of the
  printed information.
* `check_convergence`: A function to check whether convergence has been
  reached. Receives a [`GrapeResult`](@ref) object `result`, and should set
  `result.converged` to `true` and `result.message` to an appropriate string in
  case of convergence. Multiple convergence checks can be performed by chaining
  functions with `âˆ˜`. The convergence check is performed after any `callback`.
* `x_tol`: Parameter for Optim.jl
* `f_tol`: Parameter for Optim.jl
* `g_tol`: Parameter for Optim.jl
* `show_trace`: Parameter for Optim.jl
* `extended_trace`:  Parameter for Optim.jl
* `show_every`: Parameter for Optim.jl
* `allow_f_increases`: Parameter for Optim.jl
* `optimizer`: An optional Optim.jl optimizer (`Optim.AbstractOptimizer`
  instance). If not given, an [L-BFGS-B](https://github.com/Gnimuc/LBFGSB.jl)
  optimizer will be used.
* `prop_method`: The propagation method to use for each trajectory, see below.
* `verbose=false`: If `true`, print information during initialization
* `rethrow_exceptions`: By default, any exception ends the optimization, but
  still returns a [`GrapeResult`](@ref) that captures the message associated
  with the exception. This is to avoid losing results from a long-running
  optimization when an exception occurs in a later iteration. If
  `rethrow_exceptions=true`, instead of capturing the exception, it will be
  thrown normally.

# Trajectory propagation

GRAPE may involve three types of propagation:

* A forward propagation for every [`Trajectory`](@ref) in the `problem`
* A backward propagation for every trajectory
* A backward propagation of a
  [gradient generator](@extref QuantumGradientGenerators.GradGenerator)
  for every trajectory.

The keyword arguments for each propagation (see [`propagate`](@ref)) are
determined from any properties of each [`Trajectory`](@ref) that have a `prop_`
prefix, cf. [`init_prop_trajectory`](@ref).

In situations where different parameters are required for the forward and
backward propagation, instead of the `prop_` prefix, the `fw_prop_` and
`bw_prop_` prefix can be used, respectively. These override any setting with
the `prop_` prefix. Similarly, properties for the backward propagation of the
gradient generators can be set with properties that have a `grad_prop_` prefix.
These prefixes apply both to the properties of each [`Trajectory`](@ref) and
the problem keyword arguments.

Note that the propagation method for each propagation must be specified. In
most cases, it is sufficient (and recommended) to pass a global `prop_method`
problem keyword argument.
"""
optimize(problem, method::Val{:GRAPE}) = optimize_grape(problem)
optimize(problem, method::Val{:grape}) = optimize_grape(problem)

"""
See [`optimize(problem; method=GRAPE, kwargs...)`](@ref optimize(::Any, ::Val{:GRAPE})).
"""
function optimize_grape(problem)
    # TODO: check if x_tol, f_tol, g_tol are used necessary / used correctly
    callback = get(problem.kwargs, :callback, (args...) -> nothing)
    if haskey(problem.kwargs, :update_hook) || haskey(problem.kwargs, :info_hook)
        msg = "The `update_hook` and `info_hook` arguments have been superseded by the `callback` argument"
        throw(ArgumentError(msg))
    end
    check_convergence! = get(problem.kwargs, :check_convergence, res -> res)
    verbose = get(problem.kwargs, :verbose, false)
    gradient_method = get(problem.kwargs, :gradient_method, :gradgen)
    taylor_grad_max_order = get(problem.kwargs, :taylor_grad_max_order, 100)
    taylor_grad_tolerance = get(problem.kwargs, :taylor_grad_tolerance, 1e-16)
    taylor_grad_check_convergence =
        get(problem.kwargs, :taylor_grad_check_convergence, true)

    wrk = GrapeWrk(problem; verbose)

    Î¨â‚€ = [traj.initial_state for traj âˆˆ wrk.trajectories]
    Î¨tgt = Union{eltype(Î¨â‚€),Nothing}[
        (hasproperty(traj, :target_state) ? traj.target_state : nothing) for
        traj âˆˆ wrk.trajectories
    ]

    J = wrk.J_parts
    tlist = wrk.result.tlist
    J_T = wrk.kwargs[:J_T]
    J_a_func = get(wrk.kwargs, :J_a, nothing)
    âˆ‡J_T = wrk.grad_J_T
    Î»â‚ = get(wrk.kwargs, :lambda_a, 1.0)
    chi = wrk.kwargs[:chi]  # guaranteed to exist in `GrapeWrk` constructor
    grad_J_a = nothing
    if !isnothing(J_a_func)
        if haskey(wrk.kwargs, :grad_J_a)
            grad_J_a = wrk.kwargs[:grad_J_a]
        else
            # With a manually given `grad_J_a`, the `make_grad_J_a` function
            # should never be called. So we can't use `get` to set this.
            grad_J_a = make_grad_J_a(J_a_func, tlist)
        end
    end

    Ï„ = wrk.result.tau_vals
    âˆ‡Ï„ = wrk.tau_grads
    N_T = length(tlist) - 1
    N = length(wrk.trajectories)
    L = length(wrk.controls)
    Î¦ = wrk.fw_storage

    # Calculate the functional only; optionally store.
    # Side-effects:
    # set Î¨, Ï„, wrk.result.f_calls, wrk.fg_count wrk.J_parts
    function f(F, G, pulsevals; storage=nothing, count_call=true)
        if pulsevals â‰¢ wrk.pulsevals
            # Ideally, the optimizer uses the original `pulsevals`. LBFGSB
            # does, but Optim.jl does not. Thus, for Optim.jl, we need to copy
            # back the values.
            wrk.pulsevals .= pulsevals
        end
        @assert !isnothing(F)
        @assert isnothing(G)
        if count_call
            wrk.result.f_calls += 1
            wrk.fg_count[2] += 1
        end
        @threadsif wrk.use_threads for k = 1:N
            local Î¦â‚– = isnothing(storage) ? nothing : storage[k]
            reinit_prop!(wrk.fw_propagators[k], Î¨â‚€[k]; transform_control_ranges)
            (Î¦â‚– !== nothing) && write_to_storage!(Î¦â‚–, 1, Î¨â‚€[k])
            for n = 1:N_T  # `n` is the index for the time interval
                local Î¨â‚– = prop_step!(wrk.fw_propagators[k])
                if haskey(wrk.fw_prop_kwargs[k], :callback)
                    local cb = wrk.fw_prop_kwargs[k][:callback]
                    local observables =
                        get(wrk.fw_prop_kwargs[k], :observables, _StoreState())
                    cb(wrk.fw_propagators[k], observables)
                end
                (Î¦â‚– !== nothing) && write_to_storage!(Î¦â‚–, n + 1, Î¨â‚–)
            end
            local Î¨â‚– = wrk.fw_propagators[k].state
            Ï„[k] = isnothing(Î¨tgt[k]) ? NaN : (Î¨tgt[k] â‹… Î¨â‚–)
        end
        Î¨ = [p.state for p âˆˆ wrk.fw_propagators]
        if wrk.J_T_takes_tau
            J[1] = J_T(Î¨, wrk.trajectories; tau=Ï„)
        else
            J[1] = J_T(Î¨, wrk.trajectories)
        end
        if !isnothing(J_a_func)
            J[2] = Î»â‚ * J_a_func(pulsevals, tlist)
        end
        return sum(J)
    end

    # Calculate the functional and the gradient G â‰¡ âˆ‡J_T
    # Side-effects:
    # as in f(...); wrk.grad_J_T, wrk.grad_J_a
    function fg_gradgen!(F, G, pulsevals)

        if !isnothing(DEBUG_FH)  # DEBUG
            println(DEBUG_FH, "# fg_gradgen! in iter $(wrk.result.iter)")
        end
        if isnothing(G)  # functional only
            return f(F, G, pulsevals)
        end
        wrk.result.fg_calls += 1
        wrk.fg_count[1] += 1

        # forward propagation and storage of states
        J_val_guess = sum(wrk.J_parts)
        J_val = f(J_val_guess, nothing, pulsevals; storage=Î¦, count_call=false)

        # backward propagation of combined Ï‡-state and gradient
        Î¨ = [p.state for p âˆˆ wrk.fw_propagators]
        if wrk.chi_takes_tau
            Ï‡ = chi(Î¨, wrk.trajectories; tau=Ï„)  # Ï„ is set in f()
        else
            Ï‡ = chi(Î¨, wrk.trajectories)
        end
        wrk.chi_states = Ï‡  # for easier debugging in a callback
        @threadsif wrk.use_threads for k = 1:N
            local Î¨â‚– = wrk.fw_propagators[k].state  # memory reuse
            local Ï‡Ìƒâ‚– = GradVector(Ï‡[k], length(wrk.controls))
            reinit_prop!(wrk.bw_grad_propagators[k], Ï‡Ìƒâ‚–; transform_control_ranges)
            for n = N_T:-1:1  # N_T is the number of time slices
                Ï‡Ìƒâ‚– = prop_step!(wrk.bw_grad_propagators[k])
                if haskey(wrk.bw_grad_prop_kwargs[k], :callback)
                    local cb = wrk.bw_grad_prop_kwargs[k][:callback]
                    local observables =
                        get(wrk.fw_prop_kwargs[k], :observables, _StoreState())
                    cb(wrk.bw_grad_propagators[k], observables)
                end
                if supports_inplace(Î¨â‚–)
                    get_from_storage!(Î¨â‚–, Î¦[k], n)
                else
                    Î¨â‚– = get_from_storage(Î¦[k], n)
                end
                for l = 1:L
                    âˆ‡Ï„[k][n, l] = Ï‡Ìƒâ‚–.grad_states[l] â‹… Î¨â‚–
                    if !isnothing(DEBUG_FH) && (k == 1) && (l == 1) # DEBUG
                        msg = "n = $(@sprintf("%04d", n)), Î¨â‚–=$(_state(Î¨â‚–)), Ï‡â‚–=$(_state(Ï‡Ìƒâ‚–.state)), Ï‡Ìƒâ‚—â‚–=$(_state(Ï‡Ìƒâ‚–.grad_states[l]; fmt=:exp)) â‡’ âˆ‡Ï„â‚™ = $(_c(âˆ‡Ï„[k][n, l]; fmt=:exp))"
                        println(DEBUG_FH, msg)
                    end
                end
                resetgradvec!(Ï‡Ìƒâ‚–)
                set_state!(wrk.bw_grad_propagators[k], Ï‡Ìƒâ‚–)
            end
        end

        _grad_J_T_via_chi!(âˆ‡J_T, Ï„, âˆ‡Ï„)
        copyto!(G, âˆ‡J_T)
        if !isnothing(grad_J_a)
            wrk.grad_J_a = grad_J_a(pulsevals, tlist)
            axpy!(Î»â‚, wrk.grad_J_a, G)
        end
        return J_val

    end

    # Calculate the functional and the gradient G â‰¡ âˆ‡J_T
    # Side-effects:
    # as in f(...); wrk.grad_J_T, wrk.grad_J_a
    function fg_taylor!(F, G, pulsevals)

        if !isnothing(DEBUG_FH)  # DEBUG
            println(DEBUG_FH, "# fg_taylor! in iter $(wrk.result.iter)")
        end
        if isnothing(G)  # functional only
            return f(F, G, pulsevals)
        end
        wrk.result.fg_calls += 1
        wrk.fg_count[1] += 1

        # forward propagation and storage of states
        J_val_guess = sum(wrk.J_parts)
        J_val = f(J_val_guess, nothing, pulsevals; storage=Î¦, count_call=false)

        # backward propagation of Ï‡-state
        Î¨ = [p.state for p âˆˆ wrk.fw_propagators]
        if wrk.chi_takes_tau
            Ï‡ = chi(Î¨, wrk.trajectories; tau=Ï„)  # Ï„ is set in f()
        else
            Ï‡ = chi(Î¨, wrk.trajectories)
        end
        wrk.chi_states = Ï‡  # for easier debugging in a callback
        @threadsif wrk.use_threads for k = 1:N
            local Î¨â‚– = wrk.fw_propagators[k].state  # memory reuse
            reinit_prop!(wrk.bw_propagators[k], Ï‡[k]; transform_control_ranges)
            local Hâ‚–âº = wrk.adjoint_trajectories[k].generator
            local Hâ‚–â‚™âº = wrk.taylor_genops[k]
            for n = N_T:-1:1  # N_T is the number of time slices
                # TODO: It would be cleaner to encapsulate this in a
                # propagator-like interface that can reuse the gradgen
                # structure instead of the taylor_genops, control_derivs, and
                # taylor_grad_states in wrk
                if ismutable(Î¨â‚–)
                    get_from_storage!(Î¨â‚–, Î¦[k], n)
                else
                    Î¨â‚– = get_from_storage(Î¦[k], n)
                end
                for l = 1:L
                    local Î¼â‚–â‚— = wrk.control_derivs[k][l]
                    if isnothing(Î¼â‚–â‚—)
                        âˆ‡Ï„[k][n, l] = 0.0
                    else
                        local Ïµâ‚™â½â±â¾ = @view pulsevals[(n-1)*L+1:n*L]
                        local vals_dict = IdDict(
                            control => val for (control, val) âˆˆ zip(wrk.controls, Ïµâ‚™â½â±â¾)
                        )
                        local Î¼â‚—â‚–â‚™ = evaluate(Î¼â‚–â‚—, tlist, n; vals_dict)
                        if supports_inplace(Hâ‚–â‚™âº)
                            evaluate!(Hâ‚–â‚™âº, Hâ‚–âº, tlist, n; vals_dict)
                        else
                            Hâ‚–â‚™âº = evaluate(Hâ‚–âº, tlist, n; vals_dict)
                        end
                        local Ï‡â‚– = wrk.bw_propagators[k].state
                        local Ï‡Ìƒâ‚—â‚– = wrk.taylor_grad_states[l, k][1]
                        local Ï•_temp = wrk.taylor_grad_states[l, k][2:5]
                        local dt = tlist[n] - tlist[n+1]
                        @assert dt < 0.0
                        taylor_grad_step!(
                            Ï‡Ìƒâ‚—â‚–,
                            Ï‡â‚–,
                            Hâ‚–â‚™âº,
                            Î¼â‚—â‚–â‚™,
                            dt,
                            Ï•_temp;
                            check_convergence=taylor_grad_check_convergence,
                            max_order=taylor_grad_max_order,
                            tolerance=taylor_grad_tolerance
                        )
                        # TODO: taylor_grad_step for immutable states
                        âˆ‡Ï„[k][n, l] = dot(Ï‡Ìƒâ‚—â‚–, Î¨â‚–)
                        if !isnothing(DEBUG_FH) && (k == 1) && (l == 1) # DEBUG
                            msg = "n = $(@sprintf("%04d", n)), Î¨â‚–=$(_state(Î¨â‚–)), Ï‡â‚–=$(_state(Ï‡â‚–)), Ï‡Ìƒâ‚—â‚–=$(_state(Ï‡Ìƒâ‚—â‚–; fmt=:exp)) â‡’ âˆ‡Ï„â‚™ = $(_c(âˆ‡Ï„[k][n, l]; fmt=:exp))"
                            println(DEBUG_FH, msg)
                        end
                    end
                end
                prop_step!(wrk.bw_propagators[k])
                if haskey(wrk.bw_prop_kwargs[k], :callback)
                    local cb = wrk.bw_prop_kwargs[k][:callback]
                    local observables =
                        get(wrk.bw_prop_kwargs[k], :observables, _StoreState())
                    cb(wrk.bw_propagators[k], observables)
                end
            end
        end

        _grad_J_T_via_chi!(âˆ‡J_T, Ï„, âˆ‡Ï„)
        copyto!(G, âˆ‡J_T)
        if !isnothing(grad_J_a)
            wrk.grad_J_a = grad_J_a(pulsevals, tlist)
            axpy!(Î»â‚, wrk.grad_J_a, G)
        end
        return J_val

    end

    optimizer = wrk.optimizer
    atexit_filename = get(problem.kwargs, :atexit_filename, nothing)
    # atexit_filename is undocumented on purpose: this is considered a feature
    # of @optimize_or_load
    if !isnothing(atexit_filename)
        set_atexit_save_optimization(atexit_filename, wrk.result)
        if !isinteractive()
            @info "Set callback to store result in $(relpath(atexit_filename)) on unexpected exit."
            # In interactive mode, `atexit` is very unlikely, and
            # `InterruptException` is handles via try/catch instead.
        end
    end
    try
        if gradient_method == :gradgen
            run_optimizer(optimizer, wrk, fg_gradgen!, callback, check_convergence!)
        elseif gradient_method == :taylor
            run_optimizer(optimizer, wrk, fg_taylor!, callback, check_convergence!)
        else
            error("Invalid gradient_method=$(repr(gradient_method)) âˆ‰ (:gradgen, :taylor)")
        end
    catch exc
        if get(problem.kwargs, :rethrow_exceptions, false)
            rethrow()
        end
        # Primarily, this is intended to catch Ctrl-C in interactive
        # optimizations (InterruptException)
        exc_msg = sprint(showerror, exc)
        wrk.result.message = "Exception: $exc_msg"
    end

    finalize_result!(wrk)
    if !isnothing(atexit_filename)
        popfirst!(Base.atexit_hooks)
    end

    return wrk.result

end


function run_optimizer end


function update_result!(wrk::GrapeWrk, i::Int64)
    res = wrk.result
    for (k, propagator) in enumerate(wrk.fw_propagators)
        copyto!(res.states[k], propagator.state)
    end
    res.J_T_prev = res.J_T
    res.J_T = wrk.J_parts[1]
    (i > 0) && (res.iter = i)
    if i >= res.iter_stop
        res.converged = true
        res.message = "Reached maximum number of iterations"
        # Note: other convergence checks are done in user-supplied
        # check_convergence routine
    end
    prev_time = res.end_local_time
    res.end_local_time = now()
    res.secs = Dates.toms(res.end_local_time - prev_time) / 1000.0
end


function finalize_result!(wrk::GrapeWrk)
    L = length(wrk.controls)
    res = wrk.result
    res.end_local_time = now()
    N_T = length(res.tlist) - 1
    for l = 1:L
        Ïµ_opt = wrk.pulsevals[(l-1)*N_T+1:l*N_T]
        res.optimized_controls[l] = discretize(Ïµ_opt, res.tlist)
    end
end


make_print_iters(::Val{:GRAPE}; kwargs...) = make_grape_print_iters(; kwargs...)
make_print_iters(::Val{:grape}; kwargs...) = make_grape_print_iters(; kwargs...)


"""Print optimization progress as a table.

This functions serves as the default `info_hook` for an optimization with
GRAPE.
"""
function make_grape_print_iters(; kwargs...)

    headers = ["iter.", "J_T", "|âˆ‡J_T|", "Î”J_T", "FG(F)", "secs"]
    store_iter_info = Set(get(kwargs, :store_iter_info, Set()))
    info_vals = Vector{Any}(undef, length(headers))
    fill!(info_vals, nothing)
    store_iter = false
    store_J_T = false
    store_grad_norm = false
    store_Î”J_T = false
    store_counts = false
    store_secs = false
    for item in store_iter_info
        if item == "iter."
            store_iter = true
        elseif item == "J_T"
            store_J_T = true
        elseif item == "|âˆ‡J_T|"
            store_grad_norm = true
        elseif item == "Î”J_T"
            store_Î”J_T = true
        elseif item == "FG(F)"
            store_counts = true
        elseif item == "secs"
            store_secs = true
        else
            msg = "Item $(repr(item)) in `store_iter_info` is not one of $(repr(headers)))"
            throw(ArgumentError(msg))
        end
    end

    function print_table(wrk, iteration, args...)

        J_T = wrk.result.J_T
        Î”J_T = J_T - wrk.result.J_T_prev
        secs = wrk.result.secs
        grad_norm = norm(wrk.grad_J_T)
        counts = Tuple(wrk.fg_count)

        iter_stop = "$(get(wrk.kwargs, :iter_stop, 5000))"
        width = Dict(
            "iter." => max(length("$iter_stop"), 6),
            "J_T" => 11,
            "|âˆ‡J_T|" => 11,
            "|âˆ‡J_a|" => 11,
            "|âˆ‡J|" => 11,
            "Î”J" => 11,
            "Î”J_T" => 11,
            "FG(F)" => 8,
            "secs" => 8,
        )

        store_iter && (info_vals[1] = iteration)
        store_J_T && (info_vals[2] = J_T)
        store_grad_norm && (info_vals[3] = grad_norm)
        store_Î”J_T && (info_vals[4] = Î”J_T)
        store_counts && (info_vals[5] = counts)
        store_secs && (info_vals[6] = secs)

        if iteration == 0
            for header in headers
                w = width[header]
                print(lpad(header, w))
            end
            print("\n")
        end

        strs = [
            "$iteration",
            @sprintf("%.2e", J_T),
            @sprintf("%.2e", grad_norm),
            (iteration > 0) ? @sprintf("%.2e", Î”J_T) : "n/a",
            @sprintf("%d(%d)", counts[1], counts[2]),
            @sprintf("%.1f", secs),
        ]
        for (str, header) in zip(strs, headers)
            w = width[header]
            print(lpad(str, w))
        end
        print("\n")
        flush(stdout)

        return Tuple((value for value in info_vals if (value !== nothing)))

    end

    return print_table

end


# Gradient for an arbitrary functional evaluated via Ï‡-states.
#
# ```julia
# _grad_J_T_via_chi!(âˆ‡J_T, Ï„, âˆ‡Ï„)
# ```
#
# sets the (vectorized) elements of the gradient `âˆ‡J_T` to the gradient
# ``âˆ‚J_T/âˆ‚Ïµ_{nl}`` for an arbitrary functional ``J_T=J_T(\{|Ï•_k(T)âŸ©\})``, under
# the assumption that
#
# ```math
# \begin{aligned}
#     Ï„_k &= âŸ¨Ï‡_k|Ï•_k(T)âŸ© \quad \text{with} \quad |Ï‡_kâŸ© &= -âˆ‚J_T/âˆ‚âŸ¨Ï•_k(T)|
#     \quad \text{and} \\
#     âˆ‡Ï„_{knl} &= âˆ‚Ï„_k/âˆ‚Ïµ_{nl}\,,
# \end{aligned}
# ```
#
# where ``|Ï•_k(T)âŸ©`` is a state resulting from the forward propagation of some
# initial state ``|Ï•_kâŸ©`` under the pulse values ``Ïµ_{nl}`` where ``l`` numbers
# the controls and ``n`` numbers the time slices. The ``Ï„_k`` are the elements
# of `Ï„` and ``âˆ‡Ï„_{knl}`` corresponds to `âˆ‡Ï„[k][n, l]`.
#
# In this case,
#
# ```math
# (âˆ‡J_T)_{nl} = âˆ‚J_T/âˆ‚Ïµ_{nl} = -2 \Re \sum_k âˆ‡Ï„_{knl}\,.
# ```
#
# Note that the definition of the ``|Ï‡_kâŸ©`` matches exactly the definition of
# the boundary condition for the backward propagation in Krotov's method, see
# [`QuantumControl.Functionals.make_chi`](@ref). Specifically, there is a
# minus sign in front of the derivative, compensated by the minus sign in the
# factor ``(-2)`` of the final ``(âˆ‡J_T)_{nl}``.
function _grad_J_T_via_chi!(âˆ‡J_T, Ï„, âˆ‡Ï„)
    N = length(Ï„) # number of trajectories
    N_T, L = size(âˆ‡Ï„[1])  # number of time intervals/controls
    for l = 1:L
        for n = 1:N_T
            âˆ‡J_T[(l-1)*N_T+n] = real(sum([âˆ‡Ï„[k][n, l] for k = 1:N]))
        end
    end
    lmul!(-2, âˆ‡J_T)
    return âˆ‡J_T
end


# Evaluate `|Î¨ÌƒÌƒ â‰¡ (âˆ‚ exp[-i HÌ‚ dt] / âˆ‚Ïµ) |Î¨âŸ©` with `Î¼Ì‚ = âˆ‚HÌ‚/âˆ‚Ïµ` via an expansion
# into a Taylor series. See Kuprov and Rogers,  J. Chem. Phys. 131, 234108
# (2009), Eq. (20). That equation can be rewritten in a recursive formula
#
# ```math
# |Î¨ÌƒâŸ© = \sum_{n=1}^{âˆ} \frac{(-i dt)^n}{n!} |Î¦â‚™âŸ©
# ```
#
# with
#
# ```math
# \begin{align}
#   |Î¦_1âŸ© &= Î¼Ì‚ |Î¨âŸ©                  \\
#   |Ï•_nâŸ© &= Î¼Ì‚ HÌ‚â¿â»Â¹ |Î¨âŸ© + HÌ‚ |Î¦â‚™â‚‹â‚âŸ©
# \end{align}
# ```
# TODO: this should probably be adapted to static states (avoiding in-place)
function taylor_grad_step!(
    Î¨Ìƒ,
    Î¨,
    HÌ‚,
    Î¼Ì‚,
    dt,           # positive for fw-prop, negative for bw-prop
    temp_states;  # need at least 4 states similar to Î¨
    check_convergence=true,
    max_order=100,
    tolerance=1e-16
)

    Ï•â‚™, Ï•â‚™â‚‹â‚, HÌ‚â¿Î¨, HÌ‚â¿â»Â¹Î¨ = temp_states
    mul!(Ï•â‚™â‚‹â‚, Î¼Ì‚, Î¨)
    mul!(HÌ‚â¿â»Â¹Î¨, HÌ‚, Î¨)
    Î± = -1im * dt
    mul!(Î¨Ìƒ, Î±, Ï•â‚™â‚‹â‚)

    for n = 2:max_order

        mul!(Ï•â‚™, HÌ‚, Ï•â‚™â‚‹â‚)               # matrix-vector product
        mul!(Ï•â‚™, Î¼Ì‚, HÌ‚â¿â»Â¹Î¨, true, true)  # (added) matrix-vector product

        Î± *= -1im * dt / n
        mul!(Î¨Ìƒ, Î±, Ï•â‚™, true, true)      # (scaled) vector-vector sum
        if check_convergence
            r = abs(Î± * norm(Ï•â‚™))
            if r < tolerance
                return Î¨Ìƒ
            end
        end

        mul!(HÌ‚â¿Î¨, HÌ‚, HÌ‚â¿â»Â¹Î¨)             # matrix-vector product
        HÌ‚â¿Î¨, HÌ‚â¿â»Â¹Î¨ = HÌ‚â¿â»Â¹Î¨, HÌ‚â¿Î¨  # swap...
        Ï•â‚™, Ï•â‚™â‚‹â‚ = Ï•â‚™â‚‹â‚, Ï•â‚™      # .... without copy

    end

    if check_convergence && max_order > 1
        # should have returned inside the loop
        error("taylor_grad_step! did not converge within $max_order iterations")
    else
        return Î¨Ìƒ
    end

end


function transform_control_ranges(c, Ïµ_min, Ïµ_max, check)
    if check
        return (min(Ïµ_min, 2 * Ïµ_min), max(Ïµ_max, 2 * Ïµ_max))
    else
        return (min(Ïµ_min, 5 * Ïµ_min), max(Ïµ_max, 5 * Ïµ_max))
    end
end
