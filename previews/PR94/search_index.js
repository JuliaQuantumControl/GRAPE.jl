var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbrüggen and S. J. Glaser. Optimal control of coupled spin dynamics: design of NMR pulse sequences by gradient ascent algorithms. J. Magnet. Res. 172, 296 (2005).\n\n\n\nP. de Fouquières, S. G. Schirmer, S. J. Glaser and I. Kuprov. Second order gradient ascent pulse engineering. J. Magnet. Res. 212, 412 (2011).\n\n\n\nM. H. Goerz, S. C. Carrasco and V. S. Malinovsky. Quantum Optimal Control via Semi-Automatic Differentiation. Quantum 6, 871 (2022).\n\n\n\nH. Tal-Ezer and R. Kosloff. An Accurate and Efficient Scheme for Propagating the Time Dependent Schrödinger Equation. J. Chem. Phys. 81, 3967 (1984).\n\n\n\nD. L. Goodwin and I. Kuprov. Auxiliary matrix formalism for interaction representation transformations, optimal control, and spin relaxation theories. J. Chem. Phys. 143, 084113 (2015).\n\n\n\nM. H. Goerz, D. Basilewitsch, F. Gago-Encinas, M. G. Krauss, K. P. Horn, D. M. Reich and C. P. Koch. Krotov: A Python implementation of Krotov's method for quantum optimal control. SciPost Phys. 7, 080 (2019).\n\n\n\nZ. Tošner, T. Vosegaard, C. Kehlet, N. Khaneja, S. J. Glaser and N. C. Nielsen. Optimal control in NMR spectroscopy: Numerical implementation in SIMPSON. J. Magnet. Res. 197, 120 (2009).\n\n\n\nH. J. Hogben, M. Krzystyniak, G. T. Charnock, P. J. Hore and I. Kuprov. Spinach – A software library for simulation of spin dynamics in large spin systems. J. Magnet. Res. 208, 179 (2011).\n\n\n\nC. A. Ryan and contributors, pulse-finder: Matlab code for GRAPE optimal control in NMR (2013).\n\n\n\nJ. R. Johansson, P. D. Nation and F. Nori. QuTiP 2: A Python framework for the dynamics of open quantum systems. Comput. Phys. Commun. 184, 1234 (2013).\n\n\n\nN. Wittler, F. Roy, K. Pack, M. Werninghaus, A. S. Roy, D. J. Egger, S. Filipp, F. K. Wilhelm and S. Machnes. Integrated Tool Set for Control, Calibration, and Characterization of Quantum Devices Applied to Superconducting Qubits. Phys. Rev. Applied 15, 034080 (2021).\n\n\n\nM. Rossignolo, T. Reisser, A. Marshall, P. Rembold, A. Pagano, P. J. Vetter, R. S. Said, M. M. Müller, F. Motzoi, T. Calarco, F. Jelezko and S. Montangero. QuOCS: The quantum optimal control suite. Comput. Phys. Commun. 291, 108782 (2023).\n\n\n\nM. Zhang, H.-M. Yu, H. Yuan, X. Wang, R. Demkowicz-Dobrzański and J. Liu. QuanEstimation: An open-source toolkit for quantum parameter estimation. Phys. Rev. Research 4, 043057 (2022).\n\n\n\nQDYN: Fortran 95 library and utilities for quantum dynamics and optimal control (2025).\n\n\n\nC. Rackauckas and Q. Nie. DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia. J. Open Res. Softw. 5 (2017).\n\n\n\nM. H. Goerz, E. J. Halperin, J. M. Aytac, C. P. Koch and K. B. Whaley. Robustness of high-fidelity Rydberg gates with single-site addressability. Phys. Rev. A 90, 032329 (2014).\n\n\n\nB. Dash, M. H. Goerz, A. Duspayev, S. C. Carrasco, V. S. Malinovsky and G. Raithel. Rotation sensing using tractor atom interferometry. AVS Quantum Science 6, 014407 (2024).\n\n\n\nJ. P. Palao and R. Kosloff. Optimal control theory for unitary transformations. Phys. Rev. A 68, 062308 (2003).\n\n\n\nB. Kraus and J. I. Cirac. Optimal Creation of Entanglement Using a Two-Qubit Gate. Phys. Rev. A 63, 062309 (2001).\n\n\n\nP. Watts, J. Vala, M. M. Müller, T. Calarco, K. B. Whaley, D. M. Reich, M. H. Goerz and C. P. Koch. Optimizing for an arbitrary perfect entangler: I. Functionals. Phys. Rev. A 91, 062306 (2015).\n\n\n\nM. H. Goerz, G. Gualdi, D. M. Reich, C. P. Koch, F. Motzoi, K. B. Whaley, J. Vala, M. M. Müller, S. Montangero and T. Calarco. Optimizing for an arbitrary perfect entangler. II. Application. Phys. Rev. A 91, 062307 (2015).\n\n\n\nI. Kuprov and C. T. Rodgers. Derivatives of spin dynamics simulations. J. Chem. Phys. 131, 234108 (2009).\n\n\n\nN. Leung, M. Abdelhafez, J. Koch and D. Schuster. Speedup for quantum optimal control from automatic differentiation based on graphics processing units. Phys. Rev. A 95, 042318 (2017). Implementation on GitHub at https://github.com/SchusterLab/quantum-optimal-control.\n\n\n\nM. Abdelhafez, D. I. Schuster and J. Koch. Gradient-based optimal control of open quantum systems using quantum trajectories and automatic differentiation. Phys. Rev. A 99, 052327 (2019).\n\n\n\nM. Abdelhafez, B. Baker, A. Gyenis, P. Mundada, A. A. Houck, D. Schuster and J. Koch. Universal gates for protected superconducting qubits using optimal control. Phys. Rev. A 101, 022321 (2020).\n\n\n\nM. Innes and contributors. Zygote.jl: 21st century AD (2022).\n\n\n\nM. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu and X. Zheng. TensorFlow: A system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16) (2016); p. 265.\n\n\n\nK. B. Petersen and M. S. Pedersen. The Matrix Cookbook (Technical University of Denmark, 2012).\n\n\n\nA. P. Peirce, M. A. Dahleh and H. Rabitz. Optimal control of quantum-mechanical systems: Existence, numerical approximation, and applications. Phys. Rev. A 37, 4950 (1988).\n\n\n\nA. Borzì, G. Stadler and U. Hohenester. Optimal quantum control in nanostructures: Theory and application to a generic three-level system. Phys. Rev. A 66, 053811 (2002).\n\n\n\nU. Hohenester, P. K. Rekdal, A. Borzì and J. Schmiedmayer. Optimal quantum control of Bose-Einstein condensates in magnetic microtraps. Phys. Rev. A 75, 023602 (2007).\n\n\n\nG. Jäger, D. M. Reich, M. H. Goerz, C. P. Koch and U. Hohenester. Optimal quantum control of Bose-Einstein condensates in magnetic microtraps: Comparison of GRAPE and Krotov optimization schemes. Phys. Rev. A 90, 033628 (2014).\n\n\n\nC. F. Van Loan. Computing integrals involving the matrix exponential. IEEE Trans. Automat. Contr. 23, 395 (1978).\n\n\n\nS. G. Schirmer and P. de Fouquieres. Efficient algorithms for optimal control of quantum dynamics: the Krotov method unencumbered. New J. Phys. 13, 073029 (2011).\n\n\n\nJ. P. Palao, R. Kosloff and C. P. Koch. Protecting Coherence in Optimal Control Theory: State-Dependent Constraint Approach. Phys. Rev. A 77, 063412 (2008).\n\n\n\nC. Zhu, R. H. Byrd, P. Lu and J. Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Trans. Math. Softw. 23, 550 (1997).\n\n\n\nY. Qi and contributors. LBFGSB: Julia wrapper for L-BFGS-B Nonlinear Optimization Code (2022).\n\n\n\nM. H. Goerz, D. M. Reich and C. P. Koch. Optimal control theory for a unitary operation under dissipative evolution. New J. Phys. 16, 055012 (2014).\n\n\n\n","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The stable public API of the GRAPE consists of following members:","category":"page"},{"location":"api/","page":"API","title":"API","text":"GRAPE.optimize as the main function to run an optimization\nGRAPE.GrapeResult as the object returned by GRAPE.optimize, and accessible in callbacks\nQuantumControl.optimize with method=GRAPE, has a higher-level wrapper around GRAPE.optimize with extra features\nGRAPE.Trajectory as an alias of  QuantumControl.Trajectory\nGRAPE.set_default_ad_framework as an alias of QuantumControl.set_default_ad_framework","category":"page"},{"location":"api/","page":"API","title":"API","text":"The remaining functions in GRAPE documented below should not be considered part of the stable API. They are guaranteed to be stable in bugfix (x.y.z) releases, but may change in feature releases (x.y).","category":"page"},{"location":"api/","page":"API","title":"API","text":"Note that the GRAPE package does not export any symbols. All members of the public API must be explicitly imported or used with their fully qualified name.","category":"page"},{"location":"api/#api-index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#api-reference","page":"API","title":"Reference","text":"","category":"section"},{"location":"api/#GRAPE.GrapeResult","page":"API","title":"GRAPE.GrapeResult","text":"Result object returned by GRAPE.optimize.\n\nAttributes\n\nThe attributes of a GrapeResult object include\n\niter:  The number of the current iteration\nJ_T: The value of the final-time functional in the current iteration\nJ_T_prev: The value of the final-time functional in the previous iteration\nJ_a: The value of the running cost J_a in the current iteration (excluding λ_a)\nJ_a_prev: The value of J_a in the previous iteration\ntlist: The time grid on which the control are discetized.\nguess_controls: A vector of the original control fields (each field discretized to the points of tlist)\noptimized_controls: A vector of the optimized control fields in the current iterations\nrecords: A vector of tuples with values returned by a callback routine passed to optimize\nconverged: A boolean flag on whether the optimization is converged. This may be set to true by a check_convergence function.\nmessage: A message string to explain the reason for convergence. This may be set by a check_convergence function.\n\nAll of the above attributes may be referenced in a check_convergence function passed to QuantumControl.optimize(problem; method=GRAPE) or GRAPE.optimize.\n\n\n\n\n\n","category":"type"},{"location":"api/#GRAPE.GrapeWrk","page":"API","title":"GRAPE.GrapeWrk","text":"GRAPE Workspace.\n\nThe workspace is for internal use. However, it is also accessible in a callback function. The callback may use or modify some of the following attributes:\n\ntrajectories: a copy of the trajectories defining the control problem\ntlist: the time grid for the optimization\nadjoint_trajectories: The trajectories with the adjoint generator\nkwargs: The keyword arguments from the call to optimize.\ncontrols: A tuple of the original controls (probably functions)\npulsevals_guess: The combined vector of pulse values that are the guess in the current iteration. Initially, the vector is the concatenation of discretizing controls to the midpoints of the time grid.\npulsevals: The combined vector of updated pulse values in the current iteration. All the initialized propagators inside the workspace alias pulsevals such that mutating pulsevals is directly reflected in the next propagation step.\ngradient: The total gradient for the guess in the current iteration\ngrad_J_T: The current  gradient for the final-time part of the functional. This is from the last evaluation of the gradient, which may be for the optimized pulse (depending on the internal of the optimizer)\ngrad_J_a: The current  gradient for the running cost part of the functional.\nJ_parts: The two-component vector J_T J_a\nupper_bounds: Upper bound for every pulsevals; +Inf indicates no bound.\nlower_bounds: Lower bound for every pulsevals; -Inf indicates no bound.\nfg_count: A two-element vector containing the number of evaluations of the combined gradient and functional first, and the evaluations of only the functional second.\noptimizer: The backend optimizer object\noptimizer_state: The internal state object of the optimizer (nothing if the optimizer has no internal state)\nresult: The current result object\ntau_grads: The gradients ∂τₖ/ϵₗ(tₙ)\nfw_storage: The storage of states for the forward propagation, as a vector of storage contains (one for each trajectory)\nfw_propagators: The propagators used for the forward propagation\nbw_grad_propagators: The propagators used for the backward propagation of QuantumGradientGenerators.GradVector states (gradient_method=:gradgen only)\nbw_propagators: The propagators used for the backward propagation (gradient_method=:taylor only)\nuse_threads: Flag indicating whether the propagations are performed in parallel.\n\nIn addition, the following methods provide safer (non-mutating) access to information in the workspace\n\nstep_width\nsearch_direction\nnorm_search\ngradient\npulse_update\n\n\n\n\n\n","category":"type"},{"location":"api/#GRAPE.evaluate_functional-Tuple{Any, Any}","page":"API","title":"GRAPE.evaluate_functional","text":"Evaluate the optimization functional encoded in wrk for the given pulsevals.\n\nJ = evaluate_functional(pulsevals, wrk; storage=nothing, count_call=true)\n\nevaluates the functional defined during the initialization of the GRAPE workspace wrk, for the given pulse values, using wrk.fw_propagators. The pulsevals argument is a vector of Float64 values corresponding to a concatenation of all the controls, discretized to the midpoints of the time grid, cf. GrapeWrk.\n\nAs a side effect, the evaluation sets the following information in wrk:\n\nwrk.pulsevals: On output, the values of the given pulsevals. Note that pulsevals may alias wrk.pulsevals, so there is no assumption made on wrk.pulsevals other than that mutating wrk.pulsevals directly affects the propagators in wrk.\nwrk.result.f_calls: Will be incremented by one (only if count_call=true)\nwrk.fg_count[2]: Will be incremented by one (only if count_call=true)\nwrk.result.tau_vals: For any trajectory that defines a target_state, the overlap of the propagated state with that target state.\nwrk.J_parts: The parts (J_T, λₐJ_a) of the functional\n\nIf storage is given, as a vector of storage containers suitable for propagate (one for each trajectory), the forward-propagated states     will be stored there.\n\nReturns J as sum(wrk.J_parts).\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.evaluate_gradient!-Tuple{Any, Any, Any}","page":"API","title":"GRAPE.evaluate_gradient!","text":"Evaluate the gradient Jϵₙₗ into G, together with the functional J.\n\nJ = evaluate_gradient!(G, pulsevals, wrk)\n\nevaluates and returns the optimization functional defined during the initialization of wrk, for the given pulse values, cf. evaluate_functional, and write the derivative of the optimization functional with respect to the pulse values into the existing array G.\n\nThe evaluation of the functional uses uses wrk.fw_propagators. The evaluation of the gradient happens either via a backward propagation of an extended \"gradient vector\" using wrk.bw_grad_propagators if wrk was initialized with gradient_method=:gradgen. Alternatively, if wrk was initialized with gradient_method=:taylor, the backward propagation if for a regular state, using wrk.bw_propagators, and a Taylor expansion is used for the gradient of the time evolution operator in a single time step.\n\nAs a side, effect, evaluating the gradient and functional sets the following information in wrk:\n\nwrk.pulsevals: On output, the values of the given pulsevals, see evaluate_functional.\nwrk.result.fg_calls: Will be incremented by one\nwrk.fg_count[1]: Will be incremented by one\nwrk.result.tau_vals: For any trajectory that defines a target_state, the overlap of the propagated state with that target state.\nwrk.J_parts: The parts (J_T, λₐJ_a) of the functional\nwrk.fw_storage: For each trajectory, the forward-propagated states at each point on the time grid.\nwrk.chi_states: The normalized states χ(T) that we used as the boundary condition for the backward propagation.\nwrk.chi_states_norm: The original norm of the states χ(T), as calculated by -JΨₖ\nwrk.grad_J_T: The vector ``∂JT/∂ϵ{nl}, i.e., the gradient only for the final-time part of the functional\nwrk.grad_J_a: The vector J_aϵ_nl, i.e., the gradient only for the pulse-dependent running cost.\n\nThe gradients are wrk.grad_J_T and wrk.grad_J_a (weighted by λ_a) into are combined into the output G.\n\nReturns the value of the functional.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.gradient-Tuple{Any}","page":"API","title":"GRAPE.gradient","text":"The gradient in the current iteration.\n\ng = gradient(wrk; which=:initial)\n\nreturns the gradient associated with the guess pulse of the current iteration. Up to quasi-Newton corrections, the negative gradient determines the search_direction for the pulse_update.\n\ng = gradient(wrk; which=:final)\n\nreturns the gradient associated with the optimized pulse of the current iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.make_grape_print_iters-Tuple{}","page":"API","title":"GRAPE.make_grape_print_iters","text":"Print optimization progress as a table.\n\nprint_iters = make_grape_print_iters(; print_iter_info, store_iter_info=[])\n\ngenerates a print_iters function that can be passed as callback to GRAPE.optimize. It is also used automatically when GRAPE.optimized is called via QuantumControl.optimize with print_iters=true.\n\nThe print_iter_info keyword argument specifies what information should be printed, and defaults to [\"iter.\", \"J_T\", \"|∇J|\", \"|Δϵ|\", \"ΔJ\", \"FG(F)\", \"secs\"]. The store_iter_info similarly specifies what information should be returned from the callback, so that it can be stored in the records field of the GrapeResult object.\n\nThe available fields for print_iter_info and store_iter_info are:\n\n\"iter.\": The iteration number\n\"J_T\": The value of the final-time functional for the dynamics under the optimized pulses\n\"J_a\": The value of the pulse-dependent running cost for the optimized pulses\n\"λ_a⋅J_a\": The total contribution of J_a to the full functional J\n\"J\": The value of the optimization functional for the optimized pulses\n\"ǁ∇J_Tǁ\": The ℓ²-norm of the current gradient of the final-time functional. Note that this is usually the gradient of the optimize pulse, not the guess pulse.\n\"ǁ∇J_aǁ\": The ℓ²-norm of the the current gradient of the pulse-dependent running cost. For comparison with \"ǁ∇J_Tǁ\".\n\"λ_aǁ∇J_aǁ\": The ℓ²-norm of the the current gradient of the complete pulse-dependent running cost term. For comparison with \"ǁ∇J_Tǁ\".\n\"ǁ∇Jǁ\": The norm of the guess pulse gradient. Note that the guess pulse gradient is not the same the current gradient.\n\"ǁΔϵǁ\":  The ℓ²-norm of the pulse update\n\"ǁϵǁ\": The ℓ²-norm of optimized pulse values\n\"max|Δϵ|\" The maximum value of the pulse update (infinity norm)\n\"max|ϵ|\": The maximum value of the pulse values (infinity norm)\n\"ǁΔϵǁ/ǁϵǁ\": The ratio of the pulse update tothe optimized pulse values\n\"∫Δϵ²dt\": The L²-norm of the pulse update, summed over all pulses. A convergence measure comparable (proportional) to the running cost in Krotov's method\n\"ǁsǁ\": The norm of the search direction. Should be ǁΔϵǁ scaled by the step with α.\n\"∠°\": The angle (in degrees) between the negative gradient -∇J and the search direction s.\n\"α\": The step width as determined by the line search (Δϵ = α⋅s)\n\"ΔJ_T\": The change in the final time functional relative to the previous iteration\n\"ΔJ_a\":  The change in the control-dependent running cost relative to the previous iteration\n\"λ_a⋅ΔJ_a\": The change in the control-dependent running cost term relative to the previous iteration.\n\"ΔJ\":  The change in the total optimization functional relative to the previous iteration.\n\"FG(F)\":  The number of functional/gradient evaluation (FG), or pure functional (F) evaluations\n\"secs\":  The number of seconds of wallclock time spent on the iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.norm_search-Tuple{Any}","page":"API","title":"GRAPE.norm_search","text":"The norm of the search direction vector in the current iteration.\n\nnorm_search(wrk)\n\nreturns norm(search_direction(wrk)).\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.optimize","page":"API","title":"GRAPE.optimize","text":"Solve a quantum control problem using the GRAPE method.\n\nusing GRAPE\nresult = GRAPE.optimize(trajectories, tlist; J_T, kwargs...)\n\nminimizes a functional\n\nJ(ϵ_nl) = J_T(Ψ_k(T)) + λ_a J_a(ϵ_nl)\n\nvia the GRAPE method, where the final time functional J_T depends explicitly on the forward-propagated states Ψ_k(T), where Ψ_k(t) is the time evolution of the initial_state in the kth' element of the trajectories, and the running cost J_a depends explicitly on pulse values ϵ_nl of the l'th control discretized on the n'th interval of the time grid tlist.\n\nIt does this by calculating the gradient of the final-time functional\n\nnabla J_T equiv fracpartial J_Tpartial ϵ_nl\n= -2 Re\nunderbrace\nunderbracebigglangle χ(T) biggvert hatU^(k)_N_T dots hatU^(k)_n+1 bigg vert_equiv brachi(t_n)text(bw prop)\nfracpartial hatU^(k)_npartial ϵ_nl\n_equiv braχ_k^prime(t_n-1)\nunderbracebigg vert hatU^(k)_n-1 dots hatU^(k)_1 biggvert Ψ_k(t=0) biggrangle_equiv Psi(t_n-1)text(fw prop)\n\nwhere hatU^(k)_n is the time evolution operator for the n the interval, generally assumed to be hatU^(k)_n = exp-i hatH_kn dt_n, where hatH_kn is the operator obtained by evaluating trajectories[k].generator on the n'th time interval.\n\nThe backward-propagation of chi_k(t) has the boundary condition\n\n    chi_k(T) equiv - fracpartial J_Tpartial Psi_k(T)\n\nThe final-time gradient nabla J_T is combined with the gradient for the running costs, and the total gradient is then fed into an optimizer (L-BFGS-B by default) that iteratively changes the values ϵ_nl to minimize J.\n\nSee Background for details.\n\nReturns a GrapeResult.\n\nPositional arguments\n\ntrajectories: A vector of Trajectory objects. Each trajectory contains an initial_state and a dynamical generator (e.g., time-dependent Hamiltonian). Each trajectory may also contain arbitrary additional attributes like target_state to be used in the J_T functional\ntlist: A vector of time grid values.\n\nRequired keyword arguments\n\nJ_T: A function J_T(Ψ, trajectories) that evaluates the final time functional from a list Ψ of forward-propagated states and trajectories. The function J_T may also take a keyword argument tau. If it does, a vector containing the complex overlaps of the target states (target_state property of each trajectory in trajectories) with the propagated states will be passed to J_T.\n\nOptional keyword arguments\n\nchi: A function chi(Ψ, trajectories) that receives a list Ψ of the forward propagated states and returns a vector of states χₖ = -J_TΨₖ. If not given, it will be automatically determined from J_T via QuantumControl.Functionals.make_chi with the default parameters. Similarly to J_T, if chi accepts a keyword argument tau, it will be passed a vector of complex overlaps.\nchi_min_norm=1e-100: The minimum allowable norm for any χₖ(T). Smaller norms would mean that the gradient is zero, and will abort the optimization with an error.\nJ_a: A function J_a(pulsevals, tlist) that evaluates running costs over the pulse values, where pulsevals are the vectorized values ϵ_nl, where n are in indices of the time intervals and l are the indices over the controls, i.e., [ϵ₁₁, ϵ₂₁, …, ϵ₁₂, ϵ₂₂, …] (the pulse values for each control are contiguous). If not given, the optimization will not include a running cost.\ngradient_method=:gradgen: One of :gradgen (default) or :taylor. With gradient_method=:gradgen, the gradient is calculated using QuantumGradientGenerators. With gradient_method=:taylor, it is evaluated via a Taylor series, see Eq. (20) in Kuprov and Rogers,  J. Chem. Phys. 131, 234108 (2009) [22].\ntaylor_grad_max_order=100: If given with gradient_method=:taylor, the maximum number of terms in the Taylor series. If taylor_grad_check_convergence=true (default), if the Taylor series does not convergence within the given number of terms, throw an an error. With taylor_grad_check_convergence=true, this is the exact order of the Taylor series.\ntaylor_grad_tolerance=1e-16: If given with gradient_method=:taylor and taylor_grad_check_convergence=true, stop the Taylor series when the norm of the term falls below the given tolerance. Ignored if taylor_grad_check_convergence=false.\ntaylor_grad_check_convergence=true: If given as true (default), check the convergence after each term in the Taylor series an stop as soon as the norm of the term drops below the given number. If false, stop after exactly taylor_grad_max_order terms.\nlambda_a=1: A weight for the running cost J_a.\ngrad_J_a: A function to calculate the gradient of J_a. If not given, it will be automatically determined. See make_grad_J_a for the required interface.\nupper_bound: An upper bound for the value of any optimized control. Time-dependent upper bounds can be specified via pulse_options.\nlower_bound: A lower bound for the value of any optimized control. Time-dependent lower bounds can be specified via pulse_options.\npulse_options: A dictionary that maps every control (as obtained by get_controls from the trajectories) to a dict with the following possible keys:\n:upper_bounds: A vector of upper bound values, one for each intervals of the time grid. Values of Inf indicate an unconstrained upper bound for that time interval, respectively the global upper_bound, if given.\n:lower_bounds: A vector of lower bound values. Values of -Inf indicate an unconstrained lower bound for that time interval,\ncallback: A function that receives the GRAPE workspace and the iteration number. The function may return a tuple of values which are stored in the GrapeResult object result.records. The function can also mutate the workspace, in particular the updated pulsevals. This may be used, e.g., to apply a spectral filter to the updated pulses or to perform similar manipulations.\ncheck_convergence: A function to check whether convergence has been reached. Receives a GrapeResult object result, and must return one of the following:\nA boolean (true if convergence is reached, false otherwise)\nA string with a reason for the convergence, or an empty string if not converged.\nThe original result object or nothing, indicating that result.converged and   result.message may have been modified to indicate convergence\nThe convergence check is performed after any callback.\nprop_method: The propagation method to use for each trajectory, see below.\nverbose=false: If true, print information during initialization\nrethrow_exceptions: By default, any exception ends the optimization, but still returns a GrapeResult that captures the message associated with the exception. This is to avoid losing results from a long-running optimization when an exception occurs in a later iteration. If rethrow_exceptions=true, instead of capturing the exception, it will be thrown normally.\n\nExperimental keyword arguments\n\nThe following keyword arguments may change in non-breaking releases:\n\nx_tol: Parameter for Optim.jl\nf_tol: Parameter for Optim.jl\ng_tol: Parameter for Optim.jl\nshow_trace: Parameter for Optim.jl\nextended_trace:  Parameter for Optim.jl\nshow_every: Parameter for Optim.jl\nallow_f_increases: Parameter for Optim.jl\noptimizer: An optional Optim.jl optimizer (Optim.AbstractOptimizer instance). If not given, an L-BFGS-B optimizer will be used.\n\nTrajectory propagation\n\nGRAPE may involve three types of time propagation, all of which are implemented via the QuantumPropagators as a numerical backend:\n\nA forward propagation for every Trajectory in the trajectories\nA backward propagation for every trajectory\nA backward propagation of a gradient generator for every trajectory.\n\nThe keyword arguments for each propagation (see propagate) are determined from any properties of each Trajectory that have a prop_ prefix, cf. init_prop_trajectory.\n\nIn situations where different parameters are required for the forward and backward propagation, instead of the prop_ prefix, the fw_prop_ and bw_prop_ prefix can be used, respectively. These override any setting with the prop_ prefix. Similarly, properties for the backward propagation of the gradient generators can be set with properties that have a grad_prop_ prefix. These prefixes apply both to the properties of each Trajectory and the keyword arguments.\n\nNote that the propagation method for each propagation must be specified. In most cases, it is sufficient (and recommended) to pass a global prop_method keyword argument.\n\n\n\n\n\n","category":"function"},{"location":"api/#GRAPE.pulse_update-Tuple{Any}","page":"API","title":"GRAPE.pulse_update","text":"The vector of pulse update values for the current iteration.\n\nΔu = pulse_update(wrk)\n\nreturns a vector containing the different between the optimized pulse values and the guess pulse values of the current iteration. This should be proportional to search_direction with the proportionality factor step_width.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.search_direction-Tuple{Any}","page":"API","title":"GRAPE.search_direction","text":"The search direction used in the current iteration.\n\ns = search_direction(wrk)\n\nreturns the vector describing the search direction used in the current iteration. This should be proportional to pulse_update with the proportionality factor step_width.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.step_width-Tuple{Any}","page":"API","title":"GRAPE.step_width","text":"The step width used in the current iteration.\n\nα = step_width(wrk)\n\nreturns the scalar α so that pulse_update(wrk) = α * search_direction(wrk), see pulse_update and search_direction for the iteration described by the current GrapeWrk (for the state of wrk as available in the callback of the current iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.vec_angle-Union{Tuple{P}, Tuple{T}, Tuple{N}, Tuple{P, P}} where {N, T, P<:Union{AbstractVector{T}, NTuple{N, T}}}","page":"API","title":"GRAPE.vec_angle","text":"The angle between two vectors.\n\nϕ = vec_angle(v1, v2; unit=:rad)\n\nreturns the angle between two vectors in radians (or degrees, with unit=:degree).\n\n\n\n\n\n","category":"method"},{"location":"api/#QuantumControl.optimize-Tuple{Any, Val{:GRAPE}}","page":"API","title":"QuantumControl.optimize","text":"using GRAPE\nresult = optimize(problem; method=GRAPE, kwargs...)\n\noptimizes the given QuantumControl.ControlProblem using the GRAPE (Gradient-Ascent Pulse Engineering) method.\n\nDelegates to\n\nresult = GRAPE.optimize(\n    problem.trajectories, problem.tlist; problem.kwargs..., kwargs...\n)\n\nSee GRAPE.optimize for details and supported keyword arguments.\n\nCompared to calling GRAPE.optimize directly, the QuantumControl.optimize wrapper adds the following additional keyword arguments:\n\ncheck=true: If true (default), test that all the objects stored in the trajectories implement the required interfaces correctly\nprint_iters=true: Whether to print information after each iteration.\nprint_iter_info=[\"iter.\", \"J_T\", \"|∇J|\", \"|Δϵ|\", \"ΔJ\", \"FG(F)\", \"secs\"]: Which fields to print if print_iters=true. See make_grape_print_iters\nstore_iter_info=[]: Which fields to store in result.records, given as a list of header labels, see print_iter_info. See make_grape_print_iters\n\nThese options still allow for the normal callback argument. With QuantumcControl.optimize, the callback can be a tuple of callback functions that will be combined automatically, which GRAPE.optimize only supports as single callback function.\n\nThe GRAPE optimization may also be initiated via QuantumControl.@optimize_or_load, which additionally adds checkpointing, to ensure that an optimization result is dumped to disk in case of an unexpected shutdown.\n\n\n\n\n\n","category":"method"},{"location":"api/#QuantumControl.Trajectory","page":"API","title":"QuantumControl.Trajectory","text":"Description of a state's time evolution.\n\nTrajectory(\n    initial_state,\n    generator;\n    target_state=nothing,\n    weight=1.0,\n    kwargs...\n)\n\ndescribes the time evolution of the initial_state under a time-dependent dynamical generator (e.g., a Hamiltonian or Liouvillian).\n\nTrajectories are central to quantum control problems: an optimization functional depends on the result of propagating one or more trajectories. For example, when optimizing for a quantum gate, the optimization considers the trajectories of all logical basis states.\n\nIn addition to the initial_state and generator, a Trajectory may include data relevant to the propagation and to evaluating a particular optimization functional. Most functionals have the notion of a \"target state\" that the initial_state should evolve towards, which can be given as the target_state keyword argument. In some functionals, different trajectories enter with different weights [38], which can be given as a weight keyword argument. Any other keyword arguments are also available to a functional as properties of the Trajectory .\n\nA Trajectory can also be instantiated using all keyword arguments.\n\nProperties\n\nAll keyword arguments used in the instantiation are available as properties of the Trajectory. At a minimum, this includes initial_state, generator, target_state, and weight.\n\nBy convention, properties with a prop_ prefix, e.g., prop_method, will be taken into account when propagating the trajectory. See propagate_trajectory for details.\n\n\n\n\n\n","category":"type"},{"location":"api/#QuantumControl.set_default_ad_framework","page":"API","title":"QuantumControl.set_default_ad_framework","text":"Set the default provider for automatic differentiation.\n\nQuantumControl.set_default_ad_framework(mod; quiet=false)\n\nregisters the given module (package) as the default AD framework.\n\nThis determines the default setting for the automatic parameter in the following functions:\n\nQuantumControl.Functionals.make_chi\nQuantumControl.Functionals.make_gate_chi\nQuantumControl.Functionals.make_grad_J_a\n\nThe given mod must be a supported AD framework, e.g.,\n\nimport Zygote\nQuantumControl.set_default_ad_framework(Zygote)\n\nCurrently, there is built-in support for Zygote and FiniteDifferences.\n\nFor other packages to be used as the default AD framework, the appropriate methods for make_chi etc. must be defined.\n\nUnless quiet=true, calling set_default_ad_framework will show a message to confirm the setting.\n\nTo unset the default AD framework, use\n\nQuantumControl.set_default_ad_framework(nothing)\n\n\n\n\n\n","category":"function"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"The GRAPE package is best used via the interface provided by the QuantumControl framework, see the Relation to the QuantumControl Framework. It helps to be familiar with the concepts used in the framework and its overview.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"The package can also be used standalone, as illustrated in the previous Tutorial, and encapsulated in the API of the GRAPE.optimize function:","category":"page"},{"location":"usage/#GRAPE.optimize-usage","page":"Usage","title":"GRAPE.optimize","text":"Solve a quantum control problem using the GRAPE method.\n\nusing GRAPE\nresult = GRAPE.optimize(trajectories, tlist; J_T, kwargs...)\n\nminimizes a functional\n\nJ(ϵ_nl) = J_T(Ψ_k(T)) + λ_a J_a(ϵ_nl)\n\nvia the GRAPE method, where the final time functional J_T depends explicitly on the forward-propagated states Ψ_k(T), where Ψ_k(t) is the time evolution of the initial_state in the kth' element of the trajectories, and the running cost J_a depends explicitly on pulse values ϵ_nl of the l'th control discretized on the n'th interval of the time grid tlist.\n\nIt does this by calculating the gradient of the final-time functional\n\nnabla J_T equiv fracpartial J_Tpartial ϵ_nl\n= -2 Re\nunderbrace\nunderbracebigglangle χ(T) biggvert hatU^(k)_N_T dots hatU^(k)_n+1 bigg vert_equiv brachi(t_n)text(bw prop)\nfracpartial hatU^(k)_npartial ϵ_nl\n_equiv braχ_k^prime(t_n-1)\nunderbracebigg vert hatU^(k)_n-1 dots hatU^(k)_1 biggvert Ψ_k(t=0) biggrangle_equiv Psi(t_n-1)text(fw prop)\n\nwhere hatU^(k)_n is the time evolution operator for the n the interval, generally assumed to be hatU^(k)_n = exp-i hatH_kn dt_n, where hatH_kn is the operator obtained by evaluating trajectories[k].generator on the n'th time interval.\n\nThe backward-propagation of chi_k(t) has the boundary condition\n\n    chi_k(T) equiv - fracpartial J_Tpartial Psi_k(T)\n\nThe final-time gradient nabla J_T is combined with the gradient for the running costs, and the total gradient is then fed into an optimizer (L-BFGS-B by default) that iteratively changes the values ϵ_nl to minimize J.\n\nSee Background for details.\n\nReturns a GrapeResult.\n\nPositional arguments\n\ntrajectories: A vector of Trajectory objects. Each trajectory contains an initial_state and a dynamical generator (e.g., time-dependent Hamiltonian). Each trajectory may also contain arbitrary additional attributes like target_state to be used in the J_T functional\ntlist: A vector of time grid values.\n\nRequired keyword arguments\n\nJ_T: A function J_T(Ψ, trajectories) that evaluates the final time functional from a list Ψ of forward-propagated states and trajectories. The function J_T may also take a keyword argument tau. If it does, a vector containing the complex overlaps of the target states (target_state property of each trajectory in trajectories) with the propagated states will be passed to J_T.\n\nOptional keyword arguments\n\nchi: A function chi(Ψ, trajectories) that receives a list Ψ of the forward propagated states and returns a vector of states χₖ = -J_TΨₖ. If not given, it will be automatically determined from J_T via QuantumControl.Functionals.make_chi with the default parameters. Similarly to J_T, if chi accepts a keyword argument tau, it will be passed a vector of complex overlaps.\nchi_min_norm=1e-100: The minimum allowable norm for any χₖ(T). Smaller norms would mean that the gradient is zero, and will abort the optimization with an error.\nJ_a: A function J_a(pulsevals, tlist) that evaluates running costs over the pulse values, where pulsevals are the vectorized values ϵ_nl, where n are in indices of the time intervals and l are the indices over the controls, i.e., [ϵ₁₁, ϵ₂₁, …, ϵ₁₂, ϵ₂₂, …] (the pulse values for each control are contiguous). If not given, the optimization will not include a running cost.\ngradient_method=:gradgen: One of :gradgen (default) or :taylor. With gradient_method=:gradgen, the gradient is calculated using QuantumGradientGenerators. With gradient_method=:taylor, it is evaluated via a Taylor series, see Eq. (20) in Kuprov and Rogers,  J. Chem. Phys. 131, 234108 (2009) [22].\ntaylor_grad_max_order=100: If given with gradient_method=:taylor, the maximum number of terms in the Taylor series. If taylor_grad_check_convergence=true (default), if the Taylor series does not convergence within the given number of terms, throw an an error. With taylor_grad_check_convergence=true, this is the exact order of the Taylor series.\ntaylor_grad_tolerance=1e-16: If given with gradient_method=:taylor and taylor_grad_check_convergence=true, stop the Taylor series when the norm of the term falls below the given tolerance. Ignored if taylor_grad_check_convergence=false.\ntaylor_grad_check_convergence=true: If given as true (default), check the convergence after each term in the Taylor series an stop as soon as the norm of the term drops below the given number. If false, stop after exactly taylor_grad_max_order terms.\nlambda_a=1: A weight for the running cost J_a.\ngrad_J_a: A function to calculate the gradient of J_a. If not given, it will be automatically determined. See make_grad_J_a for the required interface.\nupper_bound: An upper bound for the value of any optimized control. Time-dependent upper bounds can be specified via pulse_options.\nlower_bound: A lower bound for the value of any optimized control. Time-dependent lower bounds can be specified via pulse_options.\npulse_options: A dictionary that maps every control (as obtained by get_controls from the trajectories) to a dict with the following possible keys:\n:upper_bounds: A vector of upper bound values, one for each intervals of the time grid. Values of Inf indicate an unconstrained upper bound for that time interval, respectively the global upper_bound, if given.\n:lower_bounds: A vector of lower bound values. Values of -Inf indicate an unconstrained lower bound for that time interval,\ncallback: A function that receives the GRAPE workspace and the iteration number. The function may return a tuple of values which are stored in the GrapeResult object result.records. The function can also mutate the workspace, in particular the updated pulsevals. This may be used, e.g., to apply a spectral filter to the updated pulses or to perform similar manipulations.\ncheck_convergence: A function to check whether convergence has been reached. Receives a GrapeResult object result, and must return one of the following:\nA boolean (true if convergence is reached, false otherwise)\nA string with a reason for the convergence, or an empty string if not converged.\nThe original result object or nothing, indicating that result.converged and   result.message may have been modified to indicate convergence\nThe convergence check is performed after any callback.\nprop_method: The propagation method to use for each trajectory, see below.\nverbose=false: If true, print information during initialization\nrethrow_exceptions: By default, any exception ends the optimization, but still returns a GrapeResult that captures the message associated with the exception. This is to avoid losing results from a long-running optimization when an exception occurs in a later iteration. If rethrow_exceptions=true, instead of capturing the exception, it will be thrown normally.\n\nExperimental keyword arguments\n\nThe following keyword arguments may change in non-breaking releases:\n\nx_tol: Parameter for Optim.jl\nf_tol: Parameter for Optim.jl\ng_tol: Parameter for Optim.jl\nshow_trace: Parameter for Optim.jl\nextended_trace:  Parameter for Optim.jl\nshow_every: Parameter for Optim.jl\nallow_f_increases: Parameter for Optim.jl\noptimizer: An optional Optim.jl optimizer (Optim.AbstractOptimizer instance). If not given, an L-BFGS-B optimizer will be used.\n\nTrajectory propagation\n\nGRAPE may involve three types of time propagation, all of which are implemented via the QuantumPropagators as a numerical backend:\n\nA forward propagation for every Trajectory in the trajectories\nA backward propagation for every trajectory\nA backward propagation of a gradient generator for every trajectory.\n\nThe keyword arguments for each propagation (see propagate) are determined from any properties of each Trajectory that have a prop_ prefix, cf. init_prop_trajectory.\n\nIn situations where different parameters are required for the forward and backward propagation, instead of the prop_ prefix, the fw_prop_ and bw_prop_ prefix can be used, respectively. These override any setting with the prop_ prefix. Similarly, properties for the backward propagation of the gradient generators can be set with properties that have a grad_prop_ prefix. These prefixes apply both to the properties of each Trajectory and the keyword arguments.\n\nNote that the propagation method for each propagation must be specified. In most cases, it is sufficient (and recommended) to pass a global prop_method keyword argument.\n\n\n\n\n\n","category":"function"},{"location":"usage/#Relation-to-the-QuantumControl-Framework","page":"Usage","title":"Relation to the QuantumControl Framework","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"The GRAPE package is associated with the broader QuantumControl framework. The role of QuantumControl in relation to GRAPE has two aspects:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"QuantumControl provides a collection of components that are useful for formulating control problems in general, for solution via GRAPE or arbitrary other methods of quantum control. This includes, for example, control functions and control amplitudes, data structures for time-dependent Hamiltonians or Liouvillians, or common optimization functionals.\nQuantumControl provides a common way to formulate a ControlProblem and general optimize and @optimize_or_load functions that particular optimization packages like GRAPE can plug in to. The aim is to encourage a common interface between different optimization packages that makes it easy to switch between different methods.","category":"page"},{"location":"usage/#QuantumControl.optimize-Tuple{ControlProblem, Val{:GRAPE}}-usage","page":"Usage","title":"QuantumControl.optimize","text":"using GRAPE\nresult = optimize(problem; method=GRAPE, kwargs...)\n\noptimizes the given QuantumControl.ControlProblem using the GRAPE (Gradient-Ascent Pulse Engineering) method.\n\nDelegates to\n\nresult = GRAPE.optimize(\n    problem.trajectories, problem.tlist; problem.kwargs..., kwargs...\n)\n\nSee GRAPE.optimize for details and supported keyword arguments.\n\nCompared to calling GRAPE.optimize directly, the QuantumControl.optimize wrapper adds the following additional keyword arguments:\n\ncheck=true: If true (default), test that all the objects stored in the trajectories implement the required interfaces correctly\nprint_iters=true: Whether to print information after each iteration.\nprint_iter_info=[\"iter.\", \"J_T\", \"|∇J|\", \"|Δϵ|\", \"ΔJ\", \"FG(F)\", \"secs\"]: Which fields to print if print_iters=true. See make_grape_print_iters\nstore_iter_info=[]: Which fields to store in result.records, given as a list of header labels, see print_iter_info. See make_grape_print_iters\n\nThese options still allow for the normal callback argument. With QuantumcControl.optimize, the callback can be a tuple of callback functions that will be combined automatically, which GRAPE.optimize only supports as single callback function.\n\nThe GRAPE optimization may also be initiated via QuantumControl.@optimize_or_load, which additionally adds checkpointing, to ensure that an optimization result is dumped to disk in case of an unexpected shutdown.\n\n\n\n\n\n","category":"method"},{"location":"background/#GRAPE-Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"We review here in detail the GRAPE method as it is implemented in GRAPE.jl. Much of the material is adapted from Goerz et al., Quantum 6, 871 (2022).","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Contents","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Pages=[\"background.md\"]\nDepth=2:3","category":"page"},{"location":"background/#Introduction","page":"Background","title":"Introduction","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The GRAPE method minimizes an optimization functional of the form","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrape-functional\nJ(ϵ_l(t))\n    = J_T(Ψ_k(T))\n    + λ_a  underbrace_l int_0^T g_a(ϵ_l(t))  dt_=J_a(ϵ_l(t))\n    + λ_b  underbrace_k int_0^T g_b(Ψ_k(t))  dt_=J_b(Ψ_k(t))\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"where ϵ_l(t) is a set of control functions defined between the initial time t=0 and the final time t=T, and Ψ_k(t) is a set of \"trajectories\" evolving from a set of initial states Psi_k(t=0) under the controls ϵ_l(t). The primary focus is on the final-time functional J_T, but running costs J_a (weighted by λ_a) may be included to penalize certain features of the control field. In principle, a state-dependent running cost J_b weighted by λ_b can also be included (and will be discussed below), although this is currently not fully implemented in GRAPE.jl.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The defining assumptions of the GRAPE method are","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The control fields epsilon_l(t) are piecewise constant on the N_T intervals of a time grid t_0 = 0 t_1 dots t_N_T = T. That is, we have a vector of pulse values with elements epsilon_nl. We use the double-index nl, for the value of the l'th control field on the n'th interval of the time grid. When the initial controls are time-continuous, for the purposes of GRAPE.jl, they are first discretized via QuantumPropagators.Controls.discretize_on_midpoints. The optimized fields are always vectors of pulse values.\nThe states Psi_k(t) evolve under an equation of motion of the form\nbeginequationlabeleqtdse\n    i hbar fracpartial Psi_k(t)partial t = hatH_k(epsilon_l(t)) Psi_k(t)\nendequation\nwith hbar = 1 (we will omit hbar in all subsequent equations).\nThis includes the Schrödinger equation, but also the Liouville equation for open quantum systems. In the latter case, Psi_k is replaced by a vectorized density matrix, and hatH_k is replaced by a Liouvillian (super-) operator describing the dynamics of the k'th trajectory. The crucial point is that Eq. \\eqref{eq:tdse} can be solved within each time interval as\nbeginequationlabeleqtime-evolution-op\n    defiimathrmi\n    Psi_k(t_n+1) = underbraceexpleft-ii hatH_kn dt_n right_=hatU^(k)_n Psi_k(t_n)\nendequation\nwhere hatH_kn = hatH_k(epsilon_nl) is hatH_k(epsilon_l(t)) evaluated at the midpoint of the n'th interval (respectively at t=0 and t=T for n=1 and n=N_T), and with the time step dt_n = (t_n - t_n-1).","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"These two assumptions allow to analytically derive the gradient (nabla J)_nl equiv fracpartial Jpartial epsilon_nl. The initial derivation of GRAPE by Khaneja et al. [1] focuses on a final-time functional J_T that depends on the overlap of each forward-propagated Psi_k(T) with a target state Psi^texttgt_k(T) and updates the pulse values epsilon_nl directly in the direction of the negative gradient. Improving on this, de Fouquières et al. [2] showed that using a quasi-Newton method to update the pulses based on the gradient information leads to a dramatic improvement in convergence and stability. Furthermore, Goodwin and Kuprov [5] improved on the precision of evaluating the gradient of a local time evolution operator, which is a critical step in the GRAPE scheme. Finally, Goerz et al. [3] generalized GRAPE to arbitrary functionals of the form \\eqref{eq:grape-functional}, bridging the gap to automatic differentiation techniques [23–25] by introducing the idea of \"semi-automatic differentiation\". This most general derivation is the basis for the implementation in GRAPE.jl.","category":"page"},{"location":"background/#tmidr","page":"Background","title":"Too Many Indices; Didn't Read","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"tip: TMIDR\nBelow, we derive the GRAPE scheme in full generality. This implies keeping track of a lot of indices:k: the index over the different trajectories, i.e., the states Psi_k(t) whose time evolution contributes to the functional\nl: the index over the different control functions epsilon_l(t) that the Hamiltonian/Liouvillian may depend on\nn: The index over the intervals of the time gridMost equations can be simplified by not worrying about k or l: If there are multiple controls, they are concatenated into a single vector of control values with a double-index nl. We really only need to keep track of n; the gradient values related to an epsilon_nl with a particular l are somewhat obviously obtained by using a particular epsilon_l(t_n). Likewise, all trajectories contribute equally to the gradients, so we just have a sum over the k index.We can further simplify by considering only final-time functionals J_T(Psi_k(T)). Running costs J_a(ϵ_l(t)) are quite straightforward to add (just take the derivative w.r.t. the values ϵ_nl), and running costs J_b(Ψ_k(t)) are too complicated to consider in any kind of \"simplified\" scheme.In essence, then, the GRAPE scheme that is implemented here can be concisely summarized asbeginequation\nfracpartial J_Tpartial epsilon_n\n= -2 Re\n   underbrace\n       underbracebigglangle chi(T) biggvert hatU_N_T dots hatU_n+1 bigg vert_equiv brachi(t_n)text(bw prop)\n       fracpartial hatU_npartial epsilon_n\n   _equiv brachi^prime(t_n-1)\n   underbracebigg vert hatU_n-1 dots hatU_1 biggvert Psi(t=0) biggrangle_equiv Psi(t_n-1)text(fw prop)\nendequationcf. Eq. \\eqref{eq:grad-at-T-U}, with the boundary condition, cf. Eq. \\eqref{eq:chi},beginequation\n    chi(T) equiv - fracpartial J_Tpartial Psi(T)\nendequationThe gradient-state chi^prime(t_n-1) is obtained either via an expansion of hatU_n into a Taylor series, or (by default), by backward-propagating an extended state tildechi(t) with gradient information [5]. The resulting scheme is illustrated in Fig. 1.","category":"page"},{"location":"background/#Prerequisite:-Wirtinger-derivatives-and-matrix-calculus","page":"Background","title":"Prerequisite: Wirtinger derivatives and matrix calculus","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Even though we are seeking the derivative of the real-valued functional J with respect to the real-valued parameter epsilon_nl, the functional still involves complex quantities via Psi_k(t) and hatH in Eq. \\eqref{eq:tdse}. In order to apply the chain rule in the derivation of the gradient, we will have to clarify the notion of derivatives in the context of complex numbers, as well as derivatives with respect to vectors (\"matrix calculus\").","category":"page"},{"location":"background/#Wirtinger-Scalars","page":"Background","title":"Derivatives w.r.t. complex scalars","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"To illustrate, let's say we introduce intermediary scalar variables z_k in mathbbC in the functional, J(epsilon_nl) rightarrow J(z_k(epsilon_nl)), with J epsilon_nl in mathbbR.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In principle, one must separate the z_k into real and imaginary parts as independent variables, J = J(Rez_k Imz_k), resulting in","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  labeleqgrad_zj_real_imag\n  (nabla J)_nl\n  equiv fracpartial Jpartial epsilon_nl\n  = sum_k left(\n    fracpartial Jpartial Rez_k\n    fracpartial Rez_kpartial epsilon_nl\n    + fracpartial Jpartial Imz_k\n    fracpartial Imz_kpartial epsilon_nl\n    right)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"An elegant alternative is to introduce Wirtinger derivatives,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginalign\n  labeleqwirtinger1\n  fracpartial Jpartial z_k\n    equiv frac12 left(\n      fracpartial Jpartial Rez_k\n      -ii fracpartial Jpartial Imz_k\n      right) \n  labeleqwirtinger2\n  fracpartial Jpartial z_k^*\n    equiv frac12 left(\n      fracpartial Jpartial Rez_k\n      +ii fracpartial Jpartial Imz_k\n      right)\n    = left(fracpartial Jpartial z_kright)^*\nendalign","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"which instead treats z_k and the conjugate value z_k^* as independent variables, so that","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  labeleqwirtinger-chainrule\n  fracpartial Jpartial epsilon_nl\n  = sum_k left(\n    fracpartial Jpartial z_k\n    fracpartial z_kpartial epsilon_nl\n    + fracpartial Jpartial z_k^*\n    fracpartial z_k^*partial epsilon_nl\n    right)\n  = 2 Re sum_k fracpartial Jpartial z_k\n    fracpartial z_kpartial epsilon_nl\n    \nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"So, we have a simple chain rule, modified only by 2 Re, where we can otherwise \"forget\" that z_k is a complex variable. The fact that J is real-valued guarantees that z_k and z_k^* can only occur in such ways that we don't have to worry about having \"lost\" z_k^*.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The derivative of the complex value z_k with respect to the real value epsilon_nl is defined straightforwardly as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  fracpartial z_kpartial epsilon_nl\n  equiv\n  fracpartial Rez_kpartial epsilon_nl\n  + ii fracpartial Imz_kpartial epsilon_nl\nendequation","category":"page"},{"location":"background/#Wirtinger-Vectors","page":"Background","title":"Derivatives w.r.t. complex vectors","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"We can now go one step further and allow for intermediate variables that are complex vectors instead of scalars, J(epsilon_nl) rightarrow J(Psi_k(epsilon_nl)). Taking the derivative w.r.t. a vector puts us in the domain of matrix calculus. Fundamentally, the derivative of a scalar with respect to a (column) vector is a (row) vector consisting of the derivatives of the scalar w.r.t. the components of the vector, and the derivative of a vector w.r.t. a scalar is the obvious vector of derivatives.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Usually, matrix calculus assumes real-valued vectors, but the extension to complex vectors via the Wirtinger derivatives discussed above is relatively straightforward. The use of Dirac (\"braket\") notation helps tremendously here: Psi_k describes a complex column vector, and Psi_k describes the corresponding row vector with complex-conjugated elements. These can take the place of z_k and z_k^* in the Wirtinger derivative. Consider, e.g.,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqJsm\nJ(Psi_k)\n= sum_k vert langle Psi_k vert Psi_k^texttgt rangle vert^2\n= sum_k langle Psi_k vert Psi_k^texttgt rangle langle Psi_k^texttgt vert Psi_k rangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"for a fixed set of \"target states\" Psi_k^texttgt.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The derivative partial Jpartial Psi_k is","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqdJ_dKet\nfracpartial Jpartial Psi_k = langle Psi_k vert Psi_k^texttgt rangle langlePsi_k^texttgtvert\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"in the same sense as Eq. \\eqref{eq:wirtinger1}. We simply treat Psi_k and Psi_k as independent variables corresponding to z_k and z_k^*. Note that the result is a \"bra\", that is, a co-state, or row vector. The braket notation resolves the question of \"layout conventions\" in matrix calculus in favor of the \"numerator layout\". Consequently, we also have a well-defined derivative w.r.t. the co-state:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\nfracpartial Jpartial Psi_k = langle Psi_k^texttgt vert Psi_k rangle vertPsi_k^texttgtrangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"which we can either get explicitly from Eq. \\eqref{eq:Jsm}, differentiating w.r.t. Psi_k as an independent variable (and reordering the terms), or implicitly by taking the conjugate transpose of Eq. \\eqref{eq:dJ_dKet}.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"For the full chain rule of a functional J(Psi_k(epsilon_nl)), we thus find","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrad-via-chi1\n  (nabla J)_nl\n  equiv fracpartial Jpartial epsilon_nl\n  = 2 Re sum_k left(\n    fracpartial Jpartial Psi_k\n    fracpartial Psi_kpartial epsilon_nl\n  right)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"With the definition in Eq. \\eqref{eq:wirtinger1}, this corresponds directly to the scalar","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  fracpartial Jpartial epsilon_nl\n  = sum_km left(\n    fracpartial Jpartial RePsi_km\n    fracpartial RePsi_kmpartial epsilon_nl\n    + fracpartial Jpartial ImPsi_km\n    fracpartial ImPsi_kmpartial epsilon_nl\n  right)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"where the complex scalar Psi_km is the m'th element of the k'th vector, and corresponds to the z_k in Eq. \\eqref{eq:wirtinger-chainrule}.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"tip: Open Quantum Systems\nIn open quantum systems, where the state is described by a density matrix hatrho, it can be helpful to adopt the double-braket notation langlelangle hatrho_1 vert hatrho_2 ranglerangle equiv trhatrho_1^dagger hatrho_2, respectively to keep track of normal states hatrho (corresponding to Psi) and adjoint states hatrho^dagger (corresponding to Psi), even when hatrho is Hermitian and thus hatrho = hatrho^dagger. For numerical purposes, density matrices are best vectorized by concatenating the columns of hatrho into a single column vector vecrho. Thus, we do not have to be concerned with a separate definition of derivatives w.r.t. density matrices.","category":"page"},{"location":"background/#Complex-Gradient","page":"Background","title":"Complex gradient","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"warning: Warning\nSoftware frameworks for automatic differentiation such as Zygote [26] and Tensorflow [27] may define a (mathematically questionable [28]) \"gradient\" of a real-valued function J with respect to a complex vector with elements z_j asbeginequationlabeleqcomplex-gradient\n(nabla_z J)_j\nequiv fracpartial Jpartial Rez_j + ii fracpartial Jpartial Imz_j\nendequationThis differs from the Wirtinger derivatives by a factor of two (and the complex conjugate). Thus,beginequationlabeleqwirtinger-zygote-grad\nfracpartial Jpartial z_j = frac12 (nabla_z J)^*_j\nendequationin Eq. \\eqref{eq:wirtinger-chainrule}, respectively Eq. \\eqref{eq:grad-via-chi1} when using, e.g., the Zygote.gradient function.","category":"page"},{"location":"background/#Gradients-for-final-time-functionals","page":"Background","title":"Gradients for final-time functionals","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"For simplicity, we consider a functional defined entirely at final time T, the J_T term in Eq. \\eqref{eq:grape-functional}. Since J_T depends explicitly on Psi_k(T) and only implicitly on epsilon_nl, we can use the complex chain rule in Eq. \\eqref{eq:grad-via-chi1}.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Further, we define a new state","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqchi\nchi_k(T) equiv - fracpartial J_Tpartial Psi_k(T)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The minus sign in this definition is arbitrary, and is intended solely to match an identical definition in Krotov's method, the most direct alternative to GRAPE. Since chi_k(T) does not depend on epsilon_nl, we can pull forward the derivative partial  partial epsilon_nl in Eq. \\eqref{eq:grad-via-chi1}, writing it as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrad-at-T\n(nabla J_T)_nl\n= fracpartial J_Tpartial epsilon_nl\n= - 2 Re sum_k fracpartialpartial epsilon_nl langle chi_k(T) vert Psi_k(T)rangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"We end up with the gradient of J_T being the derivative of the overlap of two states chi_k(T) and Psi_k(T) at final time T.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Next, we make use of the assumption that the time evolution is piecewise constant, so that we can use the time evolution operator defined in Eq. \\eqref{eq:time-evolution-op} to write Psi_k(T) as the time evolution of an initial state Psi_k(t=0), the initial_state of the k'th trajectory in the QuantumControl.ControlProblem. That is, Psi_k(T) = hatU^(k)_N_T dots hatU^(k)_1 Psi_k(t=0) with the time evolution operator hatU^(k)_n for the n'th time interval of the time grid with N_T + 1 time grid points, cf. Eq. \\eqref{eq:time-evolution-op}. Plugging this into Eq. \\eqref{eq:grad-at-T} immediately gives us","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrad-at-T-U\nbeginsplit\nfracpartial J_Tpartial epsilon_nl\n= -2 Re sum_k fracpartialpartial epsilon_nl\n    bigglangle chi_k(T) biggvert hatU_N_T^(k) dots hatU^(k)_n dots hatU^(k)_1 biggvert Psi_k(t=0) biggrangle\n    \n= -2 Re sum_k bigglangle chi_k(t_n) biggvert fracpartial hatU^(k)_npartial epsilon_nl biggvert Psi_k(t_n-1) biggrangle\nendsplit\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\nchi_k(t_n) = U^dagger (k)_n+1 dots U^dagger(k)_N_T chi_k(T)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"i.e., a backward propagation of the state given by Eq. \\eqref{eq:chi} with the adjoint Hamiltonian or Liouvillian, and","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqpsi-time-evolution\nPsi_k(t_n-1) = hatU^(k)_n-1dots hatU^(k)_1 Psi_k(0)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"i.e., a forward propagation of the initial state of the k'th trajectory.","category":"page"},{"location":"background/#Derivative-of-the-time-evolution-operator","page":"Background","title":"Derivative of the time-evolution operator","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The last missing piece for evaluating the gradient in Eq. \\eqref{eq:grad-at-T-U} is the derivative of the time evolution operator hatU_n^(k) for the current time interval n. The operator fracpartial hatU_n^(k)partial epsilon_nl could either act to the right, being applied to Psi_k(t_n-1) during the forward propagation, or it (or rather its conjugate transpose) could act to the left, being applied to chi_k(t_n) during the backward propagation. For reasons that will be explained later on, it is numerically more efficient to include it in the backward propagation. Thus, we are given a state chi_k(t_n) and must then numerically obtain the state","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqU-deriv\nchi^prime_kl(t_n-1)\nequiv  fracpartial hatU^dagger(k)_npartial epsilon_nl chi_k(t_n)\n= fracpartialpartial epsilon_nl expleft-ii hatH^dagger_k(epsilon_nl) dt^(-)_n right chi_k(t_n)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Note the dagger and the negative time step dt^(-)_n = (t_n-1 - t_n) — in lieu of changing the sign of the imaginary unit ii — to account for the fact that we are doing a backward propagation, cf. the corresponding forward propagation in Eq. \\eqref{eq:time-evolution-op}. Of course, for a standard Schrödinger equation, hatH_kn^dagger = hatH_kn, and then the negative time step is the only difference between backward and forward propagation; but, in general, we also allow for non-Hermitian Hamiltonians or Liouvillians where it is important to use the correct (adjoint) operator.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Thus, Eq. \\eqref{eq:grad-at-T-U} turns into","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrad-via-chi-prime\nfracpartial J_Tpartial epsilon_nl\n= -2 Re sum_k bigg langle chi^prime_kl(t_n-1) biggvert Psi_k(t_n-1) bigg rangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Or, equivalently, if we had let fracpartial hatU_n^(k)partial epsilon_nl act to the right,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrad-via-psi-prime\nfracpartial J_Tpartial epsilon_nl\n= -2 Re sum_k bigg langle chi_kl(t_n) biggvert Psi^prime_k(t_n) bigg rangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with Psi^prime_k(t_n) equiv  fracpartial hatU^(k)_npartial epsilon_nl Psi_k(t_n-1).","category":"page"},{"location":"background/#Overview-Taylor","page":"Background","title":"Taylor expansion","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"There are several possibilities for evaluating Eq. \\eqref{eq:U-deriv}. One method is to expand the exponential into a Taylor series [22, Eq. (20)]","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqtaylor-op\nfracpartial hatU^dagger(k)_npartial epsilon_nl\n= sum_m=1^infty fracleft(-ii hatH^dagger_kn dt^(-)_nright)^mm\n    sum_m^prime=0^m-1\n    hatH^dagger_kn^m^prime\n    hatmu_lkn^dagger\n    hatH^dagger_kn^m-m^prime-1\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with hatH_kn equiv hatH_k(epsilon_nl) and hatmu_lkn equiv fracpartial hatH_knpartial epsilon_nl.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In practice, Eq. \\eqref{eq:taylor-op} is best evaluated recursively, while being applied to chi_k(t_n):","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\nchi^prime_kl(t_n-1) = sum_m=1^infty fracleft(-ii  dt_n^(-)right)^mm Phi^(lkn)_m\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\nbeginsplit\n  Phi^(lkn)_1 = hatmu_lkn^dagger chi_k(t_n)              \n  Phi^(lkn)_m = hatmu_lkn^dagger hatH^dagger_kn^m-1  chi_k(t_n) + hatH^dagger_kn Phi^(lkn)_m-1\nendsplit\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In GRAPE.jl, Eq. \\eqref{eq:U-deriv} can be evaluated via a Taylor expansion as described above by passing gradient_method=:taylor, with further options to limit the maximum order m.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"tip: TMIDR\nAs in the general TMIDR, the indices k and l are somewhat superfluous here. In addition, hatmu_lkn equiv fracpartial hatH_knpartial epsilon_nl still depends on epsilon_nl only for non-linear controls. Much more commonly, for linear Hamiltonians of the form hatH = hatH_0 + epsilon(t) hatmu, the derivative hatmu is just a static control operator. If hatH is a standard Hamiltonian, and thus Hermitian, we can drop the dagger. The time grid is usually uniform, so we can drop the index n from dt. Thus, a simplified version of Eq. \\eqref{eq:taylor-op} isbeginequationlabeleqtaylor-op-simplified\nfracpartial hatU^dagger_npartial epsilon_n\n= sum_m=1^infty fracleft(-ii hatH_n dt^(-)right)^mm\n    sum_m^prime=0^m-1\n    hatH_n^m^prime\n    hatmu\n    hatH_n^m-m^prime-1\nendequationwith the recursive formulabeginequation\nbeginsplit\nchi^prime(t_n-1) = sum_m=1^infty fracleft(-ii  dt^(-)right)^mm Phi_m\nPhi_1 = hatmu chi(t_n)              \nPhi_m = hatmu hatH_n^m-1  chi(t_n) + hatH_n Phi_m-1\nendsplit\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"For sufficiently small time steps, one may consider using only the first term in the Taylor series, chi^prime_kl(t_n-1) approx -ii dt_n^(-) Phi^(lkn)_1. That is, from Eq. \\eqref{eq:grad-via-chi-prime}, we get","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\nbeginsplit\nfracpartial J_Tpartial epsilon_nl\napprox 2dt_nIm sum_k bigg langle chi_kl(t_n) biggvert fracpartial hatH_knpartial epsilon_nl biggvert Psi_k(t_n-1) bigg rangle \napprox 2dt_nIm sum_k bigg langle chi_kl(t_n) biggvert fracpartial hatH_knpartial epsilon_nl biggvert Psi_k(t_n) bigg rangle\nendsplit\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"This approximation of the gradient has been used historically, including in GRAPE's original formulation [1], also because it matches optimality conditions derived in a Lagrange-multiplier formalism [29, 30] that predates GRAPE. The derivation via Lagrange multipliers also extends more easily to equations of motion beyond Eq. \\eqref{eq:tdse} such as the Gross–Pitaevskii equation [31, 32]. However, even though it is considered a \"gradient-type\" optimization, it is not considered to be within the scope of the GRAPE package (up to the ability to limit that Taylor expansion to first order). The conceptual difference is that these older methods (as well as the other \"gradient-type\", Krotov's method) derive optimality conditions first (via functional derivatives), and then add time discretization to arrive at a numerical scheme. In contrast, GRAPE discretizes first, and then obtains gradients via simple derivatives w.r.t. the pulse values epsilon_nl. This concept of \"discretize first\" is the core concept exploited in GRAPE.jl.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"After GRAPE's original formulation [1], it was quickly realized that high-precision gradients are essential for numerical stability and convergence, in particular if the gradient is then used in a quasi-Newton method [2, 22]. Thus, low-order Taylor expansions should be avoided in most contexts.","category":"page"},{"location":"background/#Overview-Gradgen","page":"Background","title":"Gradient Generators","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"In order to evaluate Eq. \\eqref{eq:U-deriv} to high precision, one can use a trick from computational linear algebra [33] that was reformulated in the context of quantum control by Goodwin and Kuprov [5]. It allows to calculate chi^prime_kl(t_n-1) equiv fracpartial hatU^dagger(k)_npartial epsilon_nl chi_k(t_n) and chi_kl(t_n-1) = hatU^dagger(k)_n chi_k(t_n) at the same time, as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgradprop-bw\n  beginpmatrix chi^prime_k1(t_n-1)  vdots  chi^prime_kL(t_n-1)  chi_k(t_n-1) endpmatrix\n  = exp left-iiGhatH_kn^daggerdt_nright tildechi_k(t_n)\n  \nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"by backward-propagating an extended state","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgradgen-state\n    tildechi_k(t_n)\n    equiv beginpmatrix 0  vdots  0  chi_k(t_n) endpmatrix\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"of dimension N(L+1), where L is the number of controls and N is the dimension of chi_k, under a \"gradient generator\"","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgradgen\n    GhatH_kn^dagger\n    = beginpmatrix\n        hatH^dagger_kn  0  dots  0 hatmu_1kn^dagger \n        0  hatH^dagger_kn  dots  0  hatmu_2kn^dagger \n        vdots   ddots   vdots \n        0  0  dots  hatH^dagger_kn  hatmu_Lkn^dagger \n        0  0  dots  0  hatH^dagger_kn\n    endpmatrix\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"This is a purely formal way of writing the gradient generator; in practice, the extended state tildechi_k(t_n) is represented by a data structure with slots for the states chi^prime_1 … chi^prime_L, in addition to the original state chi, and GhatH_kn^dagger is a container around all the operators hatmu^dagger_lkn equiv fracpartial hatH^dagger_knpartial epsilon_nl in addition to the original (adjoint) Hamiltonian or Liouvillian hatH^dagger_kn itself. The gradient generator G is then implicitly defined by how it acts on the extended state,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n    GhatH beginpmatrix\n        chi^prime_1  vdots  chi^prime_L  chi\n    endpmatrix\n    = beginpmatrix\n        hatH chi^prime_1 + hatmu_1 chi \n        vdots \n        hatH chi^prime_L + hatmu_L chi \n        hatH chi\n    endpmatrix\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"This way, GhatH^dagger_kn replaces hatH^dagger_kn in the backwards propagation, using any of the methods in QuantumPropagators, e.g., the polynomial Chebychev propagator. The appropriate data structures for G and the extended states are implemented in the QuantumGradientGenerators package. As it provides an \"exact\" gradient independently of the time step, the use of the gradient generator is the default in GRAPE.jl, or it can be explicitly requested with gradient_method=:gradgen.","category":"page"},{"location":"background/#GRAPE-scheme","page":"Background","title":"GRAPE scheme","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"With Eq. \\eqref{eq:grad-at-T-U} and the use of gradient generators explained above, we end up with an efficient numerical scheme for evaluating the full gradient shown in Fig. 1.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"<p id=\"fig-grape-scheme\" style=\"text-align: center\">\n<a href=\"../fig/grape_scheme.png\">\n<img src=\"../fig/grape_scheme.png\" width=\"100%\"/>\n</a>\n<a href=\"#fig-grape-scheme\">Figure 1</a>: Numerical scheme for the evaluation of the gradient in <code>GRAPE.jl</code>.\n</p>","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In each iteration, we start in the bottom left with the initial state Psi_k(t=t_0=0) for the kth trajectory. This state is forward-propagated under the guess pulse (in parallel for the different trajectories) over the entire time grid until final time T with N_T time steps. In the diagram, t_-n is a shorthand for t_N_T - n. The propagation over the n'th time interval uses the pulse values epsilon_nl. In the diagram, we have omitted the index l for the different control functions epsilon_l(t) (TMIDR). All of the forward-propagated states (red in the diagram) must be stored in memory.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Having determined Psi_k(T), the state chi_k(T) is calculated according to Eq. \\eqref{eq:chi} for each trajectory k. With the default gradient_method=:gradgen that is depicted here, chi_k(T) is then converted into a zero-padded extended state tildechi_k(T), see Eq. \\eqref{eq:gradgen-state}, which is then backward propagated under a gradient-generator GhatH_kn^dagger defined according to Eq. \\eqref{eq:gradgen}.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"After each step in the backward propagation, the extended state tildechi_k(t_n)rangle contains the gradient-states chi^prime_kl(t_n), cf. Eq. \\eqref{eq:gradprop-bw}. The corresponding forward-propagated states Psi_k(t_n) are read from storage; the overlap chi^prime_kl(t_n)Psi_k(t_n) then contributes to the element (nabla J)_nl of the gradient, cf. Eq. \\eqref{eq:grad-via-chi-prime}. Note that tildechi_k(t_n)rangle must be reset in each time set, i.e., the components chi^prime_kl(t_n) must be zeroed out.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In the original formulation of GRAPE [1], chi_k(T) is always the target state associated with the k'th trajectory. This makes it arbitrary whether to forward-propagate (and store) Psi_k(t) first, or backward-propagate (and store) chi_k(t) first. However, with the generalization to arbitrary functionals [3] via the definition in Eq. \\eqref{eq:chi}, chi_k(T) can now depend on the forward-propagated states Psi_k(T). Thus, the forward propagation and storage must always precede the backward propagation. The requirement for storing the forward-propagated states also explains the choice to let fracpartial hatU_n^(k)partial epsilon_nl act to the left in Eq. \\eqref{eq:grad-at-T-U} to get Eq. \\eqref{eq:grad-via-chi-prime}. If we had instead chosen to let the derivative act to the right to get Eq. \\eqref{eq:grad-via-psi-prime}, we would have to store all of the states Psi^prime_k(t_n) in addition to just Psi_k(t_n) for every time step, which would increase the required memory L-fold for L controls.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The above scheme may be further augmented for running costs. Also, if the alternative gradient_method=:taylor is used, the backward propagation is of the normal states chi_k(T) instead of the extended chi_k(T), but the chi^prime_kl(t_n) still have to be evaluated in each time step. In any case, once the full gradient vector has been collected, it is passed to an optimizer such as L-BFGS-B.","category":"page"},{"location":"background/#Overview-SemiAD","page":"Background","title":"Semi-automatic differentiation","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"For most functionals, the boundary condition for the backward propagations, chi_k(T) as defined in Eq. \\eqref{eq:chi}, is straightforward to calculate analytically, using basic matrix calculus and Wirtinger derivatives, as in the example in the Derivative w.r.t. complex vectors. In such cases, a function that constructs the states chi_k(T) from the forward-propagated states and information in the trajectories should be implemented by hand, and either passed explicitly to QuantumControl.optimize as chi, or associated with the function passed as J_T by implementing a custom method for QuantumControl.Functionals.make_analytic_chi, see the Tip in QuantumControl.Functionals.make_chi for details.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In other situations, the derivative might be overly cumbersome [20, 21], or completely non-analytical [19]. In such cases, the chi_k(T) may be obtained via automatic differentiation [3] (AD). GRAPE.jl supports this via the QuantumControl.Functionals.make_chi function with mode=:automatic. There is no restriction to a particular AD Framework. Any supported framework like Zygote can be loaded and passed to make_chi via the automatic argument, see QuantumControl.set_default_ad_framework.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The AD overhead of evaluating J_T should be extremely minimal (negligible compared to the numerical cost of the backward and forward propagations), but it can be further simplified with analytic chain rules [3]:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"For functionals that depend on overlaps tau_k equiv Psi_k^texttgtPsi_k(T), GRAPE.jl supports a keyword argument tau in J_T. If the function that is passed as J_T supports the tau keyword argument, and if all elements of problem.trajectories have a defined target_state, then a value of complex values tau is passed as defined above.\nEq. \\eqref{eq:chi} then becomes\nbeginequation\nlabeleqchi_tau\nchi_k(T)\n= -fracpartial J_Tpartial braPsi_k(T)\n= - left(\n    fracpartial J_Tpartial tau_k^* \n    fracpartial tau_k^*partial braPsi_k(T)\nright)\n= -frac12 left(nabla_tau_k J_T right) Psi_k^texttgt\nendequation\nwhere we have used that only the complex conjugate tau_k^* =  Psi_k(T)Psi_k^texttgt of the overlap depends explicitly on the co-state braPsi_k(T). The gradient nabla_tau_k J_T defined as in Eq. \\eqref{eq:wirtinger-zygote-grad} — note the factor frac12 — can be obtained with automatic differentiation.\nWhen calling QuantumControl.Functionals.make_chi with mode=:automatic and via=:tau, the above chain rule is used automatically to simplify the AD.\nFor the optimization of quantum gates, it is common to have a logical subspace embedded in a larger physical subspace. The functional J_T in this case can often be written as a function of the achieved gate hatU_L in the logical subspace, a complex matrix with elements\nbeginequation\n  labeleqgate_definition\n  (hatU_L)_ij = Braketphi_i  Psi_j(T)\n  quad Leftrightarrow quad\n  (hatU_L)^*_ij = BraketPsi_j(T)  phi_i\nendequation\nwhere phi_i are the logical basis states, assumed to also be the initial states Psi_k(t=0) = phi_k for the forward propagation (i.e., the optimization is defined with one trajectory per basis state).\nApplying a further chain rule w.r.t. (hatU_L)_ij in Eq. \\eqref{eq:chi}, we find\nbeginequation\n  labeleqchi-gate-proto\n  chi_k(T)\n  equiv - fracpartial J_Tpartial braPsi_k(T)\n   = - sum_ij\n      fracpartial J_Tpartial(U_L)^*_ij\n      fracpartial(U_L)^*_ijpartial braPsi_k(T)\nendequation\nagain using the notation of the Wirtinger derivative. We have used that only (U_L)^*_ij depends explicitly on the co-states braPsi_k(T). Furthermore,\nbeginequation\n  fracpartial J_Tpartial(U_L)^*_ij\n  = frac12 (nabla_U_L J_T)_ij\nendequation\naccording to the definitions in Eqs. (\\ref{eq:complex-gradient}, \\ref{eq:wirtinger2}), and\nbeginequation\n  fracpartial(U_L)^*_ijpartial braPsi_k(T)\n  = fracpartialpartial braPsi_k(T) BraketPsi_j(T)  phi_i\n  = delta_jk phi_i\nendequation\nwith the Kronecker delta delta_jk. Thus, Eq. \\eqref{eq:chi-gate-proto} simplifies to\nbeginequation\n  labeleqchi_gate\n  chi_k(T)\n  = -frac12 sum_i\n    (nabla_U_L J_T)_ik phi_i\nendequation\nwhere nabla_U_L J_T is evaluated via automatic differentiation, e.g. with the Zygote.gradient function.\nThe QuantumControl.Functionals.gate_functional function can be used to construct functionals defined on top of a gate hatU_L, and QuantumControl.Functionals.make_gate_chi constructs the equivalent chi function via AD and the above chain rule.","category":"page"},{"location":"background/#Overview-Running-Costs","page":"Background","title":"Running costs","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"So far, we have only discussed the evaluation of gradients for final-time functionals J_T. We now extend the discussion of semi-automatic differentiation to the running costs g_ab in Eq. \\eqref{eq:grape-functional}. Since we are considering piecewise constant pulses, the integral over the running cost turns into a sum over the time steps. That is, we rewrite Eq. \\eqref{eq:grape-functional} as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  labeleqfunctional-discrete\n  J(epsilon_nl)\n  =\n  J_T(Psi_k(T)) +\n  lambda_a sum_n=1^N_T sum_l (g_a)_nl +\n  lambda_b sum_n=0^N_T (g_b)_n\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  (g_a)_nl = frac1dt_n g_a(epsilon_nl dt_n)qquad\n  (g_b)_n = frac1Delta t_n g_b(Psi_k(t_n) t_n)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"As in Eq. \\eqref{eq:psi-time-evolution}, we define Psi_k(t_n) = hatU^(k)_n dots hatU^(k)_1 Psi_k(t=0), with the time grid points t_0 = 0, t_N_T = T, and with hatU^(k)_n = exp-i hatH_kn dt_n as the time evolution operator for the n'th time interval, dt_n = t_n - t_n-1. Similarly, Delta t_n is the time step around the time grid point t_n, e.g. Delta t_0 = dt_1, Delta t_n = frac12(t_n+1 - t_n-1) for 1le n  N_T, and Delta t_N_T = dt_N_T. For uniform time grids, dt_n equiv Delta t_n equiv dt.","category":"page"},{"location":"background/#Field-dependent-running-costs","page":"Background","title":"Field-dependent running costs","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Typically, running costs on the control fields are direct analytic expressions, e.g., g_a(epsilon_nl) = epsilon_nl^2 to penalize large amplitudes. Thus, they are easily included in the gradient, e.g., (nabla g_a)_nl = 2 epsilon_nl. For convenience, this can also be done with automatic differentiation. This even extends to penalties on the first and second derivatives of the controls [23–25].","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In GRAPE.jl, running costs are enabled by passing a J_a function together with lambda_a to QuantumControl.optimize with method=GRAPE. The optimization also needs a function grad_J_a which can be obtained automatically, see QuantumControl.Functionals.make_grad_J_a.","category":"page"},{"location":"background/#State-Dependent-Running-Costs","page":"Background","title":"State-dependent running costs","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"More interesting is the case of state-dependent constraints. Typical examples [34] include trajectory optimizations,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  g_btexttrj(Psi_k(t_n))\n  = sum_k normPsi_k(t_n) - Psi^texttgt_k(t_n)^2\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"where the time evolution of each state Psi_k(t_n) should be close to some target evolution Psi^texttgt_k(t_n), or observable optimizations","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  g_bhatD(t)(Psi_k(t_n))\n  = sum_k BraketPsi_k(t_n)  hatD(t_n) Psi_k(t_n)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"where the expectation value of some observable hatD(t) is to be minimized. A special case of this is the minimization of the population in some forbidden subspace [35], where hatD(t_n) equiv hatD is a projector into that subspace.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"To obtain the full gradient of a functional with a state-dependent running cost, we apply the same procedure as in the section about Gradients for final-time functionals and find","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"  beginalign\n    fracpartial Jpartial epsilon_nl\n    =\n    2 Re sum_k left\n      fracpartial J_Tpartial Psi_k(T)\n      fracpartial Psi_k(T)partial epsilon_nl\n      + sum_n=0^N_T\n      fracpartial(g_b)_npartial Psi_k(t_n)\n      fracpartial Psi_k(t_n)partial epsilon_nl\n    right \n    labeleqgradJ-rc2\n    =\n    -2 Re sum_k fracpartialpartial epsilon_nl left\n      bigglangle chi_k^(0)(T) biggvert hatU_N_T dots hatU_1 biggvert Psi_k(0) biggrangle\n     + sum_n=n^N_T\n        bigglangle xi_k(t_n) biggvert hatU_n dots hatU_1 biggvert Psi_k(0) biggrangle\n    right\n  endalign","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  labeleqchi-boundary-gb1\n  chi_k^(0)(T) equiv - fracpartial J_Tpartial braPsi_k(T)\n  qquad\n  xi_k(t_n) equiv - fracpartial(g_b)_npartial braPsi_k(t_n)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"cf. Eq. \\eqref{eq:chi}. In the sum over n in Eq. \\eqref{eq:gradJ-rc2}, we have used that Psi_k(t_n) depends on epsilon_nl only for n ge n. This implies that for the final time interval, n = N_T, there is only a single term,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n    fracpartial Jpartial epsilon_N_T l\n    = -2 Re sum_k bigglangle\n      chi_k(T) biggvert\n      fracpartialhatU_N_Tpartial epsilon_N_T l  biggvert\n      Psi_k(t_N_T-1)\n    biggrangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqchi-boundary-gb\n  chi_k(T)\n  equiv\n  chi_k^(0)(T) + xi_k(T)\n  =\n  - left(\n    fracpartial J_Tpartial braPsi_k(T) +\n    fracpartial(g_b)_N_Tpartial braPsi_k(T)\n  right)\n  \nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Evaluating the gradient progressively backward in time for n = (N_T-1) dots 1, we then find a recursive relationship","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrape-gb-bw-eqm\n  fracpartial Jpartial epsilon_nl\n  = -2 Re sum_k bigglangle\n    chi_k(t_n) biggvert\n    fracpartialhatU_npartial epsilon_nl  biggvert\n    Psi_k(t_n-1)\n  biggrangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"with","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqchi-bw-gb\n  chi_k(t_n) =\n    hatU_n+1^dagger chi_k(t_n+1) -\n    fracpartial(g_b)_npartial braPsi_k(t_n)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Thus, there are no fundamental changes to the scheme in Fig. 1 in the presence of state-dependent running costs. The states Psi_k(0) must be forward-propagated and stored, and then the extended states tildechi_k(t_n) are propagated backward to produce the gradient. The only difference is that the boundary state tildechi_k(T) is now constructed based on Eq. \\eqref{eq:chi-boundary-gb} instead of Eq. \\eqref{eq:chi}. Furthermore, the backward propagation uses the discrete inhomogeneous Eq. \\eqref{eq:chi-bw-gb}. The inhomogeneity is calculated using the forward-propagated states stored previously, with the derivative of g_b performed analytically or by automatic differentiation.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"warning: Warning\nSupport for state-dependent running costs is planned for a future version of GRAPE.jl; currently, there are no parameters to pass for g_b, lambda_a, etc.","category":"page"},{"location":"background/#Optimizers","page":"Background","title":"Optimizers","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"In the preceding sections, we have discussed in detail the GRAPE method for obtaining the gradient (nabla J)_nl equiv fracpartial Jpartial epsilon_nl that determines how much the optimization functional J changes with variations of the nl'th element of a vector of pulse values, i.e., the value of the control function epsilon_l(t) on the n'th interval of the time grid (under the fundamental assumption that epsilon_l(t) is discretized to be piecewise constant.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In the original formulation of GRAPE, the values epsilon_nl would then be updated in each iteration by taking a step with a fixed step width alpha in the direction of the negative gradient [1, cf. Eq. (13)]","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgradient-descent-update\nepsilon_nl rightarrow epsilon_nl - alpha (nabla J)_nl\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In practice, the gradient can also be fed into an arbitrary gradient-based optimizer. These can include various linesearch methods to determine an optimal step width alpha, avoiding slow convergence due to alpha being too conservative, or overshooting the optimum with a value of alpha that is too large. They can also include quasi-Newton methods, that determine a \"search direction\" different from nabla J by using nabla J to estimate a Hessian.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"De Fouquières et al. [2] showed that using the BFGS quasi-Newton algorithm results in a dramatic improvement in stability and convergence. Following what is widely considered best practice, GRAPE.jl uses the memory-efficient L-BFGS-B [36, 37] implementation as the default optimization backend. This method also allows including box constraints (hence the \"-B\" suffix) for the control values. In QuantumControl.optimize with method=GRAPE, passing lower_bound, upper_bound, or (for more fine-tuned specifications), pulse_options with lower_bounds and upper_bounds vectors for the value of each control in each time interval allows using this box-constraint feature.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Even though L-BFGS-B is a remarkably robust and easy-to-use optimizer, GRAPE.jl also allows to use any of the gradient-based optimizers in the Optim toolbox. This includes the Gradient Descent of Eq. \\eqref{eq:gradient-descent-update}, potentially with the addition of a line search. Any suitable optimizer from Optim (with its own options for, e.g., the line search) can be passed to QuantumControl.optimize via the optimizer keyword.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"warning: Warning\nThe details of how to pass options to Optim optimizers or how to query the state of the optimization are considered experimental. A future version of GRAPE may also adopt the more general Optimization interface.","category":"page"},{"location":"#GRAPE.jl","page":"Home","title":"GRAPE.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\n\nVERSION = Pkg.dependencies()[Base.UUID(\"6b52fcaf-80fe-489a-93e9-9f92080510be\")].version\n\ngithub_badge = \"[![Github](https://img.shields.io/badge/JuliaQuantumControl-GRAPE.jl-blue.svg?logo=github)](https://github.com/JuliaQuantumControl/GRAPE.jl)\"\n\nversion_badge = \"![v$VERSION](https://img.shields.io/badge/version-v$VERSION-green.svg)\"\n\njoss_badge = \"[![status](https://joss.theoj.org/papers/25e7a240c129459ad160dd3fb9d009d8/status.svg)](https://joss.theoj.org/papers/25e7a240c129459ad160dd3fb9d009d8)\"\n\nMarkdown.parse(\"$github_badge $version_badge $joss_badge\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Gradient Ascent Pulse Engineering in Julia","category":"page"},{"location":"#Summary","page":"Home","title":"Summary","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The GRAPE.jl package implements Gradient Ascent Pulse Engineering [1–3], a widely used method of quantum optimal control. The quantum state of a system can be described numerically by a complex vector ketPsi(t) that evolves under a differential equation of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"defiimathrmi\nbeginequationlabeleqtdse\nii hbar fracpartial ketPsi(t)partial t = hatH(epsilon(t)) ketPsi(t)\nendequation","category":"page"},{"location":"","page":"Home","title":"Home","text":"where hatH is a matrix (the Hamiltonian, most commonly) whose elements depend in some way on the control function epsilon(t). We generally know the initial state of the system ketPsi(t=0) and want to find an epsilon(t) that minimizes some functional J that depends on the states at some final time T, as well as running costs on ketPsi(t) and values of epsilon(t) at intermediate times.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The defining feature of the GRAPE method is that it considers epsilon(t) as piecewise constant, i.e., as a vector of pulse values epsilon_n, for the n'th interval of the time grid. This allows solving Eq. \\eqref{eq:tdse} analytically for each time interval, and deriving an expression for the gradient partial J  partial epsilon_n of the optimization functional with respect to the values of the control field. The pulse values are then updated based on the gradient, in an efficient scheme detailed in Background (or the \"TMIDR\" short summary).","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Home\nRelated Software\nFeatures\nContributing\nHistory","category":"page"},{"location":"","page":"Home","title":"Home","text":"Depth = 2\nPages = [pair[2] for pair in Main.PAGES[2:end-1]]","category":"page"},{"location":"#Related-Software","page":"Home","title":"Related Software","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GRAPE.jl integrates with the JuliaQuantumControl ecosystem and, in particular, the following packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"QuantumControl.jl – The overall control framework, used to define quantum control problems independent of a particular method. GRAPE can be used by calling QuantumControl.optimize with method = GRAPE. See Relation to the QuantumControl Framework\nQuantumPropagators.jl – The numerical backend for simulating the piecewise-constant time dynamics of the system. Implements efficient schemes such as Chebychev propagation [4], but also can further delegate to DifferentialEquations.jl\nQuantumGradientGenerators.jl – Implementation of the gradient of a single-time-step evolution operator according to Goodwin and Kuprov [5], a key component of the GRAPE scheme as implemented here.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The GRAPE method (\"discretize first\") compares most directly to Krotov's method (\"derive optimality first, discretize second\") [6], implemented in Krotov.jl with a compatible interface.","category":"page"},{"location":"#Other-implementations-of-GRAPE","page":"Home","title":"Other implementations of GRAPE","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"There have been a number of implementations of the GRAPE method in different contexts.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the context of NMR:","category":"page"},{"location":"","page":"Home","title":"Home","text":"SIMPSON – Simulation program for solid-state NMR spectroscopy (C) [7]\nSpinach – Spin dynamics simulation library (Matlab) [8]\npulse-finder – Matlab code for GRAPE optimal control in NMR [9]","category":"page"},{"location":"","page":"Home","title":"Home","text":"More recent implementations, geared towards more general purposes like quantum information:","category":"page"},{"location":"","page":"Home","title":"Home","text":"QuTIP – Quantum Toolbox in Python [10]\nC3 – Toolset for control, calibration, and characterization of physical systems (Python) [11]\nQuOCS – Python software package for model- and experiment-based optimizations of quantum processes [12]\nQuanEstimation – Python-Julia-based open-source toolkit for quantum parameter estimation [13]","category":"page"},{"location":"","page":"Home","title":"Home","text":"As a direct precursor to GRAPE.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"QDYN – Fortran 95 library and collection of utilities for the simulation of quantum dynamics and optimal control with a focus on both efficiency and precision","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The GRAPE.jl package aims to avoid common shortcomings in existing implementations by emphasizing the following design goals and features:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Performance similar to that of Fortran [14], allowing to extend to quantum systems of large dimension. The numerical cost of the GRAPE method is dominated by the cost of evaluating the time evolution of the quantum system. GRAPE.jl delegates this to efficient piecewise-constant propagators in QuantumPropagators.jl or the general DifferentialEquations.jl framework [15].\nGenerality through the adoption of the concepts defined in QuantumControl.jl\nAllow functionals that depend on an arbitrary set of \"trajectories\"  Psi_k(t), each evolving under a potentially different hatH_k. In contrast to the common restriction to a single state Psi or a single unitary hatU as the dynamical state, this enables ensemble optimization for robustness against noise, e.g., Ref. [16]. The optimization over multiple trajectories is parallelized.\nEach hatH_k may depend on an arbitrary number of controls epsilon_l(t) in an arbitrary way, with a distinction between time-dependent \"amplitudes\" and \"controls\", going beyond the common assumption of linear controls, hatH = hatH_0 + epsilon(t) hatH_1.\nFlexibility to work, via multiple dispatch, with any custom data structures for quantum states Psi_k(t) or the dynamic generators hatH_k(epsilon_l(t)), enabling a wide range of applications, from NMR spin systems to superconducting circuits or trapped atoms in quantum computing, to systems with spatial degrees of freedom, e.g., Ref. [17]. This also includes open quantum systems.\nArbitrary Functionals via semi-automatic differentiation [3]. The numerical scheme implemented in GRAPE.jl is derived from a generalization that calculates the gradient of the final-time functional via the chain rule with respect to the states Psi_k(T). This allows going beyond \"standard functionals\" based on overlaps with target states [1, 18] to any computable functional, including non-analytic functionals such as entanglement measures [19–21]. The consequence of this generalization is a boundary condition for the backward-propagation as chi_k(T) = -partial J_Tpartial Psi_k(T) instead of the target state in the traditional scheme. The state chi_k(T), as well as derivatives for any running costs, can optionally be obtained via automatic differentiation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"GRAPE.jl is aimed at researchers in quantum control wanting flexibility to explore novel applications, while also requiring high numerical performance.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See CONTRIBUTING.md.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Consider using the JuliaQuantumControl Dev Environment.","category":"page"},{"location":"#History","page":"Home","title":"History","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the Releases on Github.","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This introductory tutorial illustrates the basic usage of the GRAPE package for an example of a quantum gate on two qubits. It tries to be a self-contained as possible, to highlight some of the core concepts and features of the GRAPE package:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Using arbitrary data structures for quantum states and operators\nDefining a control problem based on multiple \"trajectories\", with multi-threading\nOptimizing multiple control fields at the same time (the real and imaginary part of a physical control)\nUsing non-trivial \"control amplitudes\" to ensure smooth switch-on/off\nUsing automatic differentiation to minimize arbitrary optimization functionals\nApplying bounds on the amplitude of the control field","category":"page"},{"location":"tutorial/#A-Two-Qubit-System","page":"Tutorial","title":"A Two-Qubit System","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The quantum state of a two-qubit system is described by a complex vector spanned by the possible classical bit configurations, 00, 01, 10, and 11 in braket notation. This basis is represented as","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using StaticArrays: SVector\n\nbasis = [\n    SVector{4}(ComplexF64[1, 0, 0, 0]),  # |00⟩\n    SVector{4}(ComplexF64[0, 1, 0, 0]),  # |01⟩\n    SVector{4}(ComplexF64[0, 0, 1, 0]),  # |10⟩\n    SVector{4}(ComplexF64[0, 0, 0, 1]),  # |11⟩\n];\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"An arbitrary vector Ψ = α_00 00 + α_01 01 + α_10 10 + α_11 11 with complex coefficients α_i = α_00, α_01, α_10, α_11 is understood according to the Born rule to specify the probability as α_i^2 to find the system in the corresponding possible classical state on measurement.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We've used the StaticArrays Julia package to encode the quantum states as an SVector, which provides numerical advantages for very small vectors such as these, but also illustrates an important design choice of the GRAPE package: states can be expressed in any data structure fulfilling a well-specified interface. This allows for custom, problem-specific encodings.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Prior to a measurement, quantum mechanics postulates that a quantum state Ψ(t) evolves according to the Schrödinger equation, based on a Hamiltonian matrix hatH. We consider here a setup inspired by superconducting transmon qubits. Each qubit is driven by an oscillating microwave field with frequency near the resonance for that qubit, and the two qubits are coupled by a shared transmission line. As is common in simulating quantum systems with a fast-oscillating field, the rotating wave approximation allows us to formulate the system in the \"rotating frame\" of the microwave field center frequency ω_mw. The Hamiltonian for the two-qubit system is then","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hatH\n    = left(δ_1 hatn_1 + fracΩ(t)2 hata_1 + fracΩ^*(t)2 hata_1^daggerright)\n    +  left(δ_2 hatn_2 + fracΩ(t)2 hata_2 + fracΩ^*(t)2 hata_2^daggerright)\n    + J (hata_1hata_2^dagger + hata_1^daggerhata_2)\n    tag1","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"with the usual raising and lowering operators hata^dagger and hata and the number operator hatn = hata^dagger hata, and where δ_12 is the detuning of microwave angular central frequency from the transition frequency of the first and second qubit, respectively, J is the static coupling, and Ω(t) is the envelope of the microwave field. In the rotating frame, when the physical microwave can deviate from its central frequency ω_mw, this is equivalent to a complex amplitude, where the complex phase of Ω(t) is the phase relative to ω_mw t. Note that Eq. (1) is an oversimplified model for actual transmons, which should include more levels than just the two lowest-lying levels defining the qubit.","category":"page"},{"location":"tutorial/#Units","page":"Tutorial","title":"Units","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Generally, the elements of the Hamiltonian are in units of energy. However, the QuantumPropagators package that GRAPE uses to simulate all dynamics assumes ħ = 1, turning all elements of the Hamiltonian into angular frequencies (from the Planck relation E = ħω), expressed in units of 2π⋅Hz. It also makes \"energy\" and \"time\" directly reciprocal (only the phase ωt is relevant for the dynamics). In numerics generally, it is best for all quantities to have magnitudes between maybe 10⁻³ and 10³ to avoid floating point errors. Here, the numerical quantities are the elements of the operators and the values in the time grid. With superconducting qubits typically having energies in a GHz regime (with detunings in MHz) and a timescale of ns for operations, we can define corresponding \"internal\" units:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"const GHz = 1.0;\nconst MHz = 0.001GHz;\nconst ns = 1.0;\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can then specify any of the \"energy\" parameters below with units, e.g.,","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra: ⋅  # just so that the code looks nicer\nδ₁ = 100⋅2π⋅MHz;\nnothing # hide","category":"page"},{"location":"tutorial/#Control-Amplitudes","page":"Tutorial","title":"Control Amplitudes","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ω(t) in Eq. (1) is our control amplitude; the aim of optimal control is to find the particular Ω(t) that steers the system in some particular way. In our case: to implement a quantum gate. However, the implementation of optimal control in GRAPE assumes real-valued controls. Thus, we have to split the complex Ω(t) into an independent real and imaginary part.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We may also want to place some physical constraints onto Ω(t). For example, we may want Ω(t) to smoothly switch on from zero and off to zero at the beginning and end of the time grid. One way of achieving this is to define Ω(t) = S(t) ϵ(t) where S(t) is a static shape that has the smooth switch-on and off, and ϵ(t) is an arbitrary function that we can optimize freely.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The QuantumPropagators framework provides a ShapedAmplitude object to implement this:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using QuantumPropagators.Amplitudes: ShapedAmplitude","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For a fixed time grid ending at","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"T = 400ns;\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"we can define the function S(t) as","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using QuantumPropagators.Shapes: flattop\nshape(t) = flattop(t, T = T, t_rise = 15ns);\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where flattop is a function that switches on smoothly from zero, reaches 1.0 after 15 ns, and remains constant at 1.0 until the last 15 ns before the final time T = 400 ns, where it smoothly switches off to zero.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can then define an initial guess for ϵ(t) as a constant function and assemble that into the complete Ω(t):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"ϵ_re_guess(t; Ω₀ = 35⋅2π⋅MHz) = Ω₀\nϵ_im_guess(t) = 0.0\n\nΩ_re_guess = ShapedAmplitude(ϵ_re_guess; shape);\nΩ_im_guess = ShapedAmplitude(ϵ_im_guess; shape);\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By setting the guess for the imaginary part to zero, we let the system start exactly at the central frequency of the microwave field.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In general, the choice of the initial guess function can have a significant impact on the optimization. What makes a good guess pulse, in terms of which shape to use, or what initial amplitude (35⋅2π MHz, here) is often the result of some physical intuition, or some trial and error.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With the above definition, Ω_re_guess and Ω_im_guess are \"control amplitudes\" that are the physical control, whereas ϵ_re_guess and ϵ_im_guess are the \"controls\" from the perspective of GRAPE. The basic task of the GRAPE method is to discretize these controls to the intervals of the time grid (\"pulses\", and then iteratively update the pulse values at each time interval, based on the gradient of some optimization functional with respect to the pulse value. This distinction between physical control amplitudes and optimization control functions / pulses is a core design aspect of GRAPE, providing great flexibility in the models that can be used for optimization.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can now define an explicit time grid","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"tlist = collect(range(0, T, step = 0.1ns));\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"and plot the combined Ω(t):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Plots\nENV[\"GKSwstype\"] = \"100\" # hide\ngr() # hide\nusing QuantumPropagators.Controls: discretize\n\n\"\"\"\nPlot the given complex pulse in two panels: absolute value and phase.\n\n```julia\nfig = plot_complex_pulse(tlist, Ω; time_unit=:ns, ampl_unit=:(2π⋅MHz), kwargs...)\n```\n\ngenerates a plot of the complex field `Ω` over the time grid `tlist`.\n\nArguments:\n\n* `tlist`: A vector of time grid values\n* `Ω`: A complex vector of the same length as `tlist` or a function `Ω(t)` returning a complex number\n* `time_unit`: A symbol that evaluates to the conversion factor for the time unit\n* `ampl_unit`: A symbol that evaluates to the conversion factor of the amplitude unit\n\nAll other keyword arguments are forwarded to `Plots.plot`.\n```\n\"\"\"\nfunction plot_complex_pulse(tlist, Ω; time_unit=:ns, ampl_unit=:(2π⋅MHz), kwargs...)\n\n    Ω = discretize(Ω, tlist)  # make sure Ω is defined on *points* of `tlist`\n\n    s_ampl_unit = string(ampl_unit)\n    if startswith(s_ampl_unit, \"(2π) ⋅ \")\n        s_ampl_unit = \"2π \" * s_ampl_unit[11:end]\n    end\n\n    ax1 = plot(\n        tlist ./ eval(time_unit),\n        abs.(Ω) ./ eval(ampl_unit);\n        label=\"|Ω|\",\n        xlabel=\"time ($time_unit)\",\n        ylabel=\"amplitude ($s_ampl_unit)\",\n        kwargs...\n    )\n\n    ax2 = plot(\n        tlist ./ eval(time_unit),\n        angle.(Ω) ./ π;\n        label=\"ϕ(Ω)\",\n        xlabel=\"time ($time_unit)\",\n        ylabel=\"phase (π)\"\n    )\n\n    plot(ax1, ax2, layout=(2, 1))\n\nend\n\nconst 𝕚 = 1im\n\nfig = plot_complex_pulse(tlist, t -> Ω_re_guess(t) + 𝕚 * Ω_im_guess(t))\nusing DisplayAs; fig |> DisplayAs.SVG # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We see the expected shape with the smooth switch-on/-off and the amplitude of 35⋅2π MHz that we specified, while from the perspective of the GRAPE method, the \"guess control\" is a constant function.","category":"page"},{"location":"tutorial/#Hamiltonian","page":"Tutorial","title":"Hamiltonian","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With the separation of Ω(t) into real and imaginary parts, we can now express the Hamiltonian in Eq. (1) numerically:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"import QuantumPropagators: hamiltonian\nusing StaticArrays: SMatrix\n\n\"\"\"Construct the time-dependent Hamiltonian for a two-qubit transmon system.\n\n```julia\nĤ = transmon_hamiltonian(; δ₁, δ₂, J, Ω_re, Ω_im)\n```\n\nconstructs a [Generator](@extref `QuantumPropagators.Generators.Generator`) for\na two-transmon system truncated to two levels for each qubit, in the rotating\nwave approximation (RWA)\n\nArguments:\n\n* `δ₁`: The detuning of qubit 1 from the RWA angular frequency\n* `δ₂`: The detuning of qubit 2 from the RWA angular frequency\n* `J`: The static coupling between the two qubits, as angular frequency\n* `Ω_re`: The amplitude (angular frequency) of the real part of the microwave drive\n* `Ω_im`: The amplitude (angular frequency) of the imaginary part of the microwave drive\n\nBoth `Ω_re` and `Ω_im` can be given as a function `Ω_re(t)` and `Ω_im(t)`, or as a\nvector of values on the time grid, or on the intervals of the time grid.\n\"\"\"\nfunction transmon_hamiltonian(; δ₁, δ₂, J, Ω_re, Ω_im)\n\n    Ĥ₀ = SMatrix{4,4,ComplexF64}(\n        [0        0       0       0\n         0        δ₂      J       0\n         0        J       δ₁      0\n         0        0       0       δ₁+δ₂]\n    )\n\n    Ĥ₁_re = (1/2) * SMatrix{4,4,ComplexF64}(\n        [0      1       1      0\n         1      0       0      1\n         1      0       0      1\n         0      1       1      0]\n    )\n\n    Ĥ₁_im = (𝕚/2) * SMatrix{4,4,ComplexF64}(\n        [0      -1      -1      0\n         1       0       0     -1\n         1       0       0     -1\n         0       1       1      0]\n    )\n\n    return hamiltonian(Ĥ₀, (Ĥ₁_re, Ω_re), (Ĥ₁_im, Ω_im))\n\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where we have used the QuantumPropagators.Generators.hamiltonian function to construct an object that serves as a time-dependent Hamiltonian (a \"Generator\", in the terminology of the general QuantumControl framework, ensuring adherence to the required interface). The SMatrix used for the drift and control Hamiltonian operators match the SVector used to encode states.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We use some arbitrary but reasonable values here, with the central frequency of the microwave field centered between the frequencies of the two qubits, resulting in a detuning of ±100⋅2π MHz and a static coupling of 3⋅2π MHz.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ĥ = transmon_hamiltonian(;\n    δ₁ = 100⋅2π⋅MHz,\n    δ₂ = -100⋅2π⋅MHz,\n    J = 3⋅2π⋅MHz,\n    Ω_re = Ω_re_guess,\n    Ω_im = Ω_im_guess,\n)","category":"page"},{"location":"tutorial/#Optimization-Target","page":"Tutorial","title":"Optimization Target","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We will now use the GRAPE method to find an Ω(t) that implements a CNOT, one of the fundamental building blocks of a quantum computer. In classical terms, a CNOT gate flips the state of the second qubit if and only if the first qubit is \"1\". This is fundamentally an entangling operation, and the equivalent of an \"if\" statement in a quantum circuit.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In terms of the basis states, we have corresponding target states:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"target_states = [\n    SVector{4}(ComplexF64[1, 0, 0, 0]),  # |00⟩\n    SVector{4}(ComplexF64[0, 1, 0, 0]),  # |01⟩\n    SVector{4}(ComplexF64[0, 0, 0, 1]),  # |10⟩ → |11⟩\n    SVector{4}(ComplexF64[0, 0, 1, 0]),  # |11⟩ → |10⟩\n];\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Beyond a classical interpretation, any quantum state Ψ should be mapped to the state","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ψ^(tgt) = hatO Ψ = sum_k α_k hatO k","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"for the four basis states k = 00, 01, 10, 11, with","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hatO  beginpmatrix\n1  0  0  0\n0  1  0  0\n0  0  0  1\n0  0  1  0\nendpmatrix","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"being the matrix representation of the CNOT gate.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can use the QuantumPropagators propagators to simulate (\"propagate\") the 10 state exemplarily. QuantumPropagators can use a variety of methods to do this. We use the Cheby method here, which is extremely efficient at solving the Schrödinger equation for piecewise-constant pulses by expanding the time evolution operator into Chebychev polynomials.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using QuantumPropagators: propagate, Cheby\n\npops = propagate(\n    basis[3], Ĥ, tlist; method=Cheby, storage=true, observables=(Ψ -> Array(abs2.(Ψ)), )\n)\n\nfig = plot(tlist ./ ns, pops'; label=[\"00\" \"01\" \"10\" \"11\"], xlabel=\"time (ns)\", legend=:outertop, legend_column=-1)\nusing DisplayAs; fig |> DisplayAs.SVG # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"From the dynamics, we can see that the 10 initial state remains approximately unchanged by the guess controls, instead of evolving into the 11 state as it should.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to find a field Omega(t) that implements the desired CNOT gate, the GRAPE method minimizes an appropriate (user-supplied) mathematical functional. We define here","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"@doc raw\"\"\"Square-Modulus functional\n\n```julia\nJ_T = J_T_sm(Ψ, trajectories)\n```\n\ncalculates the real-valued scalar\n\n```math\nJ_{T,sm} = 1 - \\frac{1}{N^2} \\left\\vert\\sum_n ⟨Ψ_k(T)|Ψ_k^{(tgt)}⟩\\right\\vert^2\n```\n\nwhere the state ``|Ψ_k⟩`` is the k'th element of `Ψ` and ``|Ψ_k^{(tgt))⟩`` is\nthe state stored in the `target_state` attribute of the k'th element of the\n`trajectories` list. The ``|Ψ_k⟩`` should generally be the states obtained from\npropagating the state stored in the `initial_state` attribute of the k'th\nelement of `trajectories` forward to some final time ``T``.\n\nConceptually, this becomes zero if and only if all the forward propagated\nstates have an overlap of 1 with their respective target state, up to a\nglobal phase.\n\"\"\"\nfunction J_T_sm(Ψ, trajectories)\n    N = length(trajectories)\n    f = zero(ComplexF64)\n    for (Ψ_k, traj) in zip(Ψ, trajectories)\n        f += Ψ_k ⋅ traj.target_state\n    end\n    return 1.0 - (abs2(f) / N^2)\nend\n\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"to implement","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"J_Tsm = 1 - frac1N^2 leftvertsum_n Ψ_k(T)Ψ_k^(tgt)rightvert^2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"appropriate for optimization of a quantum gate if the states Ψ_k(T) are evolved from the N = 4 basis states (for a two-qubit gate) and the target states Ψ_k^(tgt) are chosen as hatO Ψ_k(T). A generalization of the functional J_T (which is one of the standard functionals of quantum control) is also implemented in QuantumControl.Functionals.J_T_sm. We have re-implemented it here by hand to illustrate the API that the GRAPE package expects for a functional J_T. In particular, GRAPE requires optimization functionals to be defined in terms of trajectories, as the way to implicitly define the final-time states Ψ_k(T) that enter the functional:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using GRAPE: Trajectory\n\ntrajectories = [\n    Trajectory(Ψ, Ĥ; target_state=Ψ_tgt)\n    for (Ψ, Ψ_tgt) in zip(basis, target_states)\n]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Each trajectory, at a minimum, defines an initial state Ψ and a dynamical generator Ĥ (i.e., a time-dependent Hamiltonian, in our case). In general, these can be arbitrary problem-specific objects, as long as they implement the required interfaces. Each trajectory can also attach arbitrary additional attributes that may be used by the J_T function, e.g., a target_state in our case.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Defining an optimization in terms of multiple \"trajectories\" has a number of benefits. For one, simulating the dynamics for the different trajectories can be performed in parallel, resulting in a potential speedup proportional to the number of trajectories. Second, it also enables advanced use cases such as \"ensemble optimizations\" that consider multiple \"copies\" of the quantum system, each with a different noisy Hamiltonian, in an effort to find controls that are robust with respect to these variations.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The important point is that all of the trajectories share the same set of controls, two in our case (the real and imaginary part of Ω(t) divided by our shape S(t)):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using QuantumPropagators.Controls: get_controls\n\nget_controls(trajectories)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The GRAPE method works by numerically evaluating gradients of the given functional J_T with respect to the pulse values at each interval of the time grid. As derived fully in Background, the resulting scheme depends explicitly on J_T via the definition of a set of \"adjoint states\",","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"χ_k(T) equiv - fracpartial J_Tpartial braΨ_k(T)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The calculation of these states must be implemented in a function chi that will be passed to the optimization alongside J_T. For J_Tsm, the states χ_k(T) can be calculated analytically, as","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"χ_k(T) = frac1N^2 left(sum_j Ψ_j^(tgt)Ψ_j(T)right) Ψ_k^(tgt)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"and is implemented in QuantumControl.Functionals.chi_sm.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"However, for more advanced functionals, J_T may not always have an analytic derivative that can be written in closed form, or the derivative is just too cumbersome to write out. The GRAPE package allows constructing chi via automatic differentiation, e.g., via the popular Zygote package. This feature can be enabled as follows:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"import Zygote\nusing GRAPE\n\nGRAPE.set_default_ad_framework(Zygote)","category":"page"},{"location":"tutorial/#Optimization","page":"Tutorial","title":"Optimization","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can now run the actual optimization by calling GRAPE.optimize. The main positional arguments are the trajectories that enter the functional and the time grid tlist. Beyond that, the required key arguments are J_T to give the functional and prop_method to specify the method to be used for any internal time propagation. We do not specify chi, since we are relying on the automatic differentiation that we activated above.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The optimization will run for as many iterations as given by iter_stop (5000 by default), and we can give a check_convergence callback to stop the optimization earlier, based on some value of the functional being reached. There is also a callback to print some information after each iteration.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By setting use_threads=true, we parallelize the optimization over the different trajectories. This requires that the Julia process was started with the -t <NTHREADS> option, or that the JULIA_NUM_THREADS environment variable was set before starting the Julia process.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Lastly, we make use of the GRAPE package's ability to apply box constraints, i.e., an upper_bound and lower_bound for the values of the control pulse.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"\nresult = GRAPE.optimize(\n    trajectories,\n    tlist;\n    prop_method = Cheby,\n    J_T = J_T_sm,\n    callback = GRAPE.make_grape_print_iters(),\n    iter_stop = 200,\n    check_convergence = (res -> ((res.J_T < 1e-2) && \"Gate error < 10⁻²\")),\n    upper_bound = 50⋅2π⋅MHz,\n    lower_bound = -50⋅2π⋅MHz,\n    use_threads = true,\n)\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"result # hide","category":"page"},{"location":"tutorial/#Optimization-Result","page":"Tutorial","title":"Optimization Result","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The call to GRAPE.optimize returns a results-object from which we can extract the optimized controls:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"ϵ_opt = result.optimized_controls[1] + 𝕚 * result.optimized_controls[2];\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"While the guess controls ϵ_re_guess and ϵ_im_guess that we defined when setting up the system Hamiltonian could have been either functions (as they were) or vectors of pulse values, the GRAPE method inherently discretizes all control to a time grid (the method is piecewise-constant, by definition). Thus, the optimized_controls are vectors of control values for each point in tlist","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Also remember that we used a ShapedAmplitude when setting up the Hamiltonian, with the definition Ω(t) = S(t)ϵ(t). Thus, to get the physical optimized control field, we need to multiply with that same shape S(t), now also discretized to the time grid.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ω_opt = ϵ_opt .* discretize(Ω_re_guess.shape, tlist)\n\nfig = plot_complex_pulse(tlist, Ω_opt)\nusing DisplayAs; fig |> DisplayAs.SVG # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can see how the optimization has tuned both the amplitude and the complex phase of the microwave field, while retaining the overall shape S(t). We can also simulate again the dynamics of the 10 state under the optimized fields:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ĥ_opt = transmon_hamiltonian(;\n    δ₁ = 100⋅2π⋅MHz,\n    δ₂ = -100⋅2π⋅MHz,\n    J = 3⋅2π⋅MHz,\n    Ω_re = real.(Ω_opt),\n    Ω_im = imag.(Ω_opt),\n)\n\npops = propagate(\n    basis[3], Ĥ_opt, tlist; method=Cheby, storage=true, observables=(Ψ -> Array(abs2.(Ψ)), )\n)\n\nfig = plot(tlist ./ ns, pops'; label=[\"00\" \"01\" \"10\" \"11\"], xlabel=\"time (ns)\", legend=:outertop, legend_column=-1)\nusing DisplayAs; fig |> DisplayAs.SVG # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As expected (and demanded by the \"conditional NOT\"), 10 now evolves into 11.","category":"page"}]
}
