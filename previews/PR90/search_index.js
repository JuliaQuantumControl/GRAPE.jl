var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbrüggen and S. J. Glaser. Optimal control of coupled spin dynamics: design of NMR pulse sequences by gradient ascent algorithms. J. Magnet. Res. 172, 296 (2005).\n\n\n\nI. Kuprov and C. T. Rodgers. Derivatives of spin dynamics simulations. J. Chem. Phys. 131, 234108 (2009).\n\n\n\nP. de Fouquières, S. G. Schirmer, S. J. Glaser and I. Kuprov. Second order gradient ascent pulse engineering. J. Magnet. Res. 212, 412 (2011).\n\n\n\nD. L. Goodwin and I. Kuprov. Auxiliary matrix formalism for interaction representation transformations, optimal control, and spin relaxation theories. J. Chem. Phys. 143, 084113 (2015).\n\n\n\nM. H. Goerz, S. C. Carrasco and V. S. Malinovsky. Quantum Optimal Control via Semi-Automatic Differentiation. Quantum 6, 871 (2022).\n\n\n\nN. Leung, M. Abdelhafez, J. Koch and D. Schuster. Speedup for quantum optimal control from automatic differentiation based on graphics processing units. Phys. Rev. A 95, 042318 (2017). Implementation on GitHub at https://github.com/SchusterLab/quantum-optimal-control.\n\n\n\nM. Abdelhafez, D. I. Schuster and J. Koch. Gradient-based optimal control of open quantum systems using quantum trajectories and automatic differentiation. Phys. Rev. A 99, 052327 (2019).\n\n\n\nM. Abdelhafez, B. Baker, A. Gyenis, P. Mundada, A. A. Houck, D. Schuster and J. Koch. Universal gates for protected superconducting qubits using optimal control. Phys. Rev. A 101, 022321 (2020).\n\n\n\nC. Zhu, R. H. Byrd, P. Lu and J. Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Trans. Math. Softw. 23, 550 (1997).\n\n\n\nY. Qi and contributors. LBFGSB: Julia wrapper for L-BFGS-B Nonlinear Optimization Code (2022).\n\n\n\nD. L. Goodwin and I. Kuprov. Modified Newton-Raphson GRAPE methods for optimal control of spin systems. J. Chem. Phys. 144, 204107 (2016).\n\n\n\n","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The stable API of the GRAPE consists of the documented arguments for QuantumControl.optimize with method=GRAPE and the documented properties of the GrapeResult object.","category":"page"},{"location":"api/","page":"API","title":"API","text":"The remaining functions in GRAPE documented below should not be considered part of the stable API. They are guaranteed to be stable in bugfix (x.y.z) releases, but may change in feature releases (x.y).","category":"page"},{"location":"api/#api-index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#api-reference","page":"API","title":"Reference","text":"","category":"section"},{"location":"api/#GRAPE.GrapeResult","page":"API","title":"GRAPE.GrapeResult","text":"Result object returned by optimize_grape.\n\nAttributes\n\nThe attributes of a GrapeResult object include\n\niter:  The number of the current iteration\nJ_T: The value of the final-time functional in the current iteration\nJ_T_prev: The value of the final-time functional in the previous iteration\nJ_a: The value of the running cost J_a in the current iteration (excluding λ_a)\nJ_a_prev: The value of J_a in the previous iteration\ntlist: The time grid on which the control are discetized.\nguess_controls: A vector of the original control fields (each field discretized to the points of tlist)\noptimized_controls: A vector of the optimized control fields in the current iterations\nrecords: A vector of tuples with values returned by a callback routine passed to optimize\nconverged: A boolean flag on whether the optimization is converged. This may be set to true by a check_convergence function.\nmessage: A message string to explain the reason for convergence. This may be set by a check_convergence function.\n\nAll of the above attributes may be referenced in a check_convergence function passed to optimize(problem; method=GRAPE)\n\n\n\n\n\n","category":"type"},{"location":"api/#GRAPE.GrapeWrk","page":"API","title":"GRAPE.GrapeWrk","text":"GRAPE Workspace.\n\nThe workspace is for internal use. However, it is also accessible in a callback function. The callback may use or modify some of the following attributes:\n\ntrajectories: a copy of the trajectories defining the control problem\nadjoint_trajectories: The trajectories with the adjoint generator\nkwargs: The keyword arguments from the ControlProblem or the call to optimize.\ncontrols: A tuple of the original controls (probably functions)\npulsevals_guess: The combined vector of pulse values that are the guess in the current iteration. Initially, the vector is the concatenation of discretizing controls to the midpoints of the time grid.\npulsevals: The combined vector of updated pulse values in the current iteration. All the initialized propagators inside the workspace alias pulsevals such that mutating pulsevals is directly reflected in the next propagation step.\ngradient: The total gradient for the guess in the current iteration\ngrad_J_T: The current  gradient for the final-time part of the functional. This is from the last evaluation of the gradient, which may be for the optimized pulse (depending on the internal of the optimizer)\ngrad_J_a: The current  gradient for the running cost part of the functional.\nJ_parts: The two-component vector J_T J_a\nresult: The current result object\nupper_bounds: Upper bound for every pulsevals; +Inf indicates no bound.\nlower_bounds: Lower bound for every pulsevals; -Inf indicates no bound.\nfg_count: A two-element vector containing the number of evaluations of the combined gradient and functional first, and the evaluations of only the functional second.\noptimizer: The backend optimizer object\noptimizer_state: The internal state object of the optimizer (nothing if the optimizer has no internal state)\nresult: The current result object\ntau_grads: The gradients ∂τₖ/ϵₗ(tₙ)\nfw_storage: The storage of states for the forward propagation, as a vector of storage contains (one for each trajectory)\nfw_propagators: The propagators used for the forward propagation\nbw_grad_propagators: The propagators used for the backward propagation of QuantumGradientGenerators.GradVector states (gradient_method=:gradgen only)\nbw_propagators: The propagators used for the backward propagation (gradient_method=:taylor only)\nuse_threads: Flag indicating whether the propagations are performed in parallel.\n\nIn addition, the following methods provide safer (non-mutating) access to information in the workspace\n\nstep_width\nsearch_direction\nnorm_search\ngradient\npulse_update\n\n\n\n\n\n","category":"type"},{"location":"api/#GRAPE.evaluate_functional-Tuple{Any, Any, Any}","page":"API","title":"GRAPE.evaluate_functional","text":"Evaluate the optimization functional in problem for the given pulsevals.\n\nJ = evaluate_functional(pulsevals, problem, wrk; storage=nothing, count_call=true)\n\nevaluates the functional defined in problem, for the given pulse values, using wrk.fw_propagators, where wrk is the GRAPE workspace initialized from problem. The pulsevals is a vector of Float64 values corresponding to a concatenation of all the controls in problem, discretized to the midpoints of the time grid, cf. GrapeWrk.\n\nAs a side effect, the evaluation sets the following information in wrk:\n\nwrk.pulsevals: On output, the values of the given pulsevals. Note that pulsevals may alias wrk.pulsevals, so there is no assumption made on wrk.pulsevals other than that mutating wrk.pulsevals directly affects the propagators in wrk.\nwrk.result.f_calls: Will be incremented by one (only if count_call=true)\nwrk.fg_count[2]: Will be incremented by one (only if count_call=true)\nwrk.result.tau_vals: For any trajectory that defines a target_state, the overlap of the propagated state with that target state.\nwrk.J_parts: The parts (J_T, λₐJ_a) of the functional\n\nIf storage is given, as a vector of storage containers suitable for propagate (one for each trajectory), the forward-propagated states     will be stored there.\n\nReturns J as sum(wrk.J_parts).\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.evaluate_gradient!-NTuple{4, Any}","page":"API","title":"GRAPE.evaluate_gradient!","text":"Evaluate the gradient Jϵₙₗ into G, together with the functional J.\n\nJ = evaluate_gradient!(G, pulsevals, problem, wrk)\n\nevaluates and returns the optimization functional defined in problem for the given pulse values, cf. evaluate_functional, and write the derivative of the optimization functional with respect to the pulse values into the existing array G.\n\nThe evaluation of the functional uses uses wrk.fw_propagators. The evaluation of the gradient happens either via a backward propagation of an extended \"gradient vector\" using wrk.bw_grad_propagators if problem was initialized with gradient_method=:gradgen. Alternatively, if problem was initialized with gradient_method=:taylor, the backward propagation if for a regular state, using wrk.bw_propagators, and a Taylor expansion is used for the gradient of the time evolution operator in a single time step.\n\nAs a side, effect, evaluating the gradient and functional sets the following information in wrk:\n\nwrk.pulsevals: On output, the values of the given pulsevals, see evaluate_functional.\nwrk.result.fg_calls: Will be incremented by one\nwrk.fg_count[1]: Will be incremented by one\nwrk.result.tau_vals: For any trajectory that defines a target_state, the overlap of the propagated state with that target state.\nwrk.J_parts: The parts (J_T, λₐJ_a) of the functional\nwrk.fw_storage: For each trajectory, the forward-propagated states at each point on the time grid.\nwrk.chi_states: The normalized states χ(T) that we used as the boundary condition for the backward propagation.\nwrk.chi_states_norm: The original norm of the states χ(T), as calculated by -JΨₖ\nwrk.grad_J_T: The vector ``∂JT/∂ϵ{nl}, i.e., the gradient only for the final-time part of the functional\nwrk.grad_J_a: The vector J_aϵ_nl, i.e., the gradient only for the pulse-dependent running cost.\n\nThe gradients are wrk.grad_J_T and wrk.grad_J_a (weighted by λ_a) into are combined into the output G.\n\nReturns the value of the functional.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.gradient-Tuple{Any}","page":"API","title":"GRAPE.gradient","text":"The gradient in the current iteration.\n\ng = gradient(wrk; which=:initial)\n\nreturns the gradient associated with the guess pulse of the current iteration. Up to quasi-Newton corrections, the negative gradient determines the search_direction for the pulse_update.\n\ng = gradient(wrk; which=:final)\n\nreturns the gradient associated with the optimized pulse of the current iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.make_grape_print_iters-Tuple{}","page":"API","title":"GRAPE.make_grape_print_iters","text":"Print optimization progress as a table.\n\nThis functions serves as the default info_hook for an optimization with GRAPE.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.norm_search-Tuple{Any}","page":"API","title":"GRAPE.norm_search","text":"The norm of the search direction vector in the current iteration.\n\nnorm_search(wrk)\n\nreturns norm(search_direction(wrk)).\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.optimize_grape-Tuple{Any}","page":"API","title":"GRAPE.optimize_grape","text":"See optimize(problem; method=GRAPE, kwargs...).\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.pulse_update-Tuple{Any}","page":"API","title":"GRAPE.pulse_update","text":"The vector of pulse update values for the current iteration.\n\nΔu = pulse_update(wrk)\n\nreturns a vector containing the different between the optimized pulse values and the guess pulse values of the current iteration. This should be proportional to search_direction with the proportionality factor step_width.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.search_direction-Tuple{Any}","page":"API","title":"GRAPE.search_direction","text":"The search direction used in the current iteration.\n\ns = search_direction(wrk)\n\nreturns the vector describing the search direction used in the current iteration. This should be proportional to pulse_update with the proportionality factor step_width.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.step_width-Tuple{Any}","page":"API","title":"GRAPE.step_width","text":"The step width used in the current iteration.\n\nα = step_width(wrk)\n\nreturns the scalar α so that pulse_update(wrk) = α * search_direction(wrk), see pulse_update and search_direction for the iteration described by the current GrapeWrk (for the state of wrk as available in the callback of the current iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.vec_angle-Union{Tuple{P}, Tuple{T}, Tuple{N}, Tuple{P, P}} where {N, T, P<:Union{AbstractVector{T}, NTuple{N, T}}}","page":"API","title":"GRAPE.vec_angle","text":"The angle between two vectors.\n\nϕ = vec_angle(v1, v2; unit=:rad)\n\nreturns the angle between two vectors in radians (or degrees, with unit=:degree).\n\n\n\n\n\n","category":"method"},{"location":"api/#QuantumControl.optimize-Tuple{Any, Val{:GRAPE}}","page":"API","title":"QuantumControl.optimize","text":"using GRAPE\nresult = optimize(problem; method=GRAPE, kwargs...)\n\noptimizes the given control problem via the GRAPE method, by minimizing the functional\n\nJ(ϵ_nl) = J_T(ϕ_k(T)) + λ_a J_a(ϵ_nl)\n\nwhere the final time functional J_T depends explicitly on the forward-propagated states and the running cost J_a depends explicitly on pulse values ϵ_nl of the l'th control discretized on the n'th interval of the time grid.\n\nReturns a GrapeResult.\n\nKeyword arguments that control the optimization are taken from the keyword arguments used in the instantiation of problem; any of these can be overridden with explicit keyword arguments to optimize.\n\nRequired problem keyword arguments\n\nJ_T: A function J_T(Ψ, trajectories) that evaluates the final time functional from a list Ψ of forward-propagated states and problem.trajectories. The function J_T may also take a keyword argument tau. If it does, a vector containing the complex overlaps of the target states (target_state property of each trajectory in problem.trajectories) with the propagated states will be passed to J_T.\n\nOptional problem keyword arguments\n\nchi: A function chi(Ψ, trajectories) that receives a list Ψ of the forward propagated states and returns a vector of states χₖ = -J_TΨₖ. If not given, it will be automatically determined from J_T via QuantumControl.Functionals.make_chi with the default parameters. Similarly to J_T, if chi accepts a keyword argument tau, it will be passed a vector of complex overlaps.\nchi_min_norm=1e-100: The minimum allowable norm for any χₖ(T). Smaller norms would mean that the gradient is zero, and will abort the optimization with an error.\nJ_a: A function J_a(pulsevals, tlist) that evaluates running costs over the pulse values, where pulsevals are the vectorized values ϵ_nl, where n are in indices of the time intervals and l are the indices over the controls, i.e., [ϵ₁₁, ϵ₂₁, …, ϵ₁₂, ϵ₂₂, …] (the pulse values for each control are contiguous). If not given, the optimization will not include a running cost.\ngradient_method=:gradgen: One of :gradgen (default) or :taylor. With gradient_method=:gradgen, the gradient is calculated using QuantumGradientGenerators. With gradient_method=:taylor, it is evaluated via a Taylor series, see Eq. (20) in Kuprov and Rogers,  J. Chem. Phys. 131, 234108 (2009) [2].\ntaylor_grad_max_order=100: If given with gradient_method=:taylor, the maximum number of terms in the Taylor series. If taylor_grad_check_convergence=true (default), if the Taylor series does not convergence within the given number of terms, throw an an error. With taylor_grad_check_convergence=true, this is the exact order of the Taylor series.\ntaylor_grad_tolerance=1e-16: If given with gradient_method=:taylor and taylor_grad_check_convergence=true, stop the Taylor series when the norm of the term falls below the given tolerance. Ignored if taylor_grad_check_convergence=false.\ntaylor_grad_check_convergence=true: If given as true (default), check the convergence after each term in the Taylor series an stop as soon as the norm of the term drops below the given number. If false, stop after exactly taylor_grad_max_order terms.\nlambda_a=1: A weight for the running cost J_a.\ngrad_J_a: A function to calculate the gradient of J_a. If not given, it will be automatically determined. See make_grad_J_a for the required interface.\nupper_bound: An upper bound for the value of any optimized control. Time-dependent upper bounds can be specified via pulse_options.\nlower_bound: A lower bound for the value of any optimized control. Time-dependent lower bounds can be specified via pulse_options.\npulse_options: A dictionary that maps every control (as obtained by get_controls from the problem.trajectories) to a dict with the following possible keys:\n:upper_bounds: A vector of upper bound values, one for each intervals of the time grid. Values of Inf indicate an unconstrained upper bound for that time interval, respectively the global upper_bound, if given.\n:lower_bounds: A vector of lower bound values. Values of -Inf indicate an unconstrained lower bound for that time interval,\nprint_iters=true: Whether to print information after each iteration.\nprint_iter_info=[\"iter.\", \"J_T\", \"|∇J|\", \"|Δϵ|\", \"ΔJ\", \"FG(F)\", \"secs\"]: Which fields to print if print_iters=true. If given, must be a list of header labels (strings), which can be any of the following:\n\"iter.\": The iteration number\n\"J_T\": The value of the final-time functional for the dynamics under the optimized pulses\n\"J_a\": The value of the pulse-dependent running cost for the optimized pulses\n\"λ_a⋅J_a\": The total contribution of J_a to the full functional J\n\"J\": The value of the optimization functional for the optimized pulses\n\"ǁ∇J_Tǁ\": The ℓ²-norm of the current gradient of the final-time functional. Note that this is usually the gradient of the optimize pulse, not the guess pulse.\n\"ǁ∇J_aǁ\": The ℓ²-norm of the the current gradient of the pulse-dependent running cost. For comparison with \"ǁ∇J_Tǁ\".\n\"λ_aǁ∇J_aǁ\": The ℓ²-norm of the the current gradient of the complete pulse-dependent running cost term. For comparison with \"ǁ∇J_Tǁ\".\n\"ǁ∇Jǁ\": The norm of the guess pulse gradient. Note that the guess pulse gradient is not the same the current gradient.\n\"ǁΔϵǁ\":  The ℓ²-norm of the pulse update\n\"ǁϵǁ\": The ℓ²-norm of optimized pulse values\n\"max|Δϵ|\" The maximum value of the pulse update (infinity norm)\n\"max|ϵ|\": The maximum value of the pulse values (infinity norm)\n\"ǁΔϵǁ/ǁϵǁ\": The ratio of the pulse update tothe optimized pulse values\n\"∫Δϵ²dt\": The L²-norm of the pulse update, summed over all pulses. A convergence measure comparable (proportional) to the running cost in Krotov's method\n\"ǁsǁ\": The norm of the search direction. Should be ǁΔϵǁ scaled by the step with α.\n\"∠°\": The angle (in degrees) between the negative gradient -∇J and the search direction s.\n\"α\": The step width as determined by the line search (Δϵ = α⋅s)\n\"ΔJ_T\": The change in the final time functional relative to the previous iteration\n\"ΔJ_a\":  The change in the control-dependent running cost relative to the previous iteration\n\"λ_a⋅ΔJ_a\": The change in the control-dependent running cost term relative to the previous iteration.\n\"ΔJ\":  The change in the total optimization functional relative to the previous iteration.\n\"FG(F)\":  The number of functional/gradient evaluation (FG), or pure functional (F) evaluations\n\"secs\":  The number of seconds of wallclock time spent on the iteration.\nstore_iter_info=[]: Which fields to store in result.records, given as\na list of header labels, see print_iter_info.\ncallback: A function (or tuple of functions) that receives the GRAPE workspace and the iteration number. The function may return a tuple of values which are stored in the GrapeResult object result.records. The function can also mutate the workspace, in particular the updated pulsevals. This may be used, e.g., to apply a spectral filter to the updated pulses or to perform similar manipulations. Note that print_iters=true (default) adds an automatic callback to print information after each iteration. With store_iter_info, that callback automatically stores a subset of the available information.\ncheck_convergence: A function to check whether convergence has been reached. Receives a GrapeResult object result, and should set result.converged to true and result.message to an appropriate string in case of convergence. Multiple convergence checks can be performed by chaining functions with ∘. The convergence check is performed after any callback.\nprop_method: The propagation method to use for each trajectory, see below.\nverbose=false: If true, print information during initialization\nrethrow_exceptions: By default, any exception ends the optimization, but still returns a GrapeResult that captures the message associated with the exception. This is to avoid losing results from a long-running optimization when an exception occurs in a later iteration. If rethrow_exceptions=true, instead of capturing the exception, it will be thrown normally.\n\nExperimental keyword arguments\n\nThe following keyword arguments may change in non-breaking releases:\n\nx_tol: Parameter for Optim.jl\nf_tol: Parameter for Optim.jl\ng_tol: Parameter for Optim.jl\nshow_trace: Parameter for Optim.jl\nextended_trace:  Parameter for Optim.jl\nshow_every: Parameter for Optim.jl\nallow_f_increases: Parameter for Optim.jl\noptimizer: An optional Optim.jl optimizer (Optim.AbstractOptimizer instance). If not given, an L-BFGS-B optimizer will be used.\n\nTrajectory propagation\n\nGRAPE may involve three types of propagation:\n\nA forward propagation for every Trajectory in the problem\nA backward propagation for every trajectory\nA backward propagation of a gradient generator for every trajectory.\n\nThe keyword arguments for each propagation (see propagate) are determined from any properties of each Trajectory that have a prop_ prefix, cf. init_prop_trajectory.\n\nIn situations where different parameters are required for the forward and backward propagation, instead of the prop_ prefix, the fw_prop_ and bw_prop_ prefix can be used, respectively. These override any setting with the prop_ prefix. Similarly, properties for the backward propagation of the gradient generators can be set with properties that have a grad_prop_ prefix. These prefixes apply both to the properties of each Trajectory and the problem keyword arguments.\n\nNote that the propagation method for each propagation must be specified. In most cases, it is sufficient (and recommended) to pass a global prop_method problem keyword argument.\n\n\n\n\n\n","category":"method"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"The GRAPE package is used in the context of the QuantumControl framework. You should be familiar with the concepts used in the framework and its  overview.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Set up a QuantumControl.ControlProblem with one ore more trajectories. The problem must have a set controls, see QuantumControl.Controls.get_controls(problem), that can be discretized as piecewise-constant on the intervals of the time grid, cf. QuantumPropagators.Controls.discretize_on_midpoints.\nMake sure the problem includes a well-defined final time functional J_T. The GRAPE method also requires chi to determine the boundary condition ketchi_k = partial J_T  partial braPsi_k(T). This can be determined automatically, analytically for known functions J_T, or via automatic differentiation, so it is an optional parameter.\nPropagate the system described to by problem to ensure you understand the dynamics under the guess controls!\nCall QuantumControl.optimize, or, preferably, QuantumControl.@optimize_or_load with method = GRAPE. Pass additional keyword arguments to customize GRAPE's behavior:","category":"page"},{"location":"usage/#QuantumControl.optimize-Tuple{ControlProblem, Val{:GRAPE}}-usage","page":"Usage","title":"QuantumControl.optimize","text":"using GRAPE\nresult = optimize(problem; method=GRAPE, kwargs...)\n\noptimizes the given control problem via the GRAPE method, by minimizing the functional\n\nJ(ϵ_nl) = J_T(ϕ_k(T)) + λ_a J_a(ϵ_nl)\n\nwhere the final time functional J_T depends explicitly on the forward-propagated states and the running cost J_a depends explicitly on pulse values ϵ_nl of the l'th control discretized on the n'th interval of the time grid.\n\nReturns a GrapeResult.\n\nKeyword arguments that control the optimization are taken from the keyword arguments used in the instantiation of problem; any of these can be overridden with explicit keyword arguments to optimize.\n\nRequired problem keyword arguments\n\nJ_T: A function J_T(Ψ, trajectories) that evaluates the final time functional from a list Ψ of forward-propagated states and problem.trajectories. The function J_T may also take a keyword argument tau. If it does, a vector containing the complex overlaps of the target states (target_state property of each trajectory in problem.trajectories) with the propagated states will be passed to J_T.\n\nOptional problem keyword arguments\n\nchi: A function chi(Ψ, trajectories) that receives a list Ψ of the forward propagated states and returns a vector of states χₖ = -J_TΨₖ. If not given, it will be automatically determined from J_T via QuantumControl.Functionals.make_chi with the default parameters. Similarly to J_T, if chi accepts a keyword argument tau, it will be passed a vector of complex overlaps.\nchi_min_norm=1e-100: The minimum allowable norm for any χₖ(T). Smaller norms would mean that the gradient is zero, and will abort the optimization with an error.\nJ_a: A function J_a(pulsevals, tlist) that evaluates running costs over the pulse values, where pulsevals are the vectorized values ϵ_nl, where n are in indices of the time intervals and l are the indices over the controls, i.e., [ϵ₁₁, ϵ₂₁, …, ϵ₁₂, ϵ₂₂, …] (the pulse values for each control are contiguous). If not given, the optimization will not include a running cost.\ngradient_method=:gradgen: One of :gradgen (default) or :taylor. With gradient_method=:gradgen, the gradient is calculated using QuantumGradientGenerators. With gradient_method=:taylor, it is evaluated via a Taylor series, see Eq. (20) in Kuprov and Rogers,  J. Chem. Phys. 131, 234108 (2009) [2].\ntaylor_grad_max_order=100: If given with gradient_method=:taylor, the maximum number of terms in the Taylor series. If taylor_grad_check_convergence=true (default), if the Taylor series does not convergence within the given number of terms, throw an an error. With taylor_grad_check_convergence=true, this is the exact order of the Taylor series.\ntaylor_grad_tolerance=1e-16: If given with gradient_method=:taylor and taylor_grad_check_convergence=true, stop the Taylor series when the norm of the term falls below the given tolerance. Ignored if taylor_grad_check_convergence=false.\ntaylor_grad_check_convergence=true: If given as true (default), check the convergence after each term in the Taylor series an stop as soon as the norm of the term drops below the given number. If false, stop after exactly taylor_grad_max_order terms.\nlambda_a=1: A weight for the running cost J_a.\ngrad_J_a: A function to calculate the gradient of J_a. If not given, it will be automatically determined. See make_grad_J_a for the required interface.\nupper_bound: An upper bound for the value of any optimized control. Time-dependent upper bounds can be specified via pulse_options.\nlower_bound: A lower bound for the value of any optimized control. Time-dependent lower bounds can be specified via pulse_options.\npulse_options: A dictionary that maps every control (as obtained by get_controls from the problem.trajectories) to a dict with the following possible keys:\n:upper_bounds: A vector of upper bound values, one for each intervals of the time grid. Values of Inf indicate an unconstrained upper bound for that time interval, respectively the global upper_bound, if given.\n:lower_bounds: A vector of lower bound values. Values of -Inf indicate an unconstrained lower bound for that time interval,\nprint_iters=true: Whether to print information after each iteration.\nprint_iter_info=[\"iter.\", \"J_T\", \"|∇J|\", \"|Δϵ|\", \"ΔJ\", \"FG(F)\", \"secs\"]: Which fields to print if print_iters=true. If given, must be a list of header labels (strings), which can be any of the following:\n\"iter.\": The iteration number\n\"J_T\": The value of the final-time functional for the dynamics under the optimized pulses\n\"J_a\": The value of the pulse-dependent running cost for the optimized pulses\n\"λ_a⋅J_a\": The total contribution of J_a to the full functional J\n\"J\": The value of the optimization functional for the optimized pulses\n\"ǁ∇J_Tǁ\": The ℓ²-norm of the current gradient of the final-time functional. Note that this is usually the gradient of the optimize pulse, not the guess pulse.\n\"ǁ∇J_aǁ\": The ℓ²-norm of the the current gradient of the pulse-dependent running cost. For comparison with \"ǁ∇J_Tǁ\".\n\"λ_aǁ∇J_aǁ\": The ℓ²-norm of the the current gradient of the complete pulse-dependent running cost term. For comparison with \"ǁ∇J_Tǁ\".\n\"ǁ∇Jǁ\": The norm of the guess pulse gradient. Note that the guess pulse gradient is not the same the current gradient.\n\"ǁΔϵǁ\":  The ℓ²-norm of the pulse update\n\"ǁϵǁ\": The ℓ²-norm of optimized pulse values\n\"max|Δϵ|\" The maximum value of the pulse update (infinity norm)\n\"max|ϵ|\": The maximum value of the pulse values (infinity norm)\n\"ǁΔϵǁ/ǁϵǁ\": The ratio of the pulse update tothe optimized pulse values\n\"∫Δϵ²dt\": The L²-norm of the pulse update, summed over all pulses. A convergence measure comparable (proportional) to the running cost in Krotov's method\n\"ǁsǁ\": The norm of the search direction. Should be ǁΔϵǁ scaled by the step with α.\n\"∠°\": The angle (in degrees) between the negative gradient -∇J and the search direction s.\n\"α\": The step width as determined by the line search (Δϵ = α⋅s)\n\"ΔJ_T\": The change in the final time functional relative to the previous iteration\n\"ΔJ_a\":  The change in the control-dependent running cost relative to the previous iteration\n\"λ_a⋅ΔJ_a\": The change in the control-dependent running cost term relative to the previous iteration.\n\"ΔJ\":  The change in the total optimization functional relative to the previous iteration.\n\"FG(F)\":  The number of functional/gradient evaluation (FG), or pure functional (F) evaluations\n\"secs\":  The number of seconds of wallclock time spent on the iteration.\nstore_iter_info=[]: Which fields to store in result.records, given as\na list of header labels, see print_iter_info.\ncallback: A function (or tuple of functions) that receives the GRAPE workspace and the iteration number. The function may return a tuple of values which are stored in the GrapeResult object result.records. The function can also mutate the workspace, in particular the updated pulsevals. This may be used, e.g., to apply a spectral filter to the updated pulses or to perform similar manipulations. Note that print_iters=true (default) adds an automatic callback to print information after each iteration. With store_iter_info, that callback automatically stores a subset of the available information.\ncheck_convergence: A function to check whether convergence has been reached. Receives a GrapeResult object result, and should set result.converged to true and result.message to an appropriate string in case of convergence. Multiple convergence checks can be performed by chaining functions with ∘. The convergence check is performed after any callback.\nprop_method: The propagation method to use for each trajectory, see below.\nverbose=false: If true, print information during initialization\nrethrow_exceptions: By default, any exception ends the optimization, but still returns a GrapeResult that captures the message associated with the exception. This is to avoid losing results from a long-running optimization when an exception occurs in a later iteration. If rethrow_exceptions=true, instead of capturing the exception, it will be thrown normally.\n\nExperimental keyword arguments\n\nThe following keyword arguments may change in non-breaking releases:\n\nx_tol: Parameter for Optim.jl\nf_tol: Parameter for Optim.jl\ng_tol: Parameter for Optim.jl\nshow_trace: Parameter for Optim.jl\nextended_trace:  Parameter for Optim.jl\nshow_every: Parameter for Optim.jl\nallow_f_increases: Parameter for Optim.jl\noptimizer: An optional Optim.jl optimizer (Optim.AbstractOptimizer instance). If not given, an L-BFGS-B optimizer will be used.\n\nTrajectory propagation\n\nGRAPE may involve three types of propagation:\n\nA forward propagation for every Trajectory in the problem\nA backward propagation for every trajectory\nA backward propagation of a gradient generator for every trajectory.\n\nThe keyword arguments for each propagation (see propagate) are determined from any properties of each Trajectory that have a prop_ prefix, cf. init_prop_trajectory.\n\nIn situations where different parameters are required for the forward and backward propagation, instead of the prop_ prefix, the fw_prop_ and bw_prop_ prefix can be used, respectively. These override any setting with the prop_ prefix. Similarly, properties for the backward propagation of the gradient generators can be set with properties that have a grad_prop_ prefix. These prefixes apply both to the properties of each Trajectory and the problem keyword arguments.\n\nNote that the propagation method for each propagation must be specified. In most cases, it is sufficient (and recommended) to pass a global prop_method problem keyword argument.\n\n\n\n\n\n","category":"method"},{"location":"background/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The GRAPE methods minimizes an optimization functional of the form","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrape-functional\nJ(ϵ_l(t))\n    = J_T(Ψ_k(T))\n    + λ_a  underbrace_l int_0^T g_a(ϵ_l(t))  dt_=J_a(ϵ_l(t))\n    + λ_b  underbrace_k int_0^T g_b(Ψ_k(t))  dt_=J_b(Ψ_k(t))\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"where ϵ_l(t) is a set of control functions defined between the initial time t=0 and the final time t=T, and Ψ_k(t) is a set of \"trajectories\" evolving from a set of initial states Psi_k(t=0) under the controls ϵ_l(t). The primary focus is on the final-time functional J_T, but running costs J_a (weighted by λ_a) may be included penalize certain features of the control field. In principle, a state-dependent running cost J_b weighted by λ_b can also be included (and will be discussed below), although this is currently not fully implemented in GRAPE.jl.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The defining assumptions of the GRAPE method are","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The control fields epsilon_l(t) are piecewise-constant on the intervals of a time grid. That is, we have a vector of pulse values with elements epsilon_nl. We use the double-index nl, for the value of the l'th control field on the n'th interval of the time grid.\nThe states ketPsi_k(t) evolve under an equation of motion of the form","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqtdse\n    i hbar fracpartial ketPsi_k(t)partial t = hatH_k(epsilon_l(t)) ketPsi_k(t)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"This includes the Schrödinger equation, but also the Liouville equation for open quantum systems. In the latter case ketPsi_k is replaced by a vectorized density matrix, and hatH_k is replaced by a Liouvillian (super-) operator describing the dynamics of the k'th trajectory. The crucial point is that Eq. \\eqref{eq:tdse} can be solved analytically within each time interval as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqtime-evolution-op\n    ketPsi_k(t_n+1) = underbraceexpleft-i hatH_kn dt_n right_=hatU_kn ketPsi_k(t_n)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"These two assumptions allow to analytically derive the gradient (nabla J)_nl equiv fracpartial Jpartial epsilon_nl. The initial derivation of GRAPE by Khaneja et al. [1] focuses on a final-time functional J_T that depends of the overlap of each forward-propagated ketPsi_k(T) with a target state ketPsi^texttgt_k(T) and updates the pulse values epsilon_nl directly in the direction of the negative gradient. Improving on this, de Fouquières et al. [3] showed that using a quasi-Newton method to update the pulses based on the gradient information leads to a dramatic improvement in convergence and stability. Furthermore, Goodwin and Kuprov [4] improved on the precision of evaluating the gradient of a local time evolution operator, which is a critical step in the GRAPE scheme. Finally, Goerz et al. [5] generalized GRAPE to arbitrary functionals of the form \\eqref{eq:grape-functional}, bridging the gap to automatic differentiation techniques [6–8] by introducing the technique of \"semi-automatic differentiation\". This most general derivation is the basis for the implementation in GRAPE.jl, and is reproduced below.","category":"page"},{"location":"background/#Prerequisite:-Wirtinger-derivatives-and-matrix-calculus","page":"Background","title":"Prerequisite: Wirtinger derivatives and matrix calculus","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Even though we are seeking the derivative of the real-valued functional J with respect to the real-valued parameter epsilon_nl, the functional still involves complex quantities via ketPsi_k(t) and hatH in Eq. \\eqref{eq:tdse}. In order to apply the chain rule in the derivation of the gradient, we will have to clarify the notion of derivatives in the context of complex numbers, as well as derivatives with respect to vectors (\"matrix calculus\").","category":"page"},{"location":"background/#Derivatives-w.r.t.-complex-scalars","page":"Background","title":"Derivatives w.r.t. complex scalars","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"To illustrate, let's say we introduce intermediary scalar variables z_k in mathbbC in the functional, J(epsilon_nl) rightarrow J(z_k(epsilon_nl)), with J epsilon_nl in mathbbR.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In principle, one must separate the z_k into real and imaginary part as independent variables, J = J(Rez_k Imz_k), resulting in","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  labeleqgrad_zj_real_imag\n  (nabla J)_nl\n  equiv fracpartial Jpartial epsilon_nl\n  = sum_k left(\n    fracpartial Jpartial Rez_k\n    fracpartial Rez_kpartial epsilon_nl\n    + fracpartial Jpartial Imz_k\n    fracpartial Imz_kpartial epsilon_nl\n    right)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"An elegant alternative is to introduce Wirtinger derivatives,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"defiimathrmi\nbeginalign\n  labeleqwirtinger1\n  fracpartial Jpartial z_k\n    equiv frac12 left(\n      fracpartial Jpartial Rez_k\n      -ii fracpartial Jpartial Imz_k\n      right) \n  labeleqwirtinger2\n  fracpartial Jpartial z_k^*\n    equiv frac12 left(\n      fracpartial Jpartial Rez_k\n      +ii fracpartial Jpartial Imz_k\n      right)\n    = left(fracpartial Jpartial z_kright)^*\nendalign","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"which instead treats z_k and the conjugate value z_k^* as independent variables, so that","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  labeleqwirtinger_chainrule\n  fracpartial Jpartial epsilon_nl\n  = sum_k left(\n    fracpartial Jpartial z_k\n    fracpartial z_kpartial epsilon_nl\n    + fracpartial Jpartial z_k^*\n    fracpartial z_k^*partial epsilon_nl\n    right)\n  = 2 Re sum_k fracpartial Jpartial z_k\n    fracpartial z_kpartial epsilon_nl\n    \nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"So, we have a simple chain rules, modified only by 2 Re, where we can otherwise \"forget\" that z_k is a complex variable. The fact that J in mathbbR guarantees that z_k and z_k^* can only occur in such ways that we don't have to worry about having \"lost\" z_k^*.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The derivative of the complex value z_k with respect to the real value epsilon_nl is defined straightforwardly as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  fracpartial z_kpartial epsilon_nl\n  equiv\n  fracpartial Rez_kpartial epsilon_nl\n  + ii fracpartial Imz_kpartial epsilon_nl\nendequation","category":"page"},{"location":"background/#Derivatives-w.r.t.-complex-vectors","page":"Background","title":"Derivatives w.r.t. complex vectors","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"We can now go one step further and allow for intermediate variables that are complex vectors instead of scalars, J(epsilon_nl) rightarrow J(ketPsi_k(epsilon_nl)). Taking the derivative w.r.t. a vector puts us in the domain of matrix calculus. Fundamentally, the derivative of a scalar with respect to a (column) vector is a (row) vector consisting of the derivatives of the scalar w.r.t. the components of the vector, and the derivative of a vector w.r.t. a scalar is the obvious vector of derivatives.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Usually, matrix calculus assumes real-valued vectors, but the extension to complex vectors via the Wirtinger derivatives discussed above is a relatively straightforward. The use of Dirac (\"braket\") notation helps tremendously here: ketPsi_k describes a complex column vector, and braPsi_k describes the corresponding row vector with complex-conjugated elements. These can take the place of z_k and z_k^* in the Wirtinger derivative. Consider, e.g.,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqJsm\nJ(ketPsi_k)\n= sum_k vert langle Psi_k vert Psi_k^texttgt rangle vert^2\n= sum_k langle Psi_k vert Psi_k^texttgt rangle langle Psi_k^texttgt vert Psi_k rangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"for a fixed set of \"target states\" ketPsi_k^texttgt.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The derivative partial Jpartial ketPsi_k is","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqdJ_dKet\nfracpartial Jpartial ketPsi_k = langle Psi_k vert Psi_k^texttgt rangle langlePsi_k^texttgtvert\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"in the same sense as Eq. \\eqref{eq:wirtinger1}. We simply treat ketPsi_k and braPsi_k as independent variables corresponding to z_k and z_k^*. Note that the result is a \"bra\", that is, a co-state, or row vector. The braket notation resolves the question of \"layout conventions\" in matrix calculus in favor of the \"numerator layout\". Consequently, we also have a well-defined derivative w.r.t. the co-state:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\nfracpartial Jpartial braPsi_k = langle Psi_k^texttgt vert Psi_k rangle vertPsi_k^texttgtrangle\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"which we can either get explicitly from Eq. \\eqref{eq:Jsm}, differentiating w.r.t. ketPsi_k as an independent parameter and changing the order of the factors, or implicitly by taking the conjugate transpose of Eq. \\eqref{eq:dJ_dKet}.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"For the full chain rule of a functional J(ketPsi_k(epsilon_nl)), we thus find","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqgrad-via-chi1\n  (nabla J)_nl\n  equiv fracpartial Jpartial epsilon_nl\n  = 2 Re sum_k left(\n    fracpartial Jpartial ketPsi_k\n    fracpartial ketPsi_kpartial epsilon_nl\n  right)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"With the definition in Eq. \\eqref{eq:wirtinger1}, this corresponds directly to the scalar","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n  fracpartial Jpartial epsilon_nl\n  = sum_km left(\n    fracpartial Jpartial RePsi_km\n    fracpartial RePsi_kmpartial epsilon_nl\n    + fracpartial Jpartial ImPsi_km\n    fracpartial ImPsi_kmpartial epsilon_nl\n  right)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"where the complex scalar Psi_km is the m'th element of the k'th vector, and corresponds to the z_k in Eq. \\eqref{eq:wirtinger_chainrule}.","category":"page"},{"location":"background/#Gradients-for-final-time-functionals","page":"Background","title":"Gradients for final-time functionals","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"For simplicity, we consider a functional defined entirely at final time T, the J_T term in Eq. \\eqref{eq:grape-functional}. Since J_T depends explicitly on ketPsi_k(T) and only implicitly on epsilon_nl, we can use the complex chain rule in Eq. \\eqref{eq:grad-via-chi1}.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Further, we define a new state","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequationlabeleqchi\nketchi_k(T) equiv - fracpartial J_Tpartial braPsi_k(T)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The minus sign in this definition is arbitrary, and is intended solely to match an identical definition in Krotov's method, the most direct alternative to GRAPE. Since ketchi_k(T) does not depend on epsilon_nl, we can pull forward the derivative partial  partial epsilon_nl in Eq. \\eqref{eq:grad-via-chi1}, writing it as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginequation\n(nabla J_T)_nl\n= fracpartial J_Tpartial epsilon_nl\n= - 2 Re sum_k fracpartialpartial epsilon_nl Braketchi_k(T)  Psi_k(T)\nendequation","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"We end up with the gradient of J_T being the derivative of the overlap of two states ketchi_k(T) and ketPsi_k(T) at final time T.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Next, we make use the assumption that the time evolution is piecewise constant, so that we can use the time evolution operator defined in Eq. \\eqref{eq:time-evolution-op} to write ketPsi_k(T) as the time evolution of an initial state Psi_k(t=0), the initial_state of the k'th trajectory in the QuantumControl.ControlProblem.","category":"page"},{"location":"background/#Derivative-of-the-time-evolution-operator","page":"Background","title":"Derivative of the time-evolution operator","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Taylor\nSchirmer-gradient\nComment on first-order Taylor, Lagrange multipliers, Gross–Pitaevskii equation","category":"page"},{"location":"background/#GRAPE-scheme","page":"Background","title":"GRAPE scheme","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"It results in an efficient numerical scheme for evaluating the full gradient [5, Figure 1(a)]. The scheme extends to situations where the functional is evaluated on top of multiple propagated states vert Psi_k(t) rangle with an index k, and multiple controls epsilon_l(t), resulting in a vector of values epsilon_nl with a double-index nl. Once the gradient has been evaluated, in the original formulation of GRAPE [1], the values epsilon_nl would then be updated by taking a step with a fixed step width alpha in the direction of the negative gradient, to iteratively minimize the value of the optimization functional J. In practice, the gradient can also be fed into an arbitrary gradient-based optimizer, and in particular a quasi-Newton method like L-BFGS-B [9, 10]. This results in a dramatic improvement in stability and convergence [3], and is assumed as the default in GRAPE.jl. Gradients of the time evolution operator can be evaluated to machine precision following Goodwin and Kuprov [4]. The GRAPE method could also be extended to a true Hessian of the optimization functional [11], which would be in scope for future versions of GRAPE.jl.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"<p id=\"fig-grape-scheme\" style=\"text-align: center\">\n<a href=\"../fig/grape_scheme.png\">\n<img src=\"../fig/grape_scheme.png\" width=\"100%\"/>\n</a>\n<a href=\"#fig-grape-scheme\">Figure 1</a>: Numerical scheme for the evaluation of the gradient in GRAPE, with semi-automatic differentiation\n</p>","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"See the scheme depicted in Fig. 1.","category":"page"},{"location":"background/#Semi-automatic-differentiation","page":"Background","title":"Semi-automatic differentiation","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Same as GRAPE, up to definition of chi. Special cases for overlap functionals and gate functionals.","category":"page"},{"location":"background/#Running-costs","page":"Background","title":"Running costs","text":"","category":"section"},{"location":"background/#Comparison-with-Krotov's-method","page":"Background","title":"Comparison with Krotov's method","text":"","category":"section"},{"location":"#GRAPE.jl","page":"Home","title":"GRAPE.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\n\nVERSION = Pkg.dependencies()[Base.UUID(\"6b52fcaf-80fe-489a-93e9-9f92080510be\")].version\n\ngithub_badge = \"[![Github](https://img.shields.io/badge/JuliaQuantumControl-GRAPE.jl-blue.svg?logo=github)](https://github.com/JuliaQuantumControl/GRAPE.jl)\"\n\nversion_badge = \"![v$VERSION](https://img.shields.io/badge/version-v$VERSION-green.svg)\"\n\nMarkdown.parse(\"$github_badge $version_badge\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Gradient Ascent Pulse Engineering in Julia","category":"page"},{"location":"#Summary","page":"Home","title":"Summary","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The GRAPE.jl package implements Gradient Ascent Pulse Engineering [1], a widely used method of quantum optimal control. The quantum state of a system can be described by a complex vector ketPsi(t) that evolves under a differential equation of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"defiimathrmi\nbeginequationlabeleqtdse\nii hbar fracpartial ketPsi(t)partial t = hatH(epsilon(t)) ketPsi(t)\nendequation","category":"page"},{"location":"","page":"Home","title":"Home","text":"where hbar is the reduced Planck constant and hatH is a matrix whose elements depend in some way on the control function epsilon(t). We generally know the initial state of the system ketPsi(t=0) and want to find an epsilon(t) that minimizes some functional J that depends on the states at some final time T, as well as running costs on ketPsi(t) and values of epsilon(t) at intermediate times.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The defining feature of the GRAPE method is that it considers epsilon(t) as piecewise constant, i.e., as a vector of pulse values epsilon_n, for the n'th interval of the time grid. This allows solving Eq. \\eqref{eq:tdse} analytically for each time interval, and deriving an expression for the gradient partial J  partial epsilon_n of the optimization functional with respect to the values of the control field. The pulse values are then updated based on the gradient, in an efficient scheme detailed in Background.","category":"page"},{"location":"#Statement-of-Need","page":"Home","title":"Statement of Need","text":"","category":"section"},{"location":"#Related-Software","page":"Home","title":"Related Software","text":"","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Depth = 2\nPages = [pair[2] for pair in Main.PAGES[2:end-1]]","category":"page"},{"location":"#History","page":"Home","title":"History","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the Releases on Github.","category":"page"}]
}
