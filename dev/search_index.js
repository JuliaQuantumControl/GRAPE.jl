var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbrüggen and S. J. Glaser. Optimal control of coupled spin dynamics: design of NMR pulse sequences by gradient ascent algorithms. J. Magnet. Res. 172, 296 (2005).\n\n\n\nP. de Fouquières, S. G. Schirmer, S. J. Glaser and I. Kuprov. Second order gradient ascent pulse engineering. J. Magnet. Res. 212, 412 (2011).\n\n\n\nM. H. Goerz, S. C. Carrasco and V. S. Malinovsky. Quantum Optimal Control via Semi-Automatic Differentiation. Quantum 6, 871 (2022).\n\n\n\nI. Kuprov and C. T. Rodgers. Derivatives of spin dynamics simulations. J. Chem. Phys. 131, 234108 (2009).\n\n\n\n","category":"page"},{"location":"api/","page":"API","title":"API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [GRAPE]","category":"page"},{"location":"api/#GRAPE.GrapeResult","page":"API","title":"GRAPE.GrapeResult","text":"Result object returned by optimize_grape.\n\nAttributes\n\nThe attributes of a GrapeResult object include\n\niter:  The number of the current iteration\nJ_T: The value of the final-time functional in the current iteration\nJ_T_prev: The value of the final-time functional in the previous iteration\ntlist: The time grid on which the control are discetized.\nguess_controls: A vector of the original control fields (each field discretized to the points of tlist)\noptimized_controls: A vector of the optimized control fields in the current iterations\nrecords: A vector of tuples with values returned by a callback routine passed to optimize\nconverged: A boolean flag on whether the optimization is converged. This may be set to true by a check_convergence function.\nmessage: A message string to explain the reason for convergence. This may be set by a check_convergence function.\n\nAll of the above attributes may be referenced in a check_convergence function passed to optimize(problem; method=GRAPE)\n\n\n\n\n\n","category":"type"},{"location":"api/#GRAPE.GrapeWrk","page":"API","title":"GRAPE.GrapeWrk","text":"GRAPE Workspace.\n\nThe workspace is for internal use. However, it is also accessible in a callback function. The callback may use or modify some of the following attributes:\n\ntrajectories: a copy of the trajectories defining the control problem\nadjoint_trajectories: The trajectories with the adjoint generator\nkwargs: The keyword arguments from the ControlProblem or the call to optimize.\ncontrols: A tuple of the original controls (probably functions)\npulsevals_guess: The combined vector of pulse values that are the guess in the current iteration. Initially, the vector is the concatenation of discretizing controls to the midpoints of the time grid.\npulsevals: The combined vector of updated pulse values in the current iteration.\ngradient: The total gradient for the guess in the current iteration\ngrad_J_T: The current gradient for the final-time part of the functional.\ngrad_J_a: The current gradient for the running cost part of the functional.\nJ_parts: The two-component vector J_T J_a\nresult: The current result object\nupper_bounds: Upper bound for every pulsevals; +Inf indicates no bound.\nlower_bounds: Lower bound for every pulsevals; -Inf indicates no bound.\nfg_count: The total number of evaluations of the functional and evaluations of the gradient, as a two-element vector.\noptimizer: The backend optimizer object\noptimizer_state: The internal state object of the optimizer (nothing if the optimizer has no internal state)\nresult: The current result object\ntau_grads: The gradients ∂τₖ/ϵₗ(tₙ)\nfw_storage: The storage of states for the forward propagation\nfw_propagators: The propagators used for the forward propagation\nbw_propagators: The propagators used for the backward propagation\nuse_threads: Flag indicating whether the propagations are performed in parallel.\n\nIn addition, the following methods provide safer (non-mutating) access to information in the workspace\n\nstep_width\nsearch_direction\ngradient\npulse_update\n\n\n\n\n\n","category":"type"},{"location":"api/#GRAPE.gradient-Tuple{Any}","page":"API","title":"GRAPE.gradient","text":"The gradient in the current iteration.\n\ng = gradient(wrk; which=:initial)\n\nreturns the gradient associated with the guess pulse of the current iteration. Up to quasi-Newton corrections, the negative gradient determines the search_direction for the pulse_update.\n\ng = gradient(wrk; which=:final)\n\nreturns the gradient associated with the optimized pulse of the current iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.make_grape_print_iters-Tuple{}","page":"API","title":"GRAPE.make_grape_print_iters","text":"Print optimization progress as a table.\n\nThis functions serves as the default info_hook for an optimization with GRAPE.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.optimize_grape-Tuple{Any}","page":"API","title":"GRAPE.optimize_grape","text":"See optimize(problem; method=GRAPE, kwargs...).\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.pulse_update-Tuple{Any}","page":"API","title":"GRAPE.pulse_update","text":"The vector of pulse update values for the current iteration.\n\nΔu = pulse_update(wrk)\n\nreturns a vector containing the different between the optimized pulse values and the guess pulse values of the current iteration. This should be proportional to search_direction with the proportionality factor step_width.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.search_direction-Tuple{Any}","page":"API","title":"GRAPE.search_direction","text":"The search direction used in the current iteration.\n\ns = search_direction(wrk)\n\nreturns the vector describing the search direction used in the current iteration. This should be proportional to pulse_update with the proportionality factor step_width.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.step_width-Tuple{Any}","page":"API","title":"GRAPE.step_width","text":"The step width used in the current iteration.\n\nα = step_width(wrk)\n\nreturns the scalar α so that pulse_update(wrk) = α * search_direction(wrk), see pulse_update and search_direction for the iteration described by the current GrapeWrk (for the state of wrk as available in the callback of the current iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#GRAPE.vec_angle-Union{Tuple{P}, Tuple{T}, Tuple{N}, Tuple{P, P}} where {N, T, P<:Union{AbstractVector{T}, Tuple{Vararg{T, N}}}}","page":"API","title":"GRAPE.vec_angle","text":"The angle between two vectors.\n\nϕ = vec_angle(v1, v2; unit=:rad)\n\nreturns the angle between two vectors in radians (or degrees, with unit=:degree).\n\n\n\n\n\n","category":"method"},{"location":"api/#QuantumControl.optimize-Tuple{Any, Val{:GRAPE}}","page":"API","title":"QuantumControl.optimize","text":"using GRAPE\nresult = optimize(problem; method=GRAPE, kwargs...)\n\noptimizes the given control problem via the GRAPE method, by minimizing the functional\n\nJ(ϵ_nl) = J_T(ϕ_k(T)) + λ_a J_a(ϵ_nl)\n\nwhere the final time functional J_T depends explicitly on the forward-propagated states and the running cost J_a depends explicitly on pulse values ϵ_nl of the l'th control discretized on the n'th interval of the time grid.\n\nReturns a GrapeResult.\n\nKeyword arguments that control the optimization are taken from the keyword arguments used in the instantiation of problem; any of these can be overridden with explicit keyword arguments to optimize.\n\nRequired problem keyword arguments\n\nJ_T: A function J_T(Ψ, trajectories) that evaluates the final time functional from a list Ψ of forward-propagated states and problem.trajectories. The function J_T may also take a keyword argument tau. If it does, a vector containing the complex overlaps of the target states (target_state property of each trajectory in problem.trajectories) with the propagated states will be passed to J_T.\n\nOptional problem keyword arguments\n\nchi: A function chi(Ψ, trajectories) that receives a list Ψ of the forward propagated states and returns a vector of states χₖ = -J_TΨₖ. If not given, it will be automatically determined from J_T via make_chi with the default parameters. Similarly to J_T, if chi accepts a keyword argument tau, it will be passed a vector of complex overlaps.\nJ_a: A function J_a(pulsevals, tlist) that evaluates running costs over the pulse values, where pulsevals are the vectorized values ϵ_nl, where n are in indices of the time intervals and l are the indices over the controls, i.e., [ϵ₁₁, ϵ₂₁, …, ϵ₁₂, ϵ₂₂, …] (the pulse values for each control are contiguous). If not given, the optimization will not include a running cost.\ngradient_method=:gradgen: One of :gradgen (default) or :taylor. With gradient_method=:gradgen, the gradient is calculated using QuantumGradientGenerators. With gradient_method=:taylor, it is evaluated via a Taylor series, see Eq. (20) in Kuprov and Rogers,  J. Chem. Phys. 131, 234108 (2009) [4].\ntaylor_grad_max_order=100: If given with gradient_method=:taylor, the maximum number of terms in the Taylor series. If taylor_grad_check_convergence=true (default), if the Taylor series does not convergence within the given number of terms, throw an an error. With taylor_grad_check_convergence=true, this is the exact order of the Taylor series.\ntaylor_grad_tolerance=1e-16: If given with gradient_method=:taylor and taylor_grad_check_convergence=true, stop the Taylor series when the norm of the term falls below the given tolerance. Ignored if taylor_grad_check_convergence=false.\ntaylor_grad_check_convergence=true: If given as true (default), check the convergence after each term in the Taylor series an stop as soon as the norm of the term drops below the given number. If false, stop after exactly taylor_grad_max_order terms.\nlambda_a=1: A weight for the running cost J_a.\ngrad_J_a: A function to calculate the gradient of J_a. If not given, it will be automatically determined. See make_grad_J_a for the required interface.\nupper_bound: An upper bound for the value of any optimized control. Time-dependent upper bounds can be specified via pulse_options.\nlower_bound: A lower bound for the value of any optimized control. Time-dependent lower bounds can be specified via pulse_options.\npulse_options: A dictionary that maps every control (as obtained by get_controls from the problem.trajectories) to a dict with the following possible keys:\n:upper_bounds: A vector of upper bound values, one for each intervals of the time grid. Values of Inf indicate an unconstrained upper bound for that time interval, respectively the global upper_bound, if given.\n:lower_bounds: A vector of lower bound values. Values of -Inf indicate an unconstrained lower bound for that time interval,\nprint_iters=true: Whether to print information after each iteration.\nstore_iter_info=Set(): Which fields from print_iters to store in result.records. A subset of Set([\"iter.\", \"J_T\", \"|∇J_T|\", \"ΔJ_T\", \"FG(F)\", \"secs\"]).\ncallback: A function (or tuple of functions) that receives the GRAPE workspace and the iteration number. The function may return a tuple of values which are stored in the GrapeResult object result.records. The function can also mutate the workspace, in particular the updated pulsevals. This may be used, e.g., to apply a spectral filter to the updated pulses or to perform similar manipulations. Note that print_iters=true (default) adds an automatic callback to print information after each iteration. With store_iter_info, that callback automatically stores a subset of the printed information.\ncheck_convergence: A function to check whether convergence has been reached. Receives a GrapeResult object result, and should set result.converged to true and result.message to an appropriate string in case of convergence. Multiple convergence checks can be performed by chaining functions with ∘. The convergence check is performed after any callback.\nx_tol: Parameter for Optim.jl\nf_tol: Parameter for Optim.jl\ng_tol: Parameter for Optim.jl\nshow_trace: Parameter for Optim.jl\nextended_trace:  Parameter for Optim.jl\nshow_every: Parameter for Optim.jl\nallow_f_increases: Parameter for Optim.jl\noptimizer: An optional Optim.jl optimizer (Optim.AbstractOptimizer instance). If not given, an L-BFGS-B optimizer will be used.\nprop_method: The propagation method to use for each trajectory, see below.\nverbose=false: If true, print information during initialization\nrethrow_exceptions: By default, any exception ends the optimization, but still returns a GrapeResult that captures the message associated with the exception. This is to avoid losing results from a long-running optimization when an exception occurs in a later iteration. If rethrow_exceptions=true, instead of capturing the exception, it will be thrown normally.\n\nTrajectory propagation\n\nGRAPE may involve three types of propagation:\n\nA forward propagation for every Trajectory in the problem\nA backward propagation for every trajectory\nA backward propagation of a gradient generator for every trajectory.\n\nThe keyword arguments for each propagation (see propagate) are determined from any properties of each Trajectory that have a prop_ prefix, cf. init_prop_trajectory.\n\nIn situations where different parameters are required for the forward and backward propagation, instead of the prop_ prefix, the fw_prop_ and bw_prop_ prefix can be used, respectively. These override any setting with the prop_ prefix. Similarly, properties for the backward propagation of the gradient generators can be set with properties that have a grad_prop_ prefix. These prefixes apply both to the properties of each Trajectory and the problem keyword arguments.\n\nNote that the propagation method for each propagation must be specified. In most cases, it is sufficient (and recommended) to pass a global prop_method problem keyword argument.\n\n\n\n\n\n","category":"method"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The following examples illustrate the use of Krotov's method:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Entangling quantum gates for coupled transmon qubits.\nOptimization of a Dissipative Quantum Gate","category":"page"},{"location":"overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GRAPE","category":"page"},{"location":"#GRAPE.jl","page":"Home","title":"GRAPE.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\n\nVERSION = Pkg.dependencies()[Base.UUID(\"6b52fcaf-80fe-489a-93e9-9f92080510be\")].version\n\ngithub_badge = \"[![Github](https://img.shields.io/badge/JuliaQuantumControl-GRAPE.jl-blue.svg?logo=github)](https://github.com/JuliaQuantumControl/GRAPE.jl)\"\n\nversion_badge = \"![v$VERSION](https://img.shields.io/badge/version-v$VERSION-green.svg)\"\n\nMarkdown.parse(\"$github_badge $version_badge\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Implementation of (second-order) GRadient Ascent Pulse Engineering (GRAPE) [1, 2] extended with semi-automatic differentiation [3].","category":"page"},{"location":"","page":"Home","title":"Home","text":"Part of QuantumControl.jl and the JuliaQuantumControl organization.","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Depth = 2\nPages = [pair[2] for pair in Main.PAGES[2:end-1]]","category":"page"},{"location":"#History","page":"Home","title":"History","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the Releases on Github.","category":"page"}]
}
